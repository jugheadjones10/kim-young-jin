{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # noqa\n",
    "import pickle  # noqa\n",
    "import numpy as np  # noqa\n",
    "\n",
    "\n",
    "class environment:\n",
    "    def steps(self, state, action):\n",
    "        self.state_win = state + action\n",
    "        self.state_lose = state - action\n",
    "        return self.state_win, self.state_lose\n",
    "\n",
    "\n",
    "class agent:\n",
    "    def __init__(self):\n",
    "        self.states = 101\n",
    "        self.v = np.zeros((self.states), dtype=\"float\")\n",
    "        self.v[-1] = 1\n",
    "        self.stable = False\n",
    "        self.theta = 0.001\n",
    "\n",
    "    def possible_actions(self, state):\n",
    "        actions = np.arange(1, min(state, 100 - state) + 1, 1)\n",
    "        return actions\n",
    "\n",
    "    def value_iteration(self, ph, env):\n",
    "        delta = self.theta\n",
    "        sweeps = []\n",
    "        sweep = 0\n",
    "        while delta >= self.theta:\n",
    "            old_values = self.v.copy()\n",
    "            for state in range(1, self.states - 1):\n",
    "                values = []\n",
    "                actions = self.possible_actions(state)\n",
    "                for a in actions:\n",
    "                    state_win, state_lose = env.steps(state, a)\n",
    "                    value = (ph * self.v[state_win]) + ((1 - ph) * self.v[state_lose])\n",
    "                    values.append(value)\n",
    "\n",
    "                values = np.array(values)\n",
    "                self.v[state] = np.amax(\n",
    "                    values\n",
    "                )  # update value function with value maximising action\n",
    "            sweeps.append(old_values)\n",
    "            sweep += 1\n",
    "            delta = np.max(np.abs(old_values - self.v))\n",
    "\n",
    "            print(f\"Probability of Heads: {ph}\")\n",
    "            print(f\"End of sweep: {sweep}, Delta = {delta}\")\n",
    "\n",
    "        return self.v, sweeps\n",
    "\n",
    "    def find_policy(self, v, env, ph):\n",
    "        stakes = []\n",
    "        for state in range(1, self.states - 1):\n",
    "            a_vals = []\n",
    "            actions = self.possible_actions(state)\n",
    "            for a in actions:\n",
    "                state_win, state_lose = env.steps(state, a)\n",
    "                a_val = (ph * v[state_win]) + ((1 - ph) * v[state_lose])\n",
    "                a_vals.append(a_val)\n",
    "\n",
    "            a_arr = np.array(a_vals)\n",
    "            a_max = np.argmax(a_arr) + 1\n",
    "            stakes.append(a_max)\n",
    "\n",
    "        return stakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    env = environment()\n",
    "    agent = agent()\n",
    "\n",
    "    phs = [0.25, 0.55]\n",
    "\n",
    "    final_vf = {}\n",
    "    sweeps = {}\n",
    "    policy = {}\n",
    "\n",
    "    for ph in phs:\n",
    "        v_func, sweep = agent.value_iteration(ph, env)\n",
    "        stakes = agent.find_policy(v_func, env, ph)\n",
    "        final_vf[str(ph)] = v_func\n",
    "        sweeps[str(ph)] = sweep\n",
    "        policy[str(ph)] = stakes\n",
    "\n",
    "    with open(\"data/final_v_functions.pickle\", \"wb\") as f:\n",
    "        pickle.dump(final_vf, f)\n",
    "\n",
    "    with open(\"data/sweeps.pickle\", \"wb\") as f:\n",
    "        pickle.dump(sweeps, f)\n",
    "\n",
    "    with open(\"data/policy.pickle\", \"wb\") as f:\n",
    "        pickle.dump(policy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/final_v_functions.pickle\", \"rb\") as f:\n",
    "    v_functions = pickle.load(f)\n",
    "\n",
    "with open(\"data/sweeps.pickle\", \"rb\") as f:\n",
    "    sweeps = pickle.load(f)\n",
    "\n",
    "with open(\"data/policy.pickle\", \"rb\") as f:\n",
    "    policy = pickle.load(f)\n",
    "\n",
    "ph = [\"0.25\", \"0.55\"]\n",
    "\n",
    "for p in ph:\n",
    "    sweep_p = []\n",
    "    policy_p = []\n",
    "    for arr in sweeps[p]:\n",
    "        sweep_p.append(arr.flatten())\n",
    "    for arr in policy[p]:\n",
    "        policy_p.append(arr)\n",
    "\n",
    "    x = np.arange(0, 101, 1)\n",
    "    x_pol = np.arange(1, 100, 1)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    fig.suptitle(f\"$P_h({p})$\", fontsize=16)\n",
    "\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax1.title.set_text(\"Value Function Approximation per Sweep\")\n",
    "\n",
    "    i = 1\n",
    "    for arr in sweep_p:\n",
    "        ax1.plot(x, arr, label=\"sweep: {}\".format(i))\n",
    "        i += 1\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    ax2.title.set_text(\"Optimal Policy\")\n",
    "    ax2.plot(x_pol, policy_p)\n",
    "\n",
    "    plt.savefig(\"images/p_{}.png\".format(p), dpi=300)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
