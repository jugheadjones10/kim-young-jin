[
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "Notes from “The Work of His Hands” by Sy Garte\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\n\n\n\n\n\nQuick thoughts on ‘Mere Christianity’ by C.S Lewis\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\n\n\n\n\n\nWhat to do about the future\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\n\n\n\n\n\nTrying to get good at math and science\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\n\n\n\n\n\nAmbitious but at peace\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kim Young Jin",
    "section": "",
    "text": "Hi! I’m currently studying Computer Science at the National University of Singapore. In a past life I was obsessed with creating software products (still am, to an extent). Now I am working to become a computational neuroscientist. I am at the core a deeply curious person - and one of the most curiosity-inducing mysteries of the world is that of intelligence. What is it, and how do we create it?\nMy thesis is that artificial intelligence needs to take inspiration from the only proof-of-existence we have, the brain, if we want a shot at AGI. Many say neuroscience is pre-paradigmic; on an initial survey of the field, it seems like there is a greater focus on finding interesting correlations rather than testing overarching theoretical models. Can the creation of artificial systems emulating discrete aspects of intelligence bridge that gap between data and conceptual clarity?\nI hope to find out.\n\nA TLDR of what I’ve been doing with life:\n\nIn my K-12 years I thought I wanted to be a writer so I got published in some obscure literary journals.\nStarted programming in high school so that I could make goofy websites.\nStudied CS at the National University of Singapore. Built huge Android apps to validate business ideas.\nServed in the Korean army for 2 years. In my free time I built random stuff using my iPad as my IDE (laptops weren’t allowed).\nCold emailed Zeet CEO for an internship because they were solving a problem I was facing (cloud deployment). Added some features, fixed some bugs. Flew to SF to meet the team.\nCold emailed Locofy CEO for an internship because they were solving a problem I was facing (frontend codegen from Figma). I believed the product would be re-invent the way frontends are built.\nTook a Leave of Absence to go all-in on the biggest product launch in Locofy’s history, where we automated away a huge portion of the Figma-to-frontend workflow for engineers.\nCame back to school, became fascinated by the brain after reading several books.\nCurrently trying to do some research at the intersection of AI and neuroscience by solving the Monkey and Banana problem with neuro-inspired methods."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m currently studying Computer Science at the National University of Singapore. In a past life I was obsessed with creating software products (still am, to an extent). Now I am working to become a computational neuroscientist. I am at the core a deeply curious person - and one of the most curiosity-inducing mysteries of the world is that of intelligence. What is it, and how do we create it?\nMy thesis is that artificial intelligence needs to take inspiration from the only proof-of-existence we have, the brain, if we want a shot at AGI. Many say neuroscience is pre-paradigmic; on an initial survey of the field, it seems like there is a greater focus on finding interesting correlations rather than testing overarching theoretical models. Can the creation of artificial systems emulating discrete aspects of intelligence bridge that gap between data and conceptual clarity?\nA TLDR of what I’ve been doing with life:\n\nIn my K-12 years I thought I wanted to be a writer so I got published in some obscure literary journals.\nStarted programming in high school so that I could make goofy websites.\nStudied CS at the National University of Singapore. Built huge Android apps to validate business ideas.\nServed in the Korean army for 2 years. In my free time I built random stuff using my iPad as my IDE (laptops weren’t allowed).\nCold emailed Zeet CEO for an internship because they were solving a problem I was facing (cloud deployment). Added some features, fixed some bugs. Flew to SF to meet the team.\nCold emailed Locofy CEO for an internship because they were solving a problem I was facing (frontend codegen from Figma). I believed the product would be re-invent the way frontends are built.\nTook a Leave of Absence to go all-in on the biggest product launch in Locofy’s history, where we automated away a huge portion of the Figma-to-frontend workflow for engineers.\nCame back to school, became fascinated by the brain after reading several books.\nCurrently trying to do some research at the intersection of AI and neuroscience by solving the Monkey and Banana problem with neuro-inspired methods."
  },
  {
    "objectID": "learning.html",
    "href": "learning.html",
    "title": "Learning",
    "section": "",
    "text": "Replication of some chapter 5 results\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\nReplication of some chapter 3 results\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\n\n\n\n\n\nNeural network and SGD from scratch\n\n\n\n\n\n\n\n\nJun 27, 2024\n\n\n\n\n\n\n\n‘Insight’ in the pigeon: antecedents and determinants of an intelligent performance\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learning/test/index.html",
    "href": "learning/test/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "learning/other/index.html",
    "href": "learning/other/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "learning/reinforcement-learning/index.html",
    "href": "learning/reinforcement-learning/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "learning/other/hey.html",
    "href": "learning/other/hey.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "learning/reinforcement-learning/sutton-and-barto/chapter-2.html",
    "href": "learning/reinforcement-learning/sutton-and-barto/chapter-2.html",
    "title": "Sutton’s Reinforcement Learning Chapter 2: Multi-armed Bandits",
    "section": "",
    "text": "import numpy as np  # noqa\nimport matplotlib.pyplot as plt  # noqa\n\n\nComparison of 10-armed bandit problem between ε-greedy and greedy methods\nActions estimates are calculated using the sample-average method.\n\ndef sample_action(true_value):\n    return np.random.normal(true_value, 1)\n\n\ndef executor(runs=2000, time_steps=1000, epsilon=0.1):\n    rewards_agg = []\n    for i in range(runs):\n        rewards_history = []\n\n        # Define true action values\n        qstar = np.random.normal(0, 1, size=10)\n\n        # Action value estimates are all initially 0\n        qa = np.zeros(10)\n        action_counts = np.zeros(10)\n\n        for n in range(time_steps):\n            if np.random.rand() &gt; epsilon:\n                # Choose action greedily\n                action = np.argmax(qa)\n            else:\n                action = np.random.choice(np.arange(10))\n\n            # Sample action_value\n            reward = sample_action(qstar[action])\n            rewards_history.append(reward)\n\n            # Update action count\n            action_counts[action] += 1\n            # Update action value estimate\n            qa[action] = qa[action] + 1 / action_counts[action] * (reward - qa[action])\n\n        rewards_agg.append(rewards_history)\n\n    stacked = np.vstack(rewards_agg)\n    averaged_array = np.mean(stacked, axis=0)\n    return averaged_array\n\n\n# Plot different runs of the experiment in different colors on the same plot\nresults = executor(epsilon=0.1)\nplt.plot(results, color=\"blue\")\n\nresults = executor(epsilon=0.01)\nplt.plot(results, color=\"red\")\n\nresults = executor(epsilon=0)\nplt.plot(results, color=\"green\")\n\n# Increase width of plot\nplt.gcf().set_size_inches(10, 5)\n\n\n\n\n\n\n\n\n\n\nExercise 2.5\nComparison of non-stationary 10-armed bandit problem between sample-average and constant step-size parameter methods\n\ndef sample_action(true_value):\n    return np.random.normal(true_value, 1)\n\n\ndef update_true_action_values(true_values):\n    return [value + np.random.normal(0, 0.01) for value in true_values]\n\n\n# Define number of runs\ndef executor(runs=2000, time_steps=1000, epsilon=0.1, alpha=0.1, constant_step=False):\n    rewards_agg = []\n    for i in range(runs):\n        rewards_history = []\n\n        # Define true action values\n        qstar = np.random.normal(0, 1, size=10)\n\n        # Action value estimates are all initially 0\n        qa = np.zeros(10)\n        action_counts = np.zeros(10)\n\n        for n in range(1, time_steps + 1):\n            if np.random.rand() &gt; epsilon:\n                # Choose action greedily\n                action = np.argmax(qa)\n            else:\n                action = np.random.choice(np.arange(10))\n\n            # Sample action_value\n            reward = sample_action(qstar[action])\n            rewards_history.append(reward)\n\n            # Update action count\n            action_counts[action] += 1\n\n            # Update action value estimate\n            if constant_step:\n                qa[action] = qa[action] + (reward - qa[action]) * alpha\n            else:\n                qa[action] = qa[action] + (reward - qa[action]) * (\n                    1 / action_counts[action]\n                )\n\n            # Update true action values (to simulate non-stationarity)\n            qstar = update_true_action_values(qstar)\n\n        rewards_agg.append(rewards_history)\n\n    stacked = np.vstack(rewards_agg)\n    averaged_array = np.mean(stacked, axis=0)\n    return averaged_array\n\n\nresults = executor(time_steps=10000, epsilon=0.1, constant_step=False)\nplt.plot(results, color=\"blue\")\n\nresults = executor(time_steps=10000, epsilon=0.1, constant_step=True)\nplt.plot(results, color=\"red\")\n\nplt.gcf().set_size_inches(10, 5)\n\n\n\n\n\n\n\n\n\n\nExercise 2.11\n\n# Need to run for different epsilon values\n\nresults = []\nfor i in range(-5, 2):\n    epsilon = 2**i\n\n    results = executor(\n        runs=1000, time_steps=200000, epsilon=epsilon, constant_step=True\n    )\n\n    # Average the last 100,000 rewards\n    average_reward = np.mean(results[-100000:])\n    results.append(average_reward)\n\n# X-axis should be the epsilons\nplt.plot(range(-5, 2), results, color=\"blue\")\n\nplt.gcf().set_size_inches(10, 5)\n\nAttributeError: 'numpy.ndarray' object has no attribute 'append'"
  },
  {
    "objectID": "posts/ambitious-and-content.html",
    "href": "posts/ambitious-and-content.html",
    "title": "Ambitious but at peace",
    "section": "",
    "text": "For most of my life I’ve felt a deep discontent myself.\nWe hear a lot of advice to “love yourself”, have a “kind inner voice”, and so on. I’ve always felt these were coping mechanisms, anesthesia to dull the pain of reality. Better to embrace the pain, I thought. Invigorate yourself with the fury of self-hatred and envy. Only the unambitious are content with the warm fuzziness of satisfaction. It’s like what they always say: the most successful people contain within them a contradiction - near delusional levels of self-belief fueled by an intense anxiety around never being good enough.\nI’m still in my early twenties, but my stance has changed somewhat. Ambition doesn’t mean you need to constantly hate where you are at. Ambition is about the magnitude of your end goals, not your mental state on the way there. If you could take a different road to the same destination, one that preserves your peace of mind, why wouldn’t you?\nAnd that other, more peaceful road, might get you to your goal faster. With peace of mind, you can keep going, day in and day out. By prioritising peace of mind, you are making the choice to run a marathon instead of a sprint. Who will be fresh-headed and grinding, ten years from now? The ambitious one who looks at every failure of his as the biggest catastrophe he has ever faced? Or the ambitious one who controls what is within his control, lets go of what is not, and is at peace with himself?\nReal strength comes from knowing that there is absolutely nothing that can happen to you that will stop you from continuing forward.\nHacker news discussion"
  },
  {
    "objectID": "posts/existence-of-god.html",
    "href": "posts/existence-of-god.html",
    "title": "Arguments for and against God",
    "section": "",
    "text": "A list of arguments and counter-arguments for the existence of God. For personal reference.\nMuch of the material for this first draft comes from Bertrand Russell’s “Why I Am Not a Christian”; what follows is a rough summary of his points.\n\nThe First Cause Argument\ngoes like this: everything has a cause. So there must have been a first cause that existed before any other cause. And that first cause, existing for eternity into the past and future, is God. This is one of those arguments that sounds plausible at first, but on closer inspection there is a logical error. If the premise is that everything has a cause, then God shuold have a cause too. If then one argues that no, there exists some things which do not have a cause, one of which is God, we can just as easily ask - why cannot the Universe be without cause? Either way, the existence of a God is not necessitated."
  },
  {
    "objectID": "posts/trying-to-get-good-at-math-and-science.html",
    "href": "posts/trying-to-get-good-at-math-and-science.html",
    "title": "Trying to get good at math and science",
    "section": "",
    "text": "Getting good at Math and Science occupies a hazy middle ground between memory and understanding, between mechanical and conceptual understanding. Memory and understanding are also not straightforward. There are layers. Understanding might beget an Aha! moment that slips cunningly away, tricking us into reassurance. Understanding once, does not mean you understand forever. So inevitably there is a component of memory in understanding. But there are also some types of understanding that are more “meta” - broad strokes that set the stage and context behind so many of a subject’s concepts. This kind of understanding can act like a pinboard for the rest of the subject, helping you see the connections and draw the lines.\nAnd further still, there is a type of understanding that can only come from practice, and this is the most difficult to acquire of them all. It can be fun, and even easy, to watch beautiful 3B1B videos or read textbooks and “understand” their theorems. You might even have memorized most of the concepts presented. But does that mean you can apply them to problems?\nTo solve the really challening problems, you need to twist and distort concepts, fit them together this way and that. If understanding gained from reading is like listening to your Dad explain how to balance on a 2-wheel bicycle, understanding gained from practice is like using every ounce of dexterity and physical instinct in your body to balance, balance, somehow try to balance!\nReading pushes information into your brain. Practice pulls from it. As strands of half-formed ideas are wrenched from your brain, they tangle with one another and with concepts wholly unconnected from what you may have been learning about. It is through that process that understanding starts to show hints of becoming yours.\nI have been thinking about how to make it easier to learn from textbooks.\nHow can we use LLMs to augment the learning process of a textbook? First, we need to accept that learning is never going to become “easy”. You need to put in the hours and the concentration, and no AI is going to do that for you. What it CAN do is act like a tool to streamline how you practice questions and try to remember content.\nA sort of “ecosystem” for autodidacts? Online forum for crowdsourced answers for textbooks, and an LLM-powered textbook reader for creating flashcards and knowledge graphs for textbooks. Maybe this can be community-driven as well? No, but maybe that defeats the purpose. Everyone just generates their own personalized flashcards, for their own learning."
  },
  {
    "objectID": "drafts/chapter-2.html",
    "href": "drafts/chapter-2.html",
    "title": "Sutton’s Reinforcement Learning Chapter 2: Multi-armed Bandits",
    "section": "",
    "text": "import numpy as np  # noqa\nimport matplotlib.pyplot as plt  # noqa\n\n\nComparison of 10-armed bandit problem between ε-greedy and greedy methods\nActions estimates are calculated using the sample-average method.\n\ndef sample_action(true_value):\n    return np.random.normal(true_value, 1)\n\n\ndef executor(runs=2000, time_steps=1000, epsilon=0.1):\n    rewards_agg = []\n    for i in range(runs):\n        rewards_history = []\n\n        # Define true action values\n        qstar = np.random.normal(0, 1, size=10)\n\n        # Action value estimates are all initially 0\n        qa = np.zeros(10)\n        action_counts = np.zeros(10)\n\n        for n in range(time_steps):\n            if np.random.rand() &gt; epsilon:\n                # Choose action greedily\n                action = np.argmax(qa)\n            else:\n                action = np.random.choice(np.arange(10))\n\n            # Sample action_value\n            reward = sample_action(qstar[action])\n            rewards_history.append(reward)\n\n            # Update action count\n            action_counts[action] += 1\n            # Update action value estimate\n            qa[action] = qa[action] + 1 / action_counts[action] * (reward - qa[action])\n\n        rewards_agg.append(rewards_history)\n\n    stacked = np.vstack(rewards_agg)\n    averaged_array = np.mean(stacked, axis=0)\n    return averaged_array\n\n\n# Plot different runs of the experiment in different colors on the same plot\nresults = executor(epsilon=0.1)\nplt.plot(results, color=\"blue\")\n\nresults = executor(epsilon=0.01)\nplt.plot(results, color=\"red\")\n\nresults = executor(epsilon=0)\nplt.plot(results, color=\"green\")\n\n# Increase width of plot\nplt.gcf().set_size_inches(10, 5)\n\n\n\n\n\n\n\n\n\n\nExercise 2.5\nComparison of non-stationary 10-armed bandit problem between sample-average and constant step-size parameter methods\n\ndef sample_action(true_value):\n    return np.random.normal(true_value, 1)\n\n\ndef update_true_action_values(true_values):\n    return [value + np.random.normal(0, 0.01) for value in true_values]\n\n\n# Define number of runs\ndef executor(runs=2000, time_steps=1000, epsilon=0.1, alpha=0.1, constant_step=False):\n    rewards_agg = []\n    for i in range(runs):\n        rewards_history = []\n\n        # Define true action values\n        qstar = np.random.normal(0, 1, size=10)\n\n        # Action value estimates are all initially 0\n        qa = np.zeros(10)\n        action_counts = np.zeros(10)\n\n        for n in range(1, time_steps + 1):\n            if np.random.rand() &gt; epsilon:\n                # Choose action greedily\n                action = np.argmax(qa)\n            else:\n                action = np.random.choice(np.arange(10))\n\n            # Sample action_value\n            reward = sample_action(qstar[action])\n            rewards_history.append(reward)\n\n            # Update action count\n            action_counts[action] += 1\n\n            # Update action value estimate\n            if constant_step:\n                qa[action] = qa[action] + (reward - qa[action]) * alpha\n            else:\n                qa[action] = qa[action] + (reward - qa[action]) * (\n                    1 / action_counts[action]\n                )\n\n            # Update true action values (to simulate non-stationarity)\n            qstar = update_true_action_values(qstar)\n\n        rewards_agg.append(rewards_history)\n\n    stacked = np.vstack(rewards_agg)\n    averaged_array = np.mean(stacked, axis=0)\n    return averaged_array\n\n\nresults = executor(time_steps=10000, epsilon=0.1, constant_step=False)\nplt.plot(results, color=\"blue\")\n\nresults = executor(time_steps=10000, epsilon=0.1, constant_step=True)\nplt.plot(results, color=\"red\")\n\nplt.gcf().set_size_inches(10, 5)\n\n\n\n\n\n\n\n\n\n\nExercise 2.11\n\n## TODO"
  },
  {
    "objectID": "posts/mere-christianity.html",
    "href": "posts/mere-christianity.html",
    "title": "Quick thoughts on ‘Mere Christianity’ by C.S Lewis",
    "section": "",
    "text": "1. Pride\nC.S Lewis talks at length about how Pride is one of those things that is easiest to criticize in others, but hardest to find in yourself.\nIt is effortless to despise the hubris of others, to whisper of their wicked arrogance behind their backs. But the great irony is that those who most despise Pride in others are in fact the most prideful of the bunch. You cannot bear to see a friend boast and receive all the attention, because you secretly wish it were you in the spotlight instead. You cannot bear to see a neighbour be successful, because you wish it were you claiming success instead.\nI had a friend who always had that cocky air about him. It used to bother me a bit too much. I thought, “What good did he do, that he has the right to strut about like that?” Implicit in this statement is my belief that my friend is not as good as he makes himself out to be. It bothered me so much, because it pricked at my own outsized Pride.\nA low self-esteem, and insecurity, and self-hate - it may seem paradoxical, but now I see that these, too, are manifestations of Pride. When you cannot accept who you are - when you refuse to let go of ambition - every small blemish is reason for you to bemoan your wretched self. Why am I so dumb? Why am I so slow? A great self-loathing grows from this stream of negativity.\nBut if you so hate these shortcomings in yourself, how much more would you despise it in others? All the people who have even less than what you have - what about them? The secretly prideful soul derives a twisted satisfaction from looking down at those beneath him.\n“At least I am better than them.”\n“Dull as I may be, at least I am less dull than them.”\nResentment at those above you translates into hate towards youself, and a superiority complex at those beneath you.\nOnly when you embrace yourself, as you are, can you embrace others, as they are.\n\n\n2. Morality\nC.S Lewis points to the moral compass within every one of us as proof of God. As physical objects obey gravity, so do we obey the moral law. But unlike gravity, we are given a choice. Our free will means that every man, every day, makes decisions that break or keep the moral law.\nI would like to believe in this. It is a compelling argument, except for that one annoying possibility that our morality comes from our Biology, not from some objective moral truth. What if evolution selected for those populations that developed emotional responses to morally “right” or “wrong” actions? Certainly we can see how it might have helped some popluations survive and not others. If that is the case, we cannot argue for an objective moral truth by only “introspecting” on our consciences.\n\n\n3. Free will\nAnd that, of course, leads to the thorny issue of free will. Without free will, none of Christianity makes sense. How can we have free will? How would free will even work? In some sense, there are echoes of the “First Cause Argument” for the existence of God. It says that everything must have a cause, and so the Universe also needs to have a “first cause”. Only God can be that first cause, because God is the only thing that can exist without itself having a cause (this argument is actually flawed - the basis is that everything must have a cause, so we are contradicting ourselves by saying that God does not have a cause).\nCould we argue in a similar way about free will? That everything needs to have a cause, but free will, as something given by God, does not have a “cause” and is solely the product of our self-determining souls?\nWhat if the day comes when we fully understand how human consciousness works, and it turns out that we actually are just machines running on good old cause and effect, living under the illusion of autonomy?\n\n\n4. I hope it is all true\nOne part of me is sceptical, but another side of me wishes it were all true. I am starting to see the consequences that an “amoral” life and mindset is having on my psyche. It is not pretty. There is a constant feeling of emptiness, a desperate hope that there is more to life than my petty ambitions and the little games that we all play.\nOne can only pray. But how can I pray, if I know no God?\n\n\nEpilogue: questions and discussion\nHere are some questions that I would like to ask practicing Christians.\n\nIf it really is true that there is a Heaven and a Hell, shouldn’t Christians be spending all their time trying to convert non-believers? I know that changing someone’s belief cannot be forced. It is not something you can make happen simply by investing time. But surely there are other things to do, like trying to build more churches, etc.? Are Christians allowed to be leisureful if they know full well that there still remain some souls who will suffer for eternity in Hell? To some bystanders, it might seem like Christans are content to “milk the cow”, so to speak, and live out their lives enjoying the peace of mind that religion brings.\nI don’t understand what it means to “receive God’s help” or be “tempted by the Devil”. I understand that we have been given free will by God, but also that we can influence things by praying. But if God can insert thoughts into people’s minds, then don’t those people not have free will? In fact, I thought God is allowing bad things to happen in the world precisely because he has given us free will and does not want to intervene. How come Christians are given a chance to circumvent this restriction through prayer?\nThere are many interpretations of the Bible. There are many interpretations of Theology. But there can only be one Truth, right? Some of these interpretations have to be wrong. Can one be “saved” even if he believes in a slightly “wrong” interpretation? If not, then shouldn’t finding this one correct interpretation be the primary pursuit of Christians? But then again, can there even be one definite interpretation of something literary like the Bible?\nIf it is not possible to converge to one correct interpretation, but there is only one True reality, does that mean the best we can do is hope we have joined the correct denomination and pray for the best? That seems awfully random and cruel of God.\nReality is complex. People spend their lives studying how reality works, and many are still wrong. Why is religion different? Yes, we cannot try to understand everything, because we are not God and therefore will not be able to understand everything. But at the same time people seem to use “God works in mysterious ways” as a get-out-of-jail-free card.\nOne thing that bugs me is that at the end of the day, everything kind of revolves around feelings, or, more precisely, perception. It all depends on the exact thing that your consciousness is experiencing. If that thing is strong enough to convince you of God’s existence, the story ends there, no one can convince you otherwise. It’s just that so many aspects of Christian life seem to revolve around these “perceptions”: “God touched my heart”, “God helped me love my brethren”, “God guided me take this path”. I mean, yeah, you could say that you really did feel God and I wouldn’t be able to refute that because I do not know your consciousness. But we do know that feelings are caused by complex biochemical processes in the body.\nWe do not yet understand consciousness enough to map every single conscious experience to some physical state that the body is in, but what if all these “feelings” turn out to be emotions caused by certain psychological states that social structures like Churches cause in you?\nUnless - and this is an interesting idea - the very mechanisms of Biology were hand-crafted by God to cause these feelings. In other words, it’s not emotions causing spiritual feelings - it’s that God engineered the world so that doing spiritual things will, through Biology, cause your consciousness to experience those most edifying feelings of being in touch with God."
  },
  {
    "objectID": "drafts/test.html",
    "href": "drafts/test.html",
    "title": "Kim Young Jin",
    "section": "",
    "text": "import numpy as np  # noqa\nimport matplotlib.pyplot as plt  # noqa\n\n\n# GAMBLER PROBLEM\n#\n# EXERCISE 4.9\n#\n# Implement value iteration for the gambler's problem and solve it for ph = 0.25 and ph = 0.55. In programming, you may\n# find it convenient to introduce two dummy states corresponding to termination with capital of 0 and 100, giving them\n# values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. Are your results stable as theta→0?\n\n\n# A wrapper class for parameters of the algorithm\nclass Params:\n    def __init__(self):\n        # Money goal to reach\n        self.max_money = 100\n\n        # Possible values of probability of the coin coming up heads (winning the bet)\n        self.p_hs = [0.25, 0.4, 0.55]\n        # self.p_hs = [0.4]\n\n        # Small number determining the accuracy of policy evaluation's estimation\n        self.theta = 10**-10\n        # self.theta = 10**-1000\n        # self.theta = 0.00001\n\n        # Discount\n        self.gamma = 1\n\n\n# A wrapper class for Value Iteration algorithm\nclass ValueIteration:\n    def __init__(self, p_h, params):\n        \"\"\"\n        :param p_h: probability of the coin coming up heads (winning the bet)\n        :param params: the parameters instance\n        \"\"\"\n        # Set up the ph value\n        self.p_h = p_h\n\n        # Set up parameters\n        self.params = params\n\n        # All possible states\n        self.S = np.arange(1, self.params.max_money)\n\n        # Value function\n        self.V = np.zeros(self.params.max_money + 1)\n        self.V[0] = 0\n        self.V[self.params.max_money] = 1\n\n        # List of value functions\n        self.Vs = []\n\n        # Policy function\n        self.pi = None\n\n        # Number of sweeps needed to complete the problem\n        self.sweep_count = None\n\n    def solve_problem(self):\n        \"\"\"\n        Resolve Gambler Problem using Value Iteration\n        \"\"\"\n        self.sweep_count = 0\n        while True:\n            delta = 0\n            for s in self.S:\n                v = self.V[s]\n                self.V[s] = np.max([self.V_eval(s, a) for a in self.A(s)])\n                delta = np.maximum(delta, abs(v - self.V[s]))\n            if self.sweep_count &lt; 3:\n                self.Vs.append(self.V.copy())\n            self.sweep_count += 1\n            if delta &lt; self.params.theta:\n                break\n        print(\"Sweeps needed:\", self.sweep_count)\n        self.Vs.append(self.V.copy())\n        self.pi = [\n            self.A(s)[np.argmax([self.V_eval(s, a) for a in self.A(s)])] for s in self.S\n        ]\n\n    def A(self, s):\n        \"\"\"\n        Get all possible actions given a state\n        :param s: state\n        :return: possible actions\n        \"\"\"\n        # All possible actions\n        return np.arange(1, np.minimum(s, self.params.max_money - s) + 1)\n\n    def V_eval(self, s, a):\n        \"\"\"\n        Compute value given a state and an action for the state following the formula:\n        sum over all s',r of p(s',r|s, a)[r + gamma*V(s')]\n        :param s: state\n        :param a: action\n        :return: value\n        \"\"\"\n        return self.params.gamma * self.V[\n            s + a\n        ] * self.p_h + self.params.gamma * self.V[s - a] * (1 - self.p_h)\n\n    def print_Vs(self):\n        \"\"\"\n        Print values for the first three sweeps and for the last one\n        \"\"\"\n        plt.figure()\n        plt.plot(self.Vs[0], label=\"sweep 1\")\n        plt.plot(self.Vs[1], label=\"sweep 2\")\n        plt.plot(self.Vs[2], label=\"sweep 3\")\n        plt.plot(self.Vs[3], label=\"sweep {}\".format(self.sweep_count))\n        plt.legend(loc=\"upper left\")\n        plt.xlabel(\"Capital\")\n        plt.ylabel(\"Value estimates\")\n        plt.title(\"Values ph={}\".format(self.p_h))\n\n    def print_pi(self):\n        \"\"\"\n        Print policy\n        \"\"\"\n        plt.figure()\n        plt.step(self.S, self.pi)\n        plt.xlabel(\"Capital\")\n        plt.ylabel(\"Final policy (stake)\")\n        plt.title(\"pi ph={}\".format(self.p_h))\n\n\ndef exercise4_9():\n    print(\"Exercise 4.9\")\n\n    # Set up parameters\n    params = Params()\n\n    for p_h in params.p_hs:\n        print(\"Problem with ph:\", p_h)\n\n        # Set up the algorithm\n        policy_iteration = ValueIteration(p_h, params)\n\n        # Solve the problem\n        policy_iteration.solve_problem()\n\n        # Show results\n        policy_iteration.print_Vs()\n        policy_iteration.print_pi()\n\n\nexercise4_9()\nplt.show()\n\nExercise 4.9\nProblem with ph: 0.25\nSweeps needed: 16\nProblem with ph: 0.4\nSweeps needed: 19\nProblem with ph: 0.55\nSweeps needed: 1838"
  },
  {
    "objectID": "drafts/chapter-4.html",
    "href": "drafts/chapter-4.html",
    "title": "Sutton’s Reinforcement Learning Chapter 4: Dynamic Programming",
    "section": "",
    "text": "import numpy as np  # noqa\nfrom typing import Callable  # noqa\nimport math  # noqa\nimport matplotlib.pyplot as plt  # noqa\nimport itertools  # noqa\nimport functools  # noqa\nfrom scipy.stats import poisson  # noqa\nimport time  # noqa\n\n\nloc_1_cars = 20\nloc_2_cars = 2\n\nnum_cars = 20\n\n# Num rent is some infinite distribution\n# But in reality it ranges from number of cars available to 0 (we can adjust the PMF correspondingly)\n# Num return is also some infinite dist, but it ranges from 0 to num_cars - num_rent\n\n# So given loc_1_cars and loc_2_cars, we can find the range of num_rent and num_ret respectively\n# We iterate and come up with all possible combinations of these values\n# Use their probability distributions to calculate probabilities of each end state\n\n\npoisson.pmf([1, 2, 3], 3)\n\narray([0.14936121, 0.22404181, 0.22404181])\n\n\n\n# one = np.arange(5)\n# two = np.arange(5)\n# one - two\n\n\ndef final_state_prob_dist(max_cars, loc_1_cars, rent_poisson, return_poisson):\n    # Multiply number of cars that can be rented with number of cars that can be returned\n    num_rows = (loc_1_cars + 1) * (max_cars + 1)\n    loc_1_cars_list = np.repeat(loc_1_cars, num_rows)\n    loc_1_cars_rent_list = np.repeat(np.arange(loc_1_cars + 1), max_cars + 1)\n    loc_1_cars_return_list = np.tile(np.arange(max_cars + 1), loc_1_cars + 1)\n    # print(loc_1_cars_list)\n    # print(loc_1_cars_rent_list)\n    # print(loc_1_cars_return_list)\n\n    final = loc_1_cars - loc_1_cars_rent_list + loc_1_cars_return_list\n    # print(final)\n    # Get the indices of the states where the final number of cars is less than or equal to max_cars\n    indices = np.where(final &lt;= max_cars)\n\n    loc_1_cars_list = loc_1_cars_list[indices]\n    loc_1_cars_rent_list = loc_1_cars_rent_list[indices]\n    loc_1_cars_return_list = loc_1_cars_return_list[indices]\n    # print(loc_1_cars_list)\n    # print(loc_1_cars_rent_list)\n    # print(loc_1_cars_return_list)\n\n    # Apply poission.pmf(loc_1_cars_rent_list, 3) to get the probabilities of each rent value\n\n    loc_1_cars_rent_prob = np.zeros(len(loc_1_cars_rent_list))\n    max_rent_prob = 1 - poisson.cdf(loc_1_cars - 1, rent_poisson)\n    for i in range(len(loc_1_cars_rent_list)):\n        if loc_1_cars_rent_list[i] == loc_1_cars:\n            loc_1_cars_rent_prob[i] = max_rent_prob\n        else:\n            loc_1_cars_rent_prob[i] = poisson.pmf(loc_1_cars_rent_list[i], rent_poisson)\n    # print(\"Loc 1 cars rent prob\", loc_1_cars_rent_prob)\n\n    max_ret = loc_1_cars - loc_1_cars_rent_list\n    # print(\"Max_ret\", max_ret)\n    loc_1_cars_return_prob = np.zeros(len(loc_1_cars_return_list))\n\n    for i in range(len(loc_1_cars_return_list)):\n        if loc_1_cars_return_list[i] == max_ret[i]:\n            loc_1_cars_return_prob[i] = 1 - poisson.cdf(max_ret[i] - 1, return_poisson)\n        else:\n            loc_1_cars_return_prob[i] = poisson.pmf(\n                loc_1_cars_return_list[i], return_poisson\n            )\n\n    # print(\"Loc 1 cars return prob\", loc_1_cars_return_prob)\n    # print(\"Sum: \", sum(np.unique(loc_1_cars_return_prob)))\n    final_cars = loc_1_cars_list - loc_1_cars_rent_list + loc_1_cars_return_list\n    final_prob = loc_1_cars_rent_prob * loc_1_cars_return_prob\n    # Sum up all final_prob for each unique end state\n    sums = np.bincount(final_cars, weights=final_prob)\n    # sum(sums)\n    return sums\n\n\nfinal_states = [1, 1, 2, 2, 3, 3]\nfinal_prob = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\nunique_final_probs = np.bincount(final_states, weights=final_prob)\nunique_final_probs\n\narray([0. , 0.3, 0.7, 1.1])\n\n\n\npoisson.pmf(np.arange(4), 3)\n\narray([0.04978707, 0.14936121, 0.22404181, 0.22404181])\n\n\n\npoisson.cdf(3, 3)\n\n0.6472318887822313\n\n\n\nnp.arange(4)\n\narray([0, 1, 2, 3])\n\n\n\n# Parameters\nloc_1_cars = 20000\nloc_2_cars = 2\nnum_cars = 100000\n\n# Generate possible values for rent and returned\nrent = np.arange(loc_1_cars + 1)\nreturned = np.arange(num_cars - loc_1_cars + 1)\n\n# Create meshgrid for all combinations\nrent_grid, returned_grid = np.meshgrid(rent, returned, indexing=\"ij\")\n\n# Calculate remaining cars\nremaining_grid = loc_1_cars - rent_grid\n\n# Calculate sum of remaining and returned cars\nsum_grid = remaining_grid + returned_grid\n\n# Filter combinations where sum exceeds num_cars\nvalid_combinations = sum_grid &lt;= num_cars\n\n# Extract valid rent and returned combinations\nvalid_rent = rent_grid[valid_combinations]\nvalid_returned = returned_grid[valid_combinations]\n\nprint(\"Valid rent combinations:\", valid_rent)\nprint(\"Valid returned combinations:\", valid_returned)\n\nValid rent combinations: [    0     0     0 ... 20000 20000 20000]\nValid returned combinations: [    0     1     2 ... 79998 79999 80000]\n\n\n\n# %%timeit\n\nloc_1_cars = 20000\nloc_2_cars = 2\nnum_cars = 100000\n\nfor rent in range(loc_1_cars + 1):\n    remaining = loc_1_cars - rent\n    for returned in range(num_cars - remaining + 1):\n        1 + 1\n\n\nExercise 4.7\n\nclass JacksCarRental:\n    def __init__(\n        self,\n        cars=20,\n        cost_per_car_moved=2,\n        profit_per_car=10,\n        loc_1_rent=3,\n        loc_2_rent=4,\n        loc_1_ret=3,\n        loc_2_ret=2,\n        max_action=5,\n        theta=10,\n        gamma=0.9,\n    ):\n        self.cars = cars\n        self.cost_per_car_moved = cost_per_car_moved\n        self.profit_per_car = profit_per_car\n        self.loc_1_rent = loc_1_rent\n        self.loc_2_rent = loc_2_rent\n        self.loc_1_ret = loc_1_ret\n        self.loc_2_ret = loc_2_ret\n        self.max_action = max_action\n        self.theta = theta\n        self.gamma = gamma\n\n        (\n            self.loc_1_rent_dist,\n            self.loc_2_rent_dist,\n            self.loc_1_ret_dist,\n            self.loc_2_ret_dist,\n        ) = (\n            poisson(self.loc_1_rent),\n            poisson(self.loc_2_rent),\n            poisson(self.loc_1_ret),\n            poisson(self.loc_2_ret),\n        )\n\n        self.states = list(itertools.product(range(cars + 1), range(cars + 1)))\n\n    def policy_iteration(self):\n        policy_stable = False\n\n        state_values = np.zeros((self.cars + 1, self.cars + 1), dtype=int)\n        policy = np.zeros((self.cars + 1, self.cars + 1), dtype=int)\n        policies = []\n        iterations = 0\n        while not policy_stable:\n            new_state_values = self.policy_evaluation(state_values, policy)\n            policy, policy_stable = self.policy_improvement(new_state_values, policy)\n            policies.append(policy.copy())\n            iterations += 1\n            print(f\"Policy iteration {iterations}\")\n        return state_values, policy, policies\n\n    def policy_evaluation(self, state_values, policy):\n        biggest_change = np.inf\n        iterations = 0\n        while biggest_change &gt; self.theta:\n            biggest_change = 0\n\n            num_states = 0\n            average_times = []\n            for i, j in self.states:\n                num_states += 1\n                action = policy[i][j]\n                original_value = state_values[i][j]\n\n                start = time.time()\n                expected_reward = self.expected_reward((i, j), action)\n                expected_value = self.expected_value_2((i, j), action, state_values)\n                # state_values[i][j] = self.expected_value((i, j), action, state_values)\n                state_values[i][j] = expected_reward + expected_value\n                end = time.time()\n                average_times.append(end - start)\n                biggest_change = max(\n                    biggest_change, abs(original_value - state_values[i][j])\n                )\n\n            iterations += 1\n            print(f\"Policy evaluation {iterations}\")\n\n        return state_values\n\n    def policy_improvement(self, state_values, policy):\n        policy_stable = True\n        for i, j in self.states:\n            old_action = policy[i][j]\n            possible_actions = list(range(-j, i + 1))\n            best_action = None\n            best_value = -np.inf\n            if possible_actions == []:\n                continue\n            for action in possible_actions:\n                action_value = self.expected_value((i, j), action, state_values)\n                best_value = max(best_value, action_value)\n                if action_value == best_value:\n                    best_action = action\n            policy[i][j] = best_action\n            if old_action != best_action:\n                policy_stable = False\n        return policy, policy_stable\n\n    def expected_reward(self, state, action):\n        (loc_1_cars, loc_2_cars) = state\n        loc_1_cars -= action\n        loc_2_cars += action\n\n        cost = -(self.cost_per_car_moved * abs(action))\n\n        # Find expected value of returned cards for loc 1\n        loc_1_req = sum(self.loc_1_rent_dist.pmf(i) * i for i in range(loc_1_cars + 1))\n        loc_1_surplus = (1 - self.loc_1_rent_dist.cdf(loc_1_cars)) * loc_1_cars\n        expected_loc_1_returned = loc_1_req + loc_1_surplus\n\n        # Find expected value of returned cards for loc 2\n        loc_2_req = sum(self.loc_2_rent_dist.pmf(i) * i for i in range(loc_2_cars + 1))\n        loc_2_surplus = (1 - self.loc_2_rent_dist.cdf(loc_2_cars)) * loc_2_cars\n        expected_loc_2_returned = loc_2_req + loc_2_surplus\n\n        return cost + self.profit_per_car * (\n            expected_loc_1_returned + expected_loc_2_returned\n        )\n\n    def expected_value_2(self, state, action, state_values):\n        (loc_1_cars, loc_2_cars) = state\n        loc_1_cars -= action\n        loc_2_cars += action\n\n        loc_1_state_dist = final_state_prob_dist(\n            self.cars, loc_1_cars, self.loc_1_rent, self.loc_1_ret\n        )\n        loc_2_state_dist = final_state_prob_dist(\n            self.cars, loc_2_cars, self.loc_2_rent, self.loc_2_ret\n        )\n\n        expected_value = 0\n        for loc_1_cars, loc_1_prob in enumerate(loc_1_state_dist):\n            for loc_2_cars, loc_2_prob in enumerate(loc_2_state_dist):\n                # reward = self.expected_reward((loc_1_cars, loc_2_cars), action)\n                expected_value += (\n                    loc_1_prob\n                    * loc_2_prob\n                    * (\n                        # reward + self.gamma * state_values[loc_1_cars][loc_2_cars]\n                        self.gamma * state_values[loc_1_cars][loc_2_cars]\n                    )\n                )\n        return expected_value\n\n    def expected_value(self, state, action, state_values):\n        expected_value = 0\n\n        (loc_1_cars, loc_2_cars) = state\n\n        # Cars moved at night\n        loc_1_cars -= action\n        loc_2_cars += action\n        cost = 2 * abs(action)\n\n        sum_prob_loc_1_rented = 0\n        for loc_1_rented in range(loc_1_cars + 1):\n            if loc_1_rented == loc_1_cars:\n                p_loc_1_rented = 1 - sum_prob_loc_1_rented\n            else:\n                p_loc_1_rented = self.poisson(self.loc_1_rent, loc_1_rented)\n                sum_prob_loc_1_rented += p_loc_1_rented\n            sum_prob_loc_2_rented = 0\n            for loc_2_rented in range(loc_2_cars + 1):\n                if loc_2_rented == loc_2_cars:\n                    p_loc_2_rented = 1 - sum_prob_loc_2_rented\n                else:\n                    p_loc_2_rented = self.poisson(self.loc_2_rent, loc_2_rented)\n                    sum_prob_loc_2_rented += p_loc_2_rented\n\n                sum_prob_loc_1_returned = 0\n                max_returnable_1 = self.cars - (loc_1_cars - loc_1_rented)\n                for loc_1_returned in range(max_returnable_1 + 1):\n                    if loc_1_returned == max_returnable_1:\n                        p_loc_1_returned = 1 - sum_prob_loc_1_returned\n                    else:\n                        p_loc_1_returned = self.poisson(self.loc_1_ret, loc_1_returned)\n                        sum_prob_loc_1_returned += p_loc_1_returned\n\n                    max_returnable_2 = self.cars - (loc_2_cars - loc_2_rented)\n                    sum_prob_loc_2_returned = 0\n                    for loc_2_returned in range(max_returnable_2 + 1):\n                        if loc_2_returned == max_returnable_2:\n                            p_loc_2_returned = 1 - sum_prob_loc_2_returned\n                        else:\n                            p_loc_2_returned = self.poisson(\n                                self.loc_2_ret, loc_2_returned\n                            )\n                            sum_prob_loc_2_returned += p_loc_2_returned\n\n                        reward = 10 * (loc_1_rented + loc_2_rented)\n                        next_loc_1_cars = loc_1_cars + loc_1_returned - loc_1_rented\n                        next_loc_2_cars = loc_2_cars + loc_2_returned - loc_2_rented\n                        value = (\n                            reward\n                            - cost\n                            + self.gamma\n                            * state_values[next_loc_1_cars][next_loc_2_cars]\n                        )\n                        probability = (\n                            p_loc_1_rented\n                            * p_loc_2_rented\n                            * p_loc_1_returned\n                            * p_loc_2_returned\n                        )\n\n                        expected_value += probability * value\n\n        return expected_value\n\n\njacks_car_rental = JacksCarRental(cars=20)\nstate_values, policy, policies = jacks_car_rental.policy_iteration()\nprint(\"State values:\", state_values)\nprint(\"Policy:\", policy)\n\nPolicy evaluation 1\nPolicy evaluation 2\n\n\nKeyboardInterrupt: \n\n\n\nimport time\n\n\ndef transition(state, action):\n    (loc_1_cars, loc_2_cars) = state\n    loc_1_cars -= action\n    loc_2_cars += action\n    return (loc_1_cars, loc_2_cars)\n\n\ndef actions(state):\n    (loc_1_cars, loc_2_cars) = state\n    return range(loc_1_cars, -loc_2_cars - 1)\n\n\n@functools.cache\ndef poisson(lamb, n):\n    \"\"\"\n    :param lamb: lambda parameter of poisson distribution, rate\n    :param n: n variable of poisson distribution, number of occurrences\n    :return: probability of the event\n    \"\"\"\n    return (lamb**n) * math.exp(-lamb) / math.factorial(n)\n\n\n# Expected value of taking action from state\ndef expected_value(state, action, state_values, discount):\n    expected_value = 0\n    (loc_1_cars, loc_2_cars) = state\n\n    # Cars moved at night\n    loc_1_cars -= action\n    loc_2_cars += action\n    cost = 2 * abs(action)\n\n    sum_prob_loc_1_rented = 0\n    for loc_1_rented in range(loc_1_cars + 1):\n        if loc_1_rented == loc_1_cars:\n            p_loc_1_rented = 1 - sum_prob_loc_1_rented\n        else:\n            p_loc_1_rented = poisson(3, loc_1_rented)\n            sum_prob_loc_1_rented += p_loc_1_rented\n        sum_prob_loc_2_rented = 0\n        for loc_2_rented in range(loc_2_cars + 1):\n            if loc_2_rented == loc_2_cars:\n                p_loc_2_rented = 1 - sum_prob_loc_2_rented\n            else:\n                p_loc_2_rented = poisson(4, loc_2_rented)\n                sum_prob_loc_2_rented += p_loc_2_rented\n\n            sum_prob_loc_1_returned = 0\n            max_returnable_1 = 20 - (loc_1_cars - loc_1_rented)\n            for loc_1_returned in range(max_returnable_1 + 1):\n                if loc_1_returned == max_returnable_1:\n                    p_loc_1_returned = 1 - sum_prob_loc_1_returned\n                else:\n                    p_loc_1_returned = poisson(3, loc_1_returned)\n                    sum_prob_loc_1_returned += p_loc_1_returned\n\n                max_returnable_2 = 20 - (loc_2_cars - loc_2_rented)\n                sum_prob_loc_2_returned = 0\n                for loc_2_returned in range(max_returnable_2 + 1):\n                    if loc_2_returned == max_returnable_2:\n                        p_loc_2_returned = 1 - sum_prob_loc_2_returned\n                    else:\n                        p_loc_2_returned = poisson(2, loc_2_returned)\n                        sum_prob_loc_2_returned += p_loc_2_returned\n\n                    reward = 10 * (loc_1_rented + loc_2_rented)\n                    next_loc_1_cars = loc_1_cars + loc_1_returned - loc_1_rented\n                    next_loc_2_cars = loc_2_cars + loc_2_returned - loc_2_rented\n                    value = (\n                        reward\n                        - cost\n                        + discount * state_values[next_loc_1_cars][next_loc_2_cars]\n                    )\n                    probability = (\n                        p_loc_1_rented\n                        * p_loc_2_rented\n                        * p_loc_1_returned\n                        * p_loc_2_returned\n                    )\n\n                    expected_value += probability * value\n\n    return expected_value\n\n\ndef policy_evaluation(states, state_values, policy, discount, theta):\n    # Big number\n    biggest_change = np.inf\n    num_iterations = 0\n    while biggest_change &gt; theta:\n        num_iterations += 1\n        print(f\"Policy evaluation iteration {num_iterations}\")\n        biggest_change = 0\n\n        num_states = 0\n        average_times = []\n        for i, j in states:\n            num_states += 1\n            # print(f\"State {num_states}\")\n            action = policy[i][j]\n            # Reward from taking the action + discounted value of the next state\n            original_value = state_values[i][j]\n            # Sum of probability of particular state reward pair multiplied by reward and next state value\n\n            # Calculate time taken for one expected value:\n            start = time.time()\n            state_values[i][j] = expected_value((i, j), action, state_values, discount)\n            end = time.time()\n            average_times.append(end - start)\n            # print(f\"Time taken for one expected value: {end - start}\")\n            biggest_change = max(\n                biggest_change, abs(original_value - state_values[i][j])\n            )\n        print(\"Average time: \", np.mean(average_times))\n        print(\"Biggest change: \", biggest_change)\n\n    return state_values\n\n\ndef policy_improvement(states, state_values, policy, discount):\n    policy_stable = True\n    for i, j in states:\n        old_action = policy[i][j]\n        possible_actions = actions((i, j))\n        best_action = None\n        best_value = -np.inf\n        for action in possible_actions:\n            # action_value = reward(s, action) + discount * state_values[next_state]\n            action_value = expected_value((i, j), action, state_values, discount)\n            best_value = max(best_value, action_value)\n            if action_value == best_value:\n                best_action = action\n        policy[i][j] = best_action\n        if old_action != best_action:\n            policy_stable = False\n        return policy, policy_stable\n\n\ndef policy_iteration(states, state_values, policy, discount=0.9, theta=10):\n    policy_stable = False\n    while not policy_stable:\n        new_state_values = policy_evaluation(\n            states, state_values, policy, discount, theta\n        )\n        policy, policy_stable = policy_improvement(\n            states, new_state_values, policy, discount\n        )\n\n    return state_values, policy\n\n\n# num_states = 21\nnum_states = 11\nstates = list(itertools.product(range(num_states), range(num_states)))\nstate_values = np.zeros((num_states, num_states), dtype=int)\npolicy = np.zeros((num_states, num_states), dtype=int)\ndiscount = 0.9\ntheta = 10\n\n# policy_evaluation(state_values, policy, discount, theta)\n\nPolicy evaluation iteration 1\n\n\nIndexError: index 11 is out of bounds for axis 0 with size 11\n\n\n\n# state_values, policy = policy_iteration()\n\nNameError: name 'policy_iteration' is not defined\n\n\n\n\nExercise 4.9\n\n# Generalized value_iteration function\n\n# The \"dynamics\" of the problem:\n# states: A list of states. Each state is represented as a tuple\n# state_values: A dictionary mapping states to their values\n# policy: A dictionary mapping states to actions\n# actions(s): The possible actions in state s\n# reward(s, a): The reward for taking action a in state s\n# transition(s, a): Returns mapping of possible next states to their probabilities, given action a was taken in state s\n\n\ndef value_iteration(\n    states: np.ndarray,\n    state_values: np.ndarray,\n    policy: list[dict[int, int]],\n    actions: Callable[[int], np.ndarray],\n    reward: Callable[[int, int], int],\n    transition: Callable[[int, int], dict[int, float]],\n    gamma=1,\n    theta=0.1,\n):\n    def action_evaluation(s, action):\n        transitions = transition(s, action)\n        value = sum(\n            probability * (reward(s, action) + gamma * state_values[next_state])\n            for next_state, probability in transitions.items()\n        )\n        return value\n\n    sweeps = []\n    sweep_count = 0\n    biggest_change = np.inf\n    while biggest_change &gt; theta:\n        biggest_change = 0\n        for s in states:\n            original_value = state_values[s]\n            best_value = -np.inf\n            possible_actions = actions(s)\n            for action in possible_actions:\n                value = action_evaluation(s, action)\n                if value &gt; best_value:\n                    best_value = value\n            state_values[s] = best_value\n            biggest_change = max(biggest_change, abs(original_value - state_values[s]))\n        if sweep_count &lt; 3:\n            sweeps.append(state_values.copy())\n        sweep_count += 1\n    sweeps.append(state_values.copy())\n\n    # Output deterministic policy pi and value function v\n    for s in states:\n        possible_actions = actions(s)\n        best_value = -np.inf\n        best_action = None\n        for a in possible_actions:\n            value = action_evaluation(s, a)\n            if value &gt; best_value:\n                best_value = value\n                best_action = a\n        policy[s] = best_action\n\n    return state_values, policy, sweeps\n\n\n# State ranges from 1 to 99\nstates = np.arange(1, 100)\n\n# State values ranges from 0 to 100.\n# We include 0 and 100 in state_values as dummy terminal states.\nstate_values = np.zeros(101)\nstate_values[100] = 1\n\npolicy = {s: 0 for s in states}\nactions = lambda s: np.arange(1, np.minimum(s, 100 - s) + 1)\n\n# Reward is 0 for this problem\nreward = lambda s, a: 0\n\n# Generate three transition functions, with differing probabilities\nresults = []\n# for ph in [0.25, 0.4, 0.55]:\n#     transition = lambda s, a: {s + a: ph, s - a: 1 - ph}\n#     state_values_result, policy_result, sweeps = value_iteration(\n#         states=states,\n#         state_values=state_values,\n#         policy=policy,\n#         actions=actions,\n#         reward=reward,\n#         transition=transition,\n#         theta=1e-10,\n#     )\n#     print(policy_result)\n#     results.append((sweeps.copy(), policy_result.copy()))\n\n{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 8, 18: 7, 19: 19, 20: 20, 21: 4, 22: 22, 23: 2, 24: 1, 25: 25, 26: 1, 27: 2, 28: 3, 29: 4, 30: 5, 31: 6, 32: 7, 33: 33, 34: 9, 35: 10, 36: 11, 37: 12, 38: 38, 39: 39, 40: 10, 41: 41, 42: 8, 43: 43, 44: 44, 45: 5, 46: 4, 47: 47, 48: 2, 49: 1, 50: 50, 51: 1, 52: 2, 53: 3, 54: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 65: 10, 66: 16, 67: 8, 68: 18, 69: 19, 70: 20, 71: 4, 72: 22, 73: 2, 74: 1, 75: 25, 76: 1, 77: 2, 78: 22, 79: 4, 80: 5, 81: 6, 82: 7, 83: 8, 84: 9, 85: 10, 86: 11, 87: 12, 88: 12, 89: 11, 90: 10, 91: 9, 92: 8, 93: 7, 94: 6, 95: 5, 96: 4, 97: 3, 98: 2, 99: 1}\n{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 1, 27: 2, 28: 3, 29: 4, 30: 5, 31: 6, 32: 7, 33: 8, 34: 34, 35: 10, 36: 11, 37: 12, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 1, 52: 2, 53: 3, 54: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 65: 15, 66: 16, 67: 17, 68: 18, 69: 19, 70: 20, 71: 21, 72: 22, 73: 23, 74: 24, 75: 25, 76: 1, 77: 2, 78: 3, 79: 4, 80: 5, 81: 6, 82: 7, 83: 8, 84: 9, 85: 10, 86: 11, 87: 12, 88: 12, 89: 11, 90: 10, 91: 9, 92: 8, 93: 7, 94: 6, 95: 5, 96: 4, 97: 3, 98: 2, 99: 1}\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1}\n\n\n\n# for sweeps, policy in results:\n#     plt.figure()\n#     for sweep in sweeps:\n#         plt.plot(sweep)\n#     plt.title(\"Value Iteration\")\n\n#     plt.figure()\n#     print(policy)\n#     plt.step(policy.keys(), policy.values())\n#     plt.title(\"Policy\")\n\n{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 8, 18: 7, 19: 19, 20: 20, 21: 4, 22: 22, 23: 2, 24: 1, 25: 25, 26: 1, 27: 2, 28: 3, 29: 4, 30: 5, 31: 6, 32: 7, 33: 33, 34: 9, 35: 10, 36: 11, 37: 12, 38: 38, 39: 39, 40: 10, 41: 41, 42: 8, 43: 43, 44: 44, 45: 5, 46: 4, 47: 47, 48: 2, 49: 1, 50: 50, 51: 1, 52: 2, 53: 3, 54: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 65: 10, 66: 16, 67: 8, 68: 18, 69: 19, 70: 20, 71: 4, 72: 22, 73: 2, 74: 1, 75: 25, 76: 1, 77: 2, 78: 22, 79: 4, 80: 5, 81: 6, 82: 7, 83: 8, 84: 9, 85: 10, 86: 11, 87: 12, 88: 12, 89: 11, 90: 10, 91: 9, 92: 8, 93: 7, 94: 6, 95: 5, 96: 4, 97: 3, 98: 2, 99: 1}\n{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 1, 27: 2, 28: 3, 29: 4, 30: 5, 31: 6, 32: 7, 33: 8, 34: 34, 35: 10, 36: 11, 37: 12, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 1, 52: 2, 53: 3, 54: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 65: 15, 66: 16, 67: 17, 68: 18, 69: 19, 70: 20, 71: 21, 72: 22, 73: 23, 74: 24, 75: 25, 76: 1, 77: 2, 78: 3, 79: 4, 80: 5, 81: 6, 82: 7, 83: 8, 84: 9, 85: 10, 86: 11, 87: 12, 88: 12, 89: 11, 90: 10, 91: 9, 92: 8, 93: 7, 94: 6, 95: 5, 96: 4, 97: 3, 98: 2, 99: 1}\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolicy = results[0][1]\nplt.step(policy.keys(), policy.values())"
  },
  {
    "objectID": "drafts/test_2.html",
    "href": "drafts/test_2.html",
    "title": "Kim Young Jin",
    "section": "",
    "text": "import os  # noqa\nimport pickle  # noqa\nimport numpy as np  # noqa\n\n\nclass environment:\n    def steps(self, state, action):\n        self.state_win = state + action\n        self.state_lose = state - action\n        return self.state_win, self.state_lose\n\n\nclass agent:\n    def __init__(self):\n        self.states = 101\n        self.v = np.zeros((self.states), dtype=\"float\")\n        self.v[-1] = 1\n        self.stable = False\n        self.theta = 0.001\n\n    def possible_actions(self, state):\n        actions = np.arange(1, min(state, 100 - state) + 1, 1)\n        return actions\n\n    def value_iteration(self, ph, env):\n        delta = self.theta\n        sweeps = []\n        sweep = 0\n        while delta &gt;= self.theta:\n            old_values = self.v.copy()\n            for state in range(1, self.states - 1):\n                values = []\n                actions = self.possible_actions(state)\n                for a in actions:\n                    state_win, state_lose = env.steps(state, a)\n                    value = (ph * self.v[state_win]) + ((1 - ph) * self.v[state_lose])\n                    values.append(value)\n\n                values = np.array(values)\n                self.v[state] = np.amax(\n                    values\n                )  # update value function with value maximising action\n            sweeps.append(old_values)\n            sweep += 1\n            delta = np.max(np.abs(old_values - self.v))\n\n            print(f\"Probability of Heads: {ph}\")\n            print(f\"End of sweep: {sweep}, Delta = {delta}\")\n\n        return self.v, sweeps\n\n    def find_policy(self, v, env, ph):\n        stakes = []\n        for state in range(1, self.states - 1):\n            a_vals = []\n            actions = self.possible_actions(state)\n            for a in actions:\n                state_win, state_lose = env.steps(state, a)\n                a_val = (ph * v[state_win]) + ((1 - ph) * v[state_lose])\n                a_vals.append(a_val)\n\n            a_arr = np.array(a_vals)\n            a_max = np.argmax(a_arr) + 1\n            stakes.append(a_max)\n\n        return stakes\n\n\n    env = environment()\n    agent = agent()\n\n    phs = [0.25, 0.55]\n\n    final_vf = {}\n    sweeps = {}\n    policy = {}\n\n    for ph in phs:\n        v_func, sweep = agent.value_iteration(ph, env)\n        stakes = agent.find_policy(v_func, env, ph)\n        final_vf[str(ph)] = v_func\n        sweeps[str(ph)] = sweep\n        policy[str(ph)] = stakes\n\n    with open(\"data/final_v_functions.pickle\", \"wb\") as f:\n        pickle.dump(final_vf, f)\n\n    with open(\"data/sweeps.pickle\", \"wb\") as f:\n        pickle.dump(sweeps, f)\n\n    with open(\"data/policy.pickle\", \"wb\") as f:\n        pickle.dump(policy, f)\n\n\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwith open(\"data/final_v_functions.pickle\", \"rb\") as f:\n    v_functions = pickle.load(f)\n\nwith open(\"data/sweeps.pickle\", \"rb\") as f:\n    sweeps = pickle.load(f)\n\nwith open(\"data/policy.pickle\", \"rb\") as f:\n    policy = pickle.load(f)\n\nph = [\"0.25\", \"0.55\"]\n\nfor p in ph:\n    sweep_p = []\n    policy_p = []\n    for arr in sweeps[p]:\n        sweep_p.append(arr.flatten())\n    for arr in policy[p]:\n        policy_p.append(arr)\n\n    x = np.arange(0, 101, 1)\n    x_pol = np.arange(1, 100, 1)\n\n    fig = plt.figure(figsize=(15, 10))\n    fig.suptitle(f\"$P_h({p})$\", fontsize=16)\n\n    ax1 = fig.add_subplot(211)\n    ax1.title.set_text(\"Value Function Approximation per Sweep\")\n\n    i = 1\n    for arr in sweep_p:\n        ax1.plot(x, arr, label=\"sweep: {}\".format(i))\n        i += 1\n\n    ax1.legend()\n\n    ax2 = fig.add_subplot(212)\n    ax2.title.set_text(\"Optimal Policy\")\n    ax2.plot(x_pol, policy_p)\n\n    plt.savefig(\"images/p_{}.png\".format(p), dpi=300)"
  },
  {
    "objectID": "posts/what-to-do-about-the-future.html",
    "href": "posts/what-to-do-about-the-future.html",
    "title": "What to do about the future",
    "section": "",
    "text": "I think I am often sucseptible to drinking the Silicon Valley Kool-Aid - build, build, build. Build and grind. Make tons of money, and then post snarky Tweets. Treat science and technology as the primary force of good in the world. As long as people pay for and use your product, you are fulfiling an unmet need. Making the world a better place, one line of code at a time.\nIt’s easy to consume this narrative and live out your life according to its doctrine.\nBut there is a more cautious line of thinking that is emerging, and it implores us to look beyond the first-order effect of our technology. It comes under many labels, but the general idea is clear: technology can have many unintended consequences, the nature of which is not at all clear to us right now, before the technology has been created. We should tread carefully.\nI used to dismiss any attempts at this sort of thinking as unnecessary “doomerism”, because the world is a complex place so of course there will be some third-order effects. But that is true of anything we do. Well-intended actions sometimes do more harm than good, and vice versa. The public discouse has also mainly revolved around AGI.\nBut I think the conversation is much bigger than that, and it really requires some introspection on the very nature of our humanity. Because there will be an increasing amount of technology that can lead us astray, distract us from dealing with the “pain of being”. There will be technology that replicates aspects of our intelligence. There will be technology that lets us augment our worst insecurities. There will be room for our insecurities to manifest themselves in our children, by giving them genetic upgrades before birth.\nThis enthralls but also terrifies me. It is like we are starting to play with the essence of the human condition, without understanding its substance. For one - what is consciousness?\nIf we agree that our consciousness is what makes us human, then how will we know when we should start treating our AI counterparts as humans as well?\n\nNeurotechnology\nThe “Supersensorium”\nWhat are the things we should ultimately “value”"
  },
  {
    "objectID": "posts/comparison.html",
    "href": "posts/comparison.html",
    "title": "Comparison",
    "section": "",
    "text": "Instead of asking why someone is better than you - which forces you to look at the uncontrollable - ask how he might be better. And if the answers to the how are outside of your control, accept the difference in leagues. But it’s very rare that there is nothing at all you can do to emulate a characteristic you admire. People in general underestimate the number of things that are in their control.\nThe interplay of mimetic desire and social media is causing a mass convergence in the world. Different “groups” find their respective groups online. Tech people go to tech Twitter. Fitness and beauty go to Instagram. And so on. Everyone sees what everyone else is doing, and tries to emulate them, causing a convergence."
  },
  {
    "objectID": "drafts/chapter-6.html",
    "href": "drafts/chapter-6.html",
    "title": "Sutton’s Reinforcement Learning Chapter 6: Temporal-Difference Learning",
    "section": "",
    "text": "import sys  # noqa\n\nimport gymnasium as gym  # noqa\nimport numpy as np  # noqa\nfrom gymnasium import spaces  # noqa\nimport matplotlib.pyplot as plt  # noqa\n\n\nclass WindyGridWorldEnv(gym.Env):\n    \"\"\"Creates the Windy GridWorld Environment\"\"\"\n\n    def __init__(\n        self,\n        GRID_HEIGHT=7,\n        GRID_WIDTH=10,\n        WIND=[0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n        START_STATE=(3, 0),\n        GOAL_STATE=(3, 7),\n        REWARD=-1,\n    ):\n        self.grid_height = GRID_HEIGHT\n        self.grid_width = GRID_WIDTH\n        self.wind = WIND\n        self.start_state = START_STATE\n        self.goal_state = GOAL_STATE\n        self.reward = REWARD\n        self.action_space = spaces.Discrete(4)\n        self.observation_space = spaces.Tuple(\n            (spaces.Discrete(self.grid_height), spaces.Discrete(self.grid_width))\n        )\n        self.actions = {\n            \"U\": 0,  # up\n            \"R\": 1,  # right\n            \"D\": 2,  # down\n            \"L\": 3,\n        }  # left\n\n        # set up destinations for each action in each state\n        self.action_destination = np.empty(\n            (self.grid_height, self.grid_width), dtype=dict\n        )\n        for i in range(0, self.grid_height):\n            for j in range(0, self.grid_width):\n                destination = dict()\n                destination[self.actions[\"U\"]] = (max(i - 1 - self.wind[j], 0), j)\n                destination[self.actions[\"D\"]] = (\n                    max(min(i + 1 - self.wind[j], self.grid_height - 1), 0),\n                    j,\n                )\n                destination[self.actions[\"L\"]] = (\n                    max(i - self.wind[j], 0),\n                    max(j - 1, 0),\n                )\n                destination[self.actions[\"R\"]] = (\n                    max(i - self.wind[j], 0),\n                    min(j + 1, self.grid_width - 1),\n                )\n                self.action_destination[i, j] = destination\n        self.nA = len(self.actions)\n\n    def step(self, action):\n        \"\"\"\n        Parameters\n        ----------\n        action : 0 = Up, 1 = Right, 2 = Down, 3 = Left\n\n        Returns\n        -------\n        ob, reward, episode_over, info : tuple\n            ob (object) :\n                 Agent current position in the grid.\n            reward (float) :\n                 Reward is -1 at every step.\n            episode_over (bool) :\n                 True if the agent reaches the goal, False otherwise.\n            info (dict) :\n                 Contains no additional information.\n        \"\"\"\n        assert self.action_space.contains(action)\n        self.observation = self.action_destination[self.observation][action]\n        if self.observation == self.goal_state:\n            return self.observation, -1, True, {}\n        return self.observation, -1, False, {}\n\n    def reset(self):\n        \"\"\"resets the agent position back to the starting position\"\"\"\n        self.observation = self.start_state\n        return self.observation\n\n    def render(self, mode=\"human\", close=False):\n        \"\"\"Renders the environment. Code borrowed and then modified\n        from\n        https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py\"\"\"\n        outfile = sys.stdout\n        nS = self.grid_height * self.grid_width\n        shape = (self.grid_height, self.grid_width)\n\n        outboard = \"\"\n        for y in range(-1, self.grid_height + 1):\n            outline = \"\"\n            for x in range(-1, self.grid_width + 1):\n                position = (y, x)\n                if self.observation == position:\n                    output = \"X\"\n                elif position == self.goal_state:\n                    output = \"G\"\n                elif position == self.start_state:\n                    output = \"S\"\n                elif x in {-1, self.grid_width} or y in {-1, self.grid_height}:\n                    output = \"#\"\n                else:\n                    output = \" \"\n\n                if position[1] == shape[1]:\n                    output += \"\\n\"\n                outline += output\n            outboard += outline\n        outboard += \"\\n\"\n        outfile.write(outboard)\n\n    def seed(self, seed=None):\n        pass\n\n\n# env = WindyGridWorldEnv()\n# env.reset()\n# done = False\n\n# while not done:\n#     env.render()\n#     action = env.action_space.sample()\n#     observation, reward, done, info = env.step(action)\n# env.render()\n\n\n# Sarsa algorithm\n\n\nclass SARSA:\n    def __init__(self, env, alpha=0.5, gamma=1, epsilon=0.1):\n        self.env = env\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n\n        # Sarsa initialization\n        self.Q = np.zeros((env.grid_height, env.grid_width, env.nA))\n\n    def epsilon_greedy(self, state):\n        if np.random.rand() &lt; self.epsilon:\n            return self.env.action_space.sample()\n        else:\n            return np.argmax(self.Q[state[0], state[1]])\n\n    def learn(self, time_steps):\n        t = 0\n        episodes = 0\n        episodes_count = []\n        while t &lt; time_steps:\n            observation = self.env.reset()\n            done = False\n            action = self.epsilon_greedy(observation)\n\n            while not done:\n                # self.env.render()\n\n                # Take action according to epsilon-greedy policy\n                observation_prime, reward, done, info = self.env.step(action)\n                action_prime = self.epsilon_greedy(observation_prime)\n\n                # We have obtained SARSA, now do SARSA update\n                self.Q[observation[0], observation[1], action] += self.alpha * (\n                    reward\n                    + self.gamma\n                    * self.Q[observation_prime[0], observation_prime[1], action_prime]\n                    - self.Q[observation[0], observation[1], action]\n                )\n\n                observation = observation_prime\n                action = action_prime\n\n                t += 1\n                episodes_count.append(episodes)\n\n            episodes += 1\n        # self.env.render()\n        return episodes_count\n\n\n# env = WindyGridWorldEnv()\n# sarsa = SARSA(env)\n# episodes_count = sarsa.learn(time_steps=8000)\n# plt.plot(episodes_count)\n\n\n\n\n\n\n\n\n\n# q_policy = np.argmax(sarsa.Q, axis=2)\n# # Replace 0, 1, 2, 3 with U, R, D, L\n# policy_arrows = np.empty((env.grid_height, env.grid_width), dtype=str)\n# for i in range(0, env.grid_height):\n#     for j in range(0, env.grid_width):\n#         policy_arrows[i, j] = list(env.actions.keys())[q_policy[i, j]]\n# print(policy_arrows)\n\n[['D' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'D']\n ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'U' 'R' 'D']\n ['R' 'R' 'R' 'R' 'U' 'L' 'R' 'R' 'U' 'D']\n ['R' 'R' 'R' 'R' 'R' 'D' 'R' 'U' 'R' 'D']\n ['R' 'R' 'R' 'R' 'R' 'R' 'U' 'D' 'L' 'L']\n ['D' 'D' 'R' 'R' 'R' 'U' 'U' 'U' 'L' 'L']\n ['R' 'R' 'R' 'R' 'U' 'U' 'U' 'U' 'U' 'L']]\n\n\n\nExercise 6.9"
  },
  {
    "objectID": "posts/sy-garte.html",
    "href": "posts/sy-garte.html",
    "title": "Notes from “The Work of His Hands” by Sy Garte",
    "section": "",
    "text": "“The Work of His Hands” is a book by Sy Garte, PhD, a biochemist atheist turned devout Christian. Sy approaches the question of God’s existence from a practicing scientist’s point of view. Here are some interesting points he makes on why science is compatible with Christianity.\n\nThe Quantum “Observer Effect”\nIn Quantum Theory, we know that some things exist in “multiple states” until some kind of observation happens. This is the classical Schroedinger’s cat experiment, where the cat is both alive and dead until the box is opened and the cat observed. I had always assumed that this was due to the act of observing - observing something requires exerting some physical effect on it, for example by firing photons at it. Apparently, that’s not true. It is literally the fact that a consciousness observes something that changes its outcome.\nThis is pretty mind-blowing. What kind of mathematics must be behind this? We don’t even understand what consciousness is. How did physicists define an “observer”?\nAnyway, Sy points to this and phenomena such as Quantum Entanglement as examples of how reality is irrational. I’m not sure if we can exactly call it “irrational” per se - just unexpected. Science is about disovering new laws behind how the world works. Of course we will uncover weird things. That’s the point!\nBut the point on consciousness warrants further investigation. Did there need to be an initial “observer” at the beginning of time for reality to exist?\n\n\nThe Three Origins\nThere are three origins that science does not have answers to. The origin of the universe, the origin of life, and the origin of humans.\nFrom the discovery that the Universe is constantly expanding, we arrived at the conclusion that all matter must have been concentrated at a single point from which the Big Bang happened. To a layman, the Big Bang might give off a reassuring veneer of good science and rigorous physics, but really the simplicity of the phrase betrays how little we do know about it. It’s as if a physicist, flabbergasted by the idea of a tremendous explosion of matter and energy just happening out of nowhere, went: “Bollocks! I’ll just call it a Big Bang because that’s all I know about it. That it was Big and that there was a Bang.”\nThe origin of life is even more interesting. Evolution only makes sense in the context of cells that can accurately replicate themselves. Only then can natural selection work its wonders. But the mechanism behind accurate replication - the tremendously complex process of DNA replication - could not itself have evolved, since it is required for evolution. Same goes for DNA transcription, which is required for evolution since it translates the genotype into phenotypes which can be selected for by the environment.\nThere are hypotheses that something like “chemical evolution” happened, where just the right chemicals were at just the right place for the formation of the first cell, or LUCA (Last Universal Common Ancestor). But they remain as hypotheses, and there are some who say it is impossible for such a spontaneous formation to happen.\nFinally comes the origin of humans. Humans possess a staggeringly sophisticated consciousness, one capable of perceiving the tiniest tinges of emotion. Evolution seems to have overshot for us humans - we are on a different plane of existence compared to even our closest evolutionary cousins. We do not know how humans evolved to be this way. We do not know how we developed such powerful brains and complex emotions. What kind of selective pressure could have caused caused humans to evolve such traits? We do not know.\n\n\nIntelligent Design?\nThere are arguments that point to the natural laws, and say: “It is remarkable that these laws are so consistent and logical across all of reality. Why should they be? How could these laws have resulted in humans, if the laws were just ‘arbitrary’, and not designed by a divine intelligence?”\nI’m not sure how much philosophical rigor is in this sentiment. Yes, we can look at reality and wonder at its beauty, but that in itself is just a feeling of awe. Why couldn’t it be that reality just really is the way it is? Why does “orderliness” necessarily have to be designed by an intelligence?\nFor me, a more convincing line of thought starts by acknowledging that all our standards are set by what we observe in the world around us. We think the natural laws are somehow implausibly logical and consistent because we have seen firsthand how things left alone can decay and go to nothingness. Using the only frame of reference we have, our conscious experience, we see that most things in the world tend towards entropy and chaos. Unless, of course, a human - an intelligent designer - intervenes. And so we look at the inventions of our species and intuitively grasp that they came into reality because of us. It becomes hard to look at our Biology and not draw from that same intuition - that there has to be an intelligent designer behind that as well.\nCan complexity and order spontaneously arise from chaos? Can intelligence arise spontaneously without a higher intelligence designing it?\nInstead of “Quis custodiet ipsos custodes?” (who will watch the watchmen), we can ask “Quis fabricat horologium fabricatoris”. Who maketh the watchmaker?"
  },
  {
    "objectID": "learning/nielson-deep-learning/mnist.html",
    "href": "learning/nielson-deep-learning/mnist.html",
    "title": "Neural network and SGD from scratch",
    "section": "",
    "text": "We write a neural network for the iris dataset from scratch, copying generously from Karpathy’s micrograd. We implement SGD for the training loop. Originally I wanted to replicate the MNIST network in Michael Nielson’s Neural Networks and Deep Learning, but training it took too long.\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport math\nimport random\nfrom graphviz import Digraph\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris  # noqa\n\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), \"+\")\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), \"*\")\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(\n            other, (int, float)\n        ), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self,), f\"**{other}\")\n\n        def _backward():\n            self.grad += other * (self.data ** (other - 1)) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __rmul__(self, other):  # other * self\n        return self * other\n\n    def __truediv__(self, other):  # self / other\n        return self * other**-1\n\n    def __neg__(self):  # -self\n        return self * -1\n\n    def __sub__(self, other):  # self - other\n        return self + (-other)\n\n    def __rsub__(self, other):  # other - self\n        return other + (-self)\n\n    def __radd__(self, other):  # other + self\n        return self + other\n\n    def tanh(self):\n        x = self.data\n        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n        out = Value(t, (self,), \"tanh\")\n\n        def _backward():\n            self.grad += (1 - t**2) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self,), \"exp\")\n\n        def _backward():\n            self.grad += out.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def sigmoid(self):\n        return Value(1) / (1 + (-self).exp())\n\n    def log(self):\n        x = self.data\n        out = Value(math.log(x), (self,), \"log\")\n\n        def _backward():\n            self.grad += (1 / x) * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(random.uniform(-1, 1))\n\n    def __call__(self, x):\n        # w * x + b\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.sigmoid()\n        return out\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n    def parameters(self):\n        return [p for neuron in self.neurons for p in neuron.parameters()]\n\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\nclass SGD:\n    def __init__(self, nn, training_data, epochs, mini_batch_size, alpha):\n        self.nn = nn\n        self.training_data = training_data\n        self.epochs = epochs\n        self.mini_batch_size = mini_batch_size\n        self.alpha = alpha\n\n    def train(self):\n        for epoch in range(self.epochs):\n            random.shuffle(self.training_data)\n            mini_batches = [\n                self.training_data[k : k + self.mini_batch_size]\n                for k in range(0, len(self.training_data), self.mini_batch_size)\n            ]\n            epoch_loss = 0\n            for mini_batch in mini_batches:\n                loss = self.update_mini_batch(mini_batch)\n                epoch_loss += loss\n            print(f\"Epoch {epoch} complete with loss {epoch_loss}\")\n\n    def update_mini_batch(self, mini_batch):\n        # Forward pass\n        cost_sum = Value(0)\n        for x, y in mini_batch:\n            pred = self.nn(x)\n            if not isinstance(pred, list):\n                pred = [pred]\n            if not isinstance(y, list):\n                y = [y]\n            squared_error = sum((pred - label) ** 2 for pred, label in zip(pred, y))\n            cost_sum += squared_error\n        final_cost = cost_sum * (1.0 / (2 * self.mini_batch_size))\n\n        # Backward pass\n        parameters = self.nn.parameters()\n        for p in parameters:\n            p.grad = 0.0\n        final_cost.backward()\n\n        # Update weights and biases (parameters) of the model\n        for p in parameters:\n            p.data = p.data - self.alpha * p.grad\n\n        return final_cost.data\n\n\nx, y = load_iris(return_X_y=True)\n# One-hot encode the ys\nx = x.tolist()\ny = np.eye(3)[y].tolist()\ndataset = list(zip(x, y))\n\n\nmlp = MLP(4, [3, 3])\ntrainer = SGD(mlp, dataset, 150, 10, 0.5)\ntrainer.train()\n\nEpoch 0 complete with loss 5.228109863636578\nEpoch 1 complete with loss 4.811590527515836\nEpoch 2 complete with loss 4.571267491177194\nEpoch 3 complete with loss 4.3829861375233214\nEpoch 4 complete with loss 4.197913007139084\nEpoch 5 complete with loss 4.038164541182581\nEpoch 6 complete with loss 3.872290225148203\nEpoch 7 complete with loss 3.7068483386813447\nEpoch 8 complete with loss 3.5886929561863177\nEpoch 9 complete with loss 3.477471591816729\nEpoch 10 complete with loss 3.3726498536848086\nEpoch 11 complete with loss 3.3036160667866366\nEpoch 12 complete with loss 3.243014975791261\nEpoch 13 complete with loss 3.1598249158928664\nEpoch 14 complete with loss 3.108013472082669\nEpoch 15 complete with loss 3.0575501646923327\nEpoch 16 complete with loss 3.0128253161335805\nEpoch 17 complete with loss 2.9899654206296997\nEpoch 18 complete with loss 2.955836597378141\nEpoch 19 complete with loss 2.9267921080388213\nEpoch 20 complete with loss 2.9016587867839307\nEpoch 21 complete with loss 2.8681642530749394\nEpoch 22 complete with loss 2.8606096728389447\nEpoch 23 complete with loss 2.834266994928878\nEpoch 24 complete with loss 2.8046677795456088\nEpoch 25 complete with loss 2.791329804175733\nEpoch 26 complete with loss 2.7822197841959526\nEpoch 27 complete with loss 2.7542529806715854\nEpoch 28 complete with loss 2.748385622854117\nEpoch 29 complete with loss 2.7382969269286637\nEpoch 30 complete with loss 2.7259479717010637\nEpoch 31 complete with loss 2.7241156029486273\nEpoch 32 complete with loss 2.716380143046377\nEpoch 33 complete with loss 2.6974342318592397\nEpoch 34 complete with loss 2.705415435792862\nEpoch 35 complete with loss 2.684288367477209\nEpoch 36 complete with loss 2.670980469632754\nEpoch 37 complete with loss 2.6805757855525045\nEpoch 38 complete with loss 2.657954832034161\nEpoch 39 complete with loss 2.64030290091774\nEpoch 40 complete with loss 2.6506611606648565\nEpoch 41 complete with loss 2.6341131864461462\nEpoch 42 complete with loss 2.6389327236528217\nEpoch 43 complete with loss 2.658157255119224\nEpoch 44 complete with loss 2.630797927144032\nEpoch 45 complete with loss 2.6329285445810564\nEpoch 46 complete with loss 2.622100270705101\nEpoch 47 complete with loss 2.6189472505952627\nEpoch 48 complete with loss 2.6111506273809186\nEpoch 49 complete with loss 2.61483406926503\nEpoch 50 complete with loss 2.6098673840710576\nEpoch 51 complete with loss 2.580657262524024\nEpoch 52 complete with loss 2.5733024075782307\nEpoch 53 complete with loss 2.610888659454109\nEpoch 54 complete with loss 2.577303476238778\nEpoch 55 complete with loss 2.57697819606955\nEpoch 56 complete with loss 2.5611336453945763\nEpoch 57 complete with loss 2.5683212672640083\nEpoch 58 complete with loss 2.5546988456671924\nEpoch 59 complete with loss 2.6061177817069003\nEpoch 60 complete with loss 2.566269792542318\nEpoch 61 complete with loss 2.5581808898703944\nEpoch 62 complete with loss 2.541730748879436\nEpoch 63 complete with loss 2.5721121857107145\nEpoch 64 complete with loss 2.5388815951221813\nEpoch 65 complete with loss 2.5603894485519123\nEpoch 66 complete with loss 2.5240267588683505\nEpoch 67 complete with loss 2.5106422711898055\nEpoch 68 complete with loss 2.536292391324494\nEpoch 69 complete with loss 2.542833260411167\nEpoch 70 complete with loss 2.5866726144730645\nEpoch 71 complete with loss 2.500508807987302\nEpoch 72 complete with loss 2.4991181693767524\nEpoch 73 complete with loss 2.466521693412633\nEpoch 74 complete with loss 2.491675284642612\nEpoch 75 complete with loss 2.4582728459269796\nEpoch 76 complete with loss 2.543997645600261\nEpoch 77 complete with loss 2.5143626507390833\nEpoch 78 complete with loss 2.477686776613415\nEpoch 79 complete with loss 2.4692707546955805\nEpoch 80 complete with loss 2.4437285301473652\nEpoch 81 complete with loss 2.4452414022300277\nEpoch 82 complete with loss 2.550917058313376\nEpoch 83 complete with loss 2.4162815305296466\nEpoch 84 complete with loss 2.4249044135608044\nEpoch 85 complete with loss 2.4337177765246456\nEpoch 86 complete with loss 2.4117656410547705\nEpoch 87 complete with loss 2.366356444296752\nEpoch 88 complete with loss 2.36608388121565\nEpoch 89 complete with loss 2.360644997007254\nEpoch 90 complete with loss 2.3559306556490296\nEpoch 91 complete with loss 2.390512291618162\nEpoch 92 complete with loss 2.3841548299816377\nEpoch 93 complete with loss 2.4245216952651822\nEpoch 94 complete with loss 2.398266995973643\nEpoch 95 complete with loss 2.490096436730295\nEpoch 96 complete with loss 2.3256276985777484\nEpoch 97 complete with loss 2.3413305531696986\nEpoch 98 complete with loss 2.255239420693627\nEpoch 99 complete with loss 2.253498801877654\nEpoch 100 complete with loss 2.30457496528148\nEpoch 101 complete with loss 2.1386931518199903\nEpoch 102 complete with loss 2.1494432157565044\nEpoch 103 complete with loss 2.1562197789521362\nEpoch 104 complete with loss 1.9681237492874832\nEpoch 105 complete with loss 1.998383459657808\nEpoch 106 complete with loss 2.0916873114669725\nEpoch 107 complete with loss 1.9370039049463867\nEpoch 108 complete with loss 1.8082898404713235\nEpoch 109 complete with loss 1.812653036751982\nEpoch 110 complete with loss 1.6724278732455857\nEpoch 111 complete with loss 1.7614745256654614\nEpoch 112 complete with loss 1.6270490775055995\nEpoch 113 complete with loss 1.7125566344208305\nEpoch 114 complete with loss 1.6009986233077105\nEpoch 115 complete with loss 1.4982073887693281\nEpoch 116 complete with loss 1.475530739883789\nEpoch 117 complete with loss 1.3555408884653557\nEpoch 118 complete with loss 1.3344281774473197\nEpoch 119 complete with loss 1.6540645857991139\nEpoch 120 complete with loss 1.3785113902670765\nEpoch 121 complete with loss 1.3377315457682462\nEpoch 122 complete with loss 1.2055035299720998\nEpoch 123 complete with loss 1.5007868813897596\nEpoch 124 complete with loss 1.2124374709105101\nEpoch 125 complete with loss 1.1751378331535662\nEpoch 126 complete with loss 1.1468478852234119\nEpoch 127 complete with loss 1.1009705359398845\nEpoch 128 complete with loss 1.2528156212580208\nEpoch 129 complete with loss 1.052100028086615\nEpoch 130 complete with loss 1.0359952882633874\nEpoch 131 complete with loss 0.9593625855542283\nEpoch 132 complete with loss 1.0620169971479294\nEpoch 133 complete with loss 1.1674910174008557\nEpoch 134 complete with loss 1.1150473251730098\nEpoch 135 complete with loss 0.8601662220541026\nEpoch 136 complete with loss 0.9902350074041312\nEpoch 137 complete with loss 0.8462934959065628\nEpoch 138 complete with loss 0.8506323739609574\nEpoch 139 complete with loss 0.8572265828640774\nEpoch 140 complete with loss 0.8723254165562639\nEpoch 141 complete with loss 0.8375449909042699\nEpoch 142 complete with loss 0.7832565729525689\nEpoch 143 complete with loss 0.8490069738978305\nEpoch 144 complete with loss 0.9592682936105128\nEpoch 145 complete with loss 0.7700136543305345\nEpoch 146 complete with loss 0.715320302076721\nEpoch 147 complete with loss 0.7109569006887824\nEpoch 148 complete with loss 0.7440288649753813\nEpoch 149 complete with loss 0.865334355902204\n\n\n\n# Evaluate model accuracy on dataset\ncorrect = 0\nfor x, y in dataset:\n    pred = mlp(x)\n    pred = [p.data for p in pred]\n    if np.argmax(pred) == np.argmax(y):\n        correct += 1\nprint(f\"Accuracy: {correct / len(dataset)}\")\n\nAccuracy: 0.9533333333333334\n\n\nPlaying around with Michael Nielson’s implementation.\n\nimport mnist_loader\n\n\n\"\"\"\nnetwork.py\n~~~~~~~~~~\n\nA module to implement the stochastic gradient descent learning\nalgorithm for a feedforward neural network.  Gradients are calculated\nusing backpropagation.  Note that I have focused on making the code\nsimple, easily readable, and easily modifiable.  It is not optimized,\nand omits many desirable features.\n\"\"\"\n\n\nclass Network(object):\n    def __init__(self, sizes):\n        \"\"\"The list ``sizes`` contains the number of neurons in the\n        respective layers of the network.  For example, if the list\n        was [2, 3, 1] then it would be a three-layer network, with the\n        first layer containing 2 neurons, the second layer 3 neurons,\n        and the third layer 1 neuron.  The biases and weights for the\n        network are initialized randomly, using a Gaussian\n        distribution with mean 0, and variance 1.  Note that the first\n        layer is assumed to be an input layer, and by convention we\n        won't set any biases for those neurons, since biases are only\n        ever used in computing the outputs from later layers.\"\"\"\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n\n    def feedforward(self, a):\n        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a) + b)\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n        \"\"\"Train the neural network using mini-batch stochastic\n        gradient descent.  The ``training_data`` is a list of tuples\n        ``(x, y)`` representing the training inputs and the desired\n        outputs.  The other non-optional parameters are\n        self-explanatory.  If ``test_data`` is provided then the\n        network will be evaluated against the test data after each\n        epoch, and partial progress printed out.  This is useful for\n        tracking progress, but slows things down substantially.\"\"\"\n        if test_data:\n            n_test = len(test_data)\n        n = len(training_data)\n        for j in range(epochs):\n            random.shuffle(training_data)\n            mini_batches = [\n                training_data[k : k + mini_batch_size]\n                for k in range(0, n, mini_batch_size)\n            ]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n            else:\n                print(f\"Epoch {j} complete\")\n\n    def update_mini_batch(self, mini_batch, eta):\n        \"\"\"Update the network's weights and biases by applying\n        gradient descent using backpropagation to a single mini batch.\n        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n        is the learning rate.\"\"\"\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [\n            w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)\n        ]\n        self.biases = [\n            b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)\n        ]\n\n    def backprop(self, x, y):\n        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n        gradient for the cost function C_x.  ``nabla_b`` and\n        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n        to ``self.biases`` and ``self.weights``.\"\"\"\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        # feedforward\n        activation = x\n        activations = [x]  # list to store all the activations, layer by layer\n        zs = []  # list to store all the z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation) + b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        # backward pass\n        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        # Note that the variable l in the loop below is used a little\n        # differently to the notation in Chapter 2 of the book.  Here,\n        # l = 1 means the last layer of neurons, l = 2 is the\n        # second-last layer, and so on.  It's a renumbering of the\n        # scheme in the book, used here to take advantage of the fact\n        # that Python can use negative indices in lists.\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n        return (nabla_b, nabla_w)\n\n    def evaluate(self, test_data):\n        \"\"\"Return the number of test inputs for which the neural\n        network outputs the correct result. Note that the neural\n        network's output is assumed to be the index of whichever\n        neuron in the final layer has the highest activation.\"\"\"\n        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n        return sum(int(x == y) for (x, y) in test_results)\n\n    def cost_derivative(self, output_activations, y):\n        \"\"\"Return the vector of partial derivatives \\partial C_x /\n        \\partial a for the output activations.\"\"\"\n        return output_activations - y\n\n\n#### Miscellaneous functions\ndef sigmoid(z):\n    \"\"\"The sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-z))\n\n\ndef sigmoid_prime(z):\n    \"\"\"Derivative of the sigmoid function.\"\"\"\n    return sigmoid(z) * (1 - sigmoid(z))\n\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n# Convert zip iterators to lists\ntraining_data = list(training_data)\nvalidation_data = list(validation_data)\ntest_data = list(test_data)\n\n\nnet = Network([784, 30, 10])\n\n\nnet.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n\nEpoch 0: 9050 / 10000\nEpoch 1: 9187 / 10000\nEpoch 2: 9289 / 10000\nEpoch 3: 9295 / 10000\nEpoch 4: 9311 / 10000\nEpoch 5: 9328 / 10000\nEpoch 6: 9369 / 10000\nEpoch 7: 9329 / 10000\nEpoch 8: 9415 / 10000\nEpoch 9: 9393 / 10000\nEpoch 10: 9377 / 10000\nEpoch 11: 9438 / 10000\nEpoch 12: 9435 / 10000\nEpoch 13: 9388 / 10000\nEpoch 14: 9445 / 10000\nEpoch 15: 9437 / 10000\nEpoch 16: 9437 / 10000\nEpoch 17: 9430 / 10000\nEpoch 18: 9452 / 10000\nEpoch 19: 9447 / 10000\nEpoch 20: 9407 / 10000\nEpoch 21: 9468 / 10000\nEpoch 22: 9455 / 10000\nEpoch 23: 9450 / 10000\nEpoch 24: 9459 / 10000\nEpoch 25: 9430 / 10000\nEpoch 26: 9449 / 10000\nEpoch 27: 9476 / 10000\nEpoch 28: 9450 / 10000\nEpoch 29: 9467 / 10000"
  },
  {
    "objectID": "learning/exploding-gradients.html",
    "href": "learning/exploding-gradients.html",
    "title": "‘Insight’ in the pigeon: antecedents and determinants of an intelligent performance",
    "section": "",
    "text": "What are exploding gradients? Why do they happen?"
  },
  {
    "objectID": "drafts/existence-of-god.html",
    "href": "drafts/existence-of-god.html",
    "title": "Arguments for and against God",
    "section": "",
    "text": "A list of arguments and counter-arguments for the existence of God. For personal reference.\nMuch of the material for this first draft comes from Bertrand Russell’s “Why I Am Not a Christian”; what follows is a rough summary of his points.\n\nThe First Cause Argument\ngoes like this: everything has a cause. So there must have been a first cause that existed before any other cause. And that first cause, existing for eternity into the past and future, is God. This is one of those arguments that sounds plausible at first, but on closer inspection there is a logical error. If the premise is that everything has a cause, then God shuold have a cause too. If then one argues that no, there exists some things which do not have a cause, one of which is God, we can just as easily ask - why cannot the Universe be without cause? Either way, the existence of a God is not necessitated."
  },
  {
    "objectID": "learning/nielson-deep-learning/chapter-3.html",
    "href": "learning/nielson-deep-learning/chapter-3.html",
    "title": "Replication of some chapter 3 results",
    "section": "",
    "text": "from micrograd import Neuron, Value\nimport matplotlib.pyplot as plt\nimport mnist_loader  # noqa\nimport torch\nimport random\nimport torch.nn as nn  # noqa\n\n\n# We are inputting 1 and want an output of 0\n\nfor w, b in [(0.6, 0.9), (2.0, 2.0)]:\n    neuron = Neuron(1)\n    neuron.w = [Value(w)]\n    neuron.b = Value(b)\n\n    losses = []\n    for i in range(300):\n        pred = neuron([1.0])\n        loss = (pred - Value(0)) ** 2\n        losses.append(loss.data)\n        for p in neuron.parameters():\n            p.grad = 0\n        loss.backward()\n        for p in neuron.parameters():\n            p.data = p.data - 0.15 * p.grad\n\n    # Add label\n    plt.plot(losses, label=f\"w={w}, b={b}\")\n\n# Display legend\nplt.legend()\n\n\n\n\n\n\n\n\nWe can see that the neuron learns very slowly in the beginning, when predicted output and actual output are the furthest apart. That shouldn’t be the case. The model should learn faster the further away our predictions are from the actual value. We can use cross-entropy loss to remedy this.\n\n# We are inputting 1 and want an output of 0\nfor w, b in [(0.6, 0.9), (2.0, 2.0)]:\n    neuron = Neuron(1)\n    neuron.w = [Value(w)]\n    neuron.b = Value(b)\n\n    losses = []\n    for i in range(300):\n        pred = neuron([1.0])\n        # Log loss\n        loss = -Value(0) * pred.log() - (1 - Value(0)) * (1 - pred).log()\n        losses.append(loss.data)\n        for p in neuron.parameters():\n            p.grad = 0\n        loss.backward()\n        for p in neuron.parameters():\n            p.data = p.data - 0.15 * p.grad\n\n    # Add label\n    plt.plot(losses, label=f\"w={w}, b={b}\")\n\n# Display legend\nplt.legend()\n\n\n\n\n\n\n\n\nWe can see that the problem faced earlier has been fixed.\n\nRegularization\nNext, we try running some experiments in pytorch to test the effects of regularization.\n\nimport network2\n\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n# Convert zip iterators to lists\ntraining_data = list(training_data)\nvalidation_data = list(validation_data)\ntest_data = list(test_data)\n\n\nnet = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\nnet.large_weight_initializer()\nnet.SGD(\n    training_data[:1000],\n    400,\n    10,\n    0.5,\n    evaluation_data=test_data,\n    monitor_evaluation_accuracy=True,\n    monitor_training_cost=True,\n)\n\nEpoch 0 training complete\nCost on training data: 1.9157265803666599\nAccuracy on evaluation data: 5336 / 10000\nEpoch 1 training complete\nCost on training data: 1.4233085154513578\nAccuracy on evaluation data: 6481 / 10000\nEpoch 2 training complete\nCost on training data: 1.140610742579291\nAccuracy on evaluation data: 7009 / 10000\nEpoch 3 training complete\nCost on training data: 0.9673757700312096\nAccuracy on evaluation data: 7278 / 10000\nEpoch 4 training complete\nCost on training data: 0.8219014211106735\nAccuracy on evaluation data: 7518 / 10000\nEpoch 5 training complete\nCost on training data: 0.7009795108242491\nAccuracy on evaluation data: 7621 / 10000\nEpoch 6 training complete\nCost on training data: 0.6210731479476688\nAccuracy on evaluation data: 7749 / 10000\nEpoch 7 training complete\nCost on training data: 0.5649499666413196\nAccuracy on evaluation data: 7741 / 10000\nEpoch 8 training complete\nCost on training data: 0.4967537573921582\nAccuracy on evaluation data: 7913 / 10000\nEpoch 9 training complete\nCost on training data: 0.4456219552812798\nAccuracy on evaluation data: 7890 / 10000\nEpoch 10 training complete\nCost on training data: 0.39901535860495846\nAccuracy on evaluation data: 7977 / 10000\nEpoch 11 training complete\nCost on training data: 0.36146464324057803\nAccuracy on evaluation data: 7938 / 10000\nEpoch 12 training complete\nCost on training data: 0.3545073579313579\nAccuracy on evaluation data: 7913 / 10000\nEpoch 13 training complete\nCost on training data: 0.3087184094679663\nAccuracy on evaluation data: 8051 / 10000\nEpoch 14 training complete\nCost on training data: 0.2797147873623128\nAccuracy on evaluation data: 8115 / 10000\nEpoch 15 training complete\nCost on training data: 0.2525545832423591\nAccuracy on evaluation data: 8133 / 10000\nEpoch 16 training complete\nCost on training data: 0.2380955942826455\nAccuracy on evaluation data: 8096 / 10000\nEpoch 17 training complete\nCost on training data: 0.22646415893206082\nAccuracy on evaluation data: 8108 / 10000\nEpoch 18 training complete\nCost on training data: 0.20683416913226718\nAccuracy on evaluation data: 8138 / 10000\nEpoch 19 training complete\nCost on training data: 0.19279183350114984\nAccuracy on evaluation data: 8127 / 10000\nEpoch 20 training complete\nCost on training data: 0.1866037912492253\nAccuracy on evaluation data: 8145 / 10000\nEpoch 21 training complete\nCost on training data: 0.17397172190964394\nAccuracy on evaluation data: 8126 / 10000\nEpoch 22 training complete\nCost on training data: 0.16138418421655057\nAccuracy on evaluation data: 8127 / 10000\nEpoch 23 training complete\nCost on training data: 0.15258576699007645\nAccuracy on evaluation data: 8159 / 10000\nEpoch 24 training complete\nCost on training data: 0.14559589206163331\nAccuracy on evaluation data: 8160 / 10000\nEpoch 25 training complete\nCost on training data: 0.13733030674883276\nAccuracy on evaluation data: 8176 / 10000\nEpoch 26 training complete\nCost on training data: 0.13286504240839203\nAccuracy on evaluation data: 8163 / 10000\nEpoch 27 training complete\nCost on training data: 0.12490575508106312\nAccuracy on evaluation data: 8185 / 10000\nEpoch 28 training complete\nCost on training data: 0.11897385331053419\nAccuracy on evaluation data: 8192 / 10000\nEpoch 29 training complete\nCost on training data: 0.11416971608153519\nAccuracy on evaluation data: 8190 / 10000\nEpoch 30 training complete\nCost on training data: 0.11001761063509545\nAccuracy on evaluation data: 8187 / 10000\nEpoch 31 training complete\nCost on training data: 0.10569402966303736\nAccuracy on evaluation data: 8204 / 10000\nEpoch 32 training complete\nCost on training data: 0.1004963762997966\nAccuracy on evaluation data: 8208 / 10000\nEpoch 33 training complete\nCost on training data: 0.09764112227836957\nAccuracy on evaluation data: 8180 / 10000\nEpoch 34 training complete\nCost on training data: 0.09319934983369883\nAccuracy on evaluation data: 8193 / 10000\nEpoch 35 training complete\nCost on training data: 0.08868491774835854\nAccuracy on evaluation data: 8206 / 10000\nEpoch 36 training complete\nCost on training data: 0.08595086097925883\nAccuracy on evaluation data: 8207 / 10000\nEpoch 37 training complete\nCost on training data: 0.08350067214392239\nAccuracy on evaluation data: 8198 / 10000\nEpoch 38 training complete\nCost on training data: 0.07869753555787781\nAccuracy on evaluation data: 8223 / 10000\nEpoch 39 training complete\nCost on training data: 0.07594792345922877\nAccuracy on evaluation data: 8217 / 10000\nEpoch 40 training complete\nCost on training data: 0.07346669031585368\nAccuracy on evaluation data: 8211 / 10000\nEpoch 41 training complete\nCost on training data: 0.0708839567187532\nAccuracy on evaluation data: 8221 / 10000\nEpoch 42 training complete\nCost on training data: 0.06831180691768503\nAccuracy on evaluation data: 8218 / 10000\nEpoch 43 training complete\nCost on training data: 0.06711002154610325\nAccuracy on evaluation data: 8236 / 10000\nEpoch 44 training complete\nCost on training data: 0.06405338526915547\nAccuracy on evaluation data: 8215 / 10000\nEpoch 45 training complete\nCost on training data: 0.06160733031909577\nAccuracy on evaluation data: 8232 / 10000\nEpoch 46 training complete\nCost on training data: 0.0596664860482716\nAccuracy on evaluation data: 8220 / 10000\nEpoch 47 training complete\nCost on training data: 0.05782624278400594\nAccuracy on evaluation data: 8228 / 10000\nEpoch 48 training complete\nCost on training data: 0.05620444418301355\nAccuracy on evaluation data: 8224 / 10000\nEpoch 49 training complete\nCost on training data: 0.05465434524748508\nAccuracy on evaluation data: 8224 / 10000\nEpoch 50 training complete\nCost on training data: 0.053404822360543044\nAccuracy on evaluation data: 8236 / 10000\nEpoch 51 training complete\nCost on training data: 0.051435838493430454\nAccuracy on evaluation data: 8220 / 10000\nEpoch 52 training complete\nCost on training data: 0.04995871675143006\nAccuracy on evaluation data: 8222 / 10000\nEpoch 53 training complete\nCost on training data: 0.049178511975360266\nAccuracy on evaluation data: 8232 / 10000\nEpoch 54 training complete\nCost on training data: 0.04761860543521613\nAccuracy on evaluation data: 8231 / 10000\nEpoch 55 training complete\nCost on training data: 0.045694060305748366\nAccuracy on evaluation data: 8236 / 10000\nEpoch 56 training complete\nCost on training data: 0.04471523134867321\nAccuracy on evaluation data: 8231 / 10000\nEpoch 57 training complete\nCost on training data: 0.04353039157951212\nAccuracy on evaluation data: 8234 / 10000\nEpoch 58 training complete\nCost on training data: 0.042274507860818385\nAccuracy on evaluation data: 8225 / 10000\nEpoch 59 training complete\nCost on training data: 0.04128529084302505\nAccuracy on evaluation data: 8232 / 10000\nEpoch 60 training complete\nCost on training data: 0.040370416208562215\nAccuracy on evaluation data: 8228 / 10000\nEpoch 61 training complete\nCost on training data: 0.03927593215490957\nAccuracy on evaluation data: 8234 / 10000\nEpoch 62 training complete\nCost on training data: 0.03849301313041344\nAccuracy on evaluation data: 8234 / 10000\nEpoch 63 training complete\nCost on training data: 0.03784698326101495\nAccuracy on evaluation data: 8241 / 10000\nEpoch 64 training complete\nCost on training data: 0.036834324900328834\nAccuracy on evaluation data: 8233 / 10000\nEpoch 65 training complete\nCost on training data: 0.036043977294875976\nAccuracy on evaluation data: 8234 / 10000\nEpoch 66 training complete\nCost on training data: 0.03532049078790165\nAccuracy on evaluation data: 8224 / 10000\nEpoch 67 training complete\nCost on training data: 0.03457195553625183\nAccuracy on evaluation data: 8234 / 10000\nEpoch 68 training complete\nCost on training data: 0.03396945778541524\nAccuracy on evaluation data: 8233 / 10000\nEpoch 69 training complete\nCost on training data: 0.033330702513722874\nAccuracy on evaluation data: 8235 / 10000\nEpoch 70 training complete\nCost on training data: 0.03261064682887517\nAccuracy on evaluation data: 8238 / 10000\nEpoch 71 training complete\nCost on training data: 0.03190709529638142\nAccuracy on evaluation data: 8241 / 10000\nEpoch 72 training complete\nCost on training data: 0.03152396913807024\nAccuracy on evaluation data: 8235 / 10000\nEpoch 73 training complete\nCost on training data: 0.03076630409249701\nAccuracy on evaluation data: 8237 / 10000\nEpoch 74 training complete\nCost on training data: 0.03023149206147074\nAccuracy on evaluation data: 8241 / 10000\nEpoch 75 training complete\nCost on training data: 0.029662199460815355\nAccuracy on evaluation data: 8235 / 10000\nEpoch 76 training complete\nCost on training data: 0.029187429963762536\nAccuracy on evaluation data: 8245 / 10000\nEpoch 77 training complete\nCost on training data: 0.028691340398401113\nAccuracy on evaluation data: 8236 / 10000\nEpoch 78 training complete\nCost on training data: 0.028152612115837413\nAccuracy on evaluation data: 8246 / 10000\nEpoch 79 training complete\nCost on training data: 0.02781349540189489\nAccuracy on evaluation data: 8244 / 10000\nEpoch 80 training complete\nCost on training data: 0.027304669134411593\nAccuracy on evaluation data: 8250 / 10000\nEpoch 81 training complete\nCost on training data: 0.026856672884772514\nAccuracy on evaluation data: 8242 / 10000\nEpoch 82 training complete\nCost on training data: 0.02639586449219745\nAccuracy on evaluation data: 8240 / 10000\nEpoch 83 training complete\nCost on training data: 0.02595796469117669\nAccuracy on evaluation data: 8243 / 10000\nEpoch 84 training complete\nCost on training data: 0.025575379849845398\nAccuracy on evaluation data: 8248 / 10000\nEpoch 85 training complete\nCost on training data: 0.025208345723867195\nAccuracy on evaluation data: 8247 / 10000\nEpoch 86 training complete\nCost on training data: 0.024804570034370994\nAccuracy on evaluation data: 8253 / 10000\nEpoch 87 training complete\nCost on training data: 0.024434796921913336\nAccuracy on evaluation data: 8251 / 10000\nEpoch 88 training complete\nCost on training data: 0.024103135768802273\nAccuracy on evaluation data: 8245 / 10000\nEpoch 89 training complete\nCost on training data: 0.02376708064146453\nAccuracy on evaluation data: 8255 / 10000\nEpoch 90 training complete\nCost on training data: 0.023434898537771164\nAccuracy on evaluation data: 8248 / 10000\nEpoch 91 training complete\nCost on training data: 0.023070941073319938\nAccuracy on evaluation data: 8236 / 10000\nEpoch 92 training complete\nCost on training data: 0.022738641763154207\nAccuracy on evaluation data: 8246 / 10000\nEpoch 93 training complete\nCost on training data: 0.022405034123781458\nAccuracy on evaluation data: 8248 / 10000\nEpoch 94 training complete\nCost on training data: 0.022116731931907656\nAccuracy on evaluation data: 8243 / 10000\nEpoch 95 training complete\nCost on training data: 0.021811183501928306\nAccuracy on evaluation data: 8240 / 10000\nEpoch 96 training complete\nCost on training data: 0.021542550179585496\nAccuracy on evaluation data: 8254 / 10000\nEpoch 97 training complete\nCost on training data: 0.0212327173512337\nAccuracy on evaluation data: 8247 / 10000\nEpoch 98 training complete\nCost on training data: 0.020955587687164007\nAccuracy on evaluation data: 8247 / 10000\nEpoch 99 training complete\nCost on training data: 0.02070081988279502\nAccuracy on evaluation data: 8243 / 10000\nEpoch 100 training complete\nCost on training data: 0.020464101024792077\nAccuracy on evaluation data: 8246 / 10000\nEpoch 101 training complete\nCost on training data: 0.020183177431458525\nAccuracy on evaluation data: 8241 / 10000\nEpoch 102 training complete\nCost on training data: 0.01990947657685897\nAccuracy on evaluation data: 8249 / 10000\nEpoch 103 training complete\nCost on training data: 0.019673095002806538\nAccuracy on evaluation data: 8235 / 10000\nEpoch 104 training complete\nCost on training data: 0.01945071220597259\nAccuracy on evaluation data: 8239 / 10000\nEpoch 105 training complete\nCost on training data: 0.019183272328162273\nAccuracy on evaluation data: 8240 / 10000\nEpoch 106 training complete\nCost on training data: 0.01896155749194874\nAccuracy on evaluation data: 8253 / 10000\nEpoch 107 training complete\nCost on training data: 0.018737027644866\nAccuracy on evaluation data: 8246 / 10000\nEpoch 108 training complete\nCost on training data: 0.01852884659595162\nAccuracy on evaluation data: 8245 / 10000\nEpoch 109 training complete\nCost on training data: 0.018326055157791767\nAccuracy on evaluation data: 8243 / 10000\nEpoch 110 training complete\nCost on training data: 0.01809023758910947\nAccuracy on evaluation data: 8248 / 10000\nEpoch 111 training complete\nCost on training data: 0.017889669482253606\nAccuracy on evaluation data: 8249 / 10000\nEpoch 112 training complete\nCost on training data: 0.017689472412734882\nAccuracy on evaluation data: 8244 / 10000\nEpoch 113 training complete\nCost on training data: 0.01748919382769571\nAccuracy on evaluation data: 8255 / 10000\nEpoch 114 training complete\nCost on training data: 0.01729858085590458\nAccuracy on evaluation data: 8250 / 10000\nEpoch 115 training complete\nCost on training data: 0.017126279843091046\nAccuracy on evaluation data: 8253 / 10000\nEpoch 116 training complete\nCost on training data: 0.016934041161704855\nAccuracy on evaluation data: 8253 / 10000\nEpoch 117 training complete\nCost on training data: 0.01674109717372653\nAccuracy on evaluation data: 8254 / 10000\nEpoch 118 training complete\nCost on training data: 0.01655962677581138\nAccuracy on evaluation data: 8245 / 10000\nEpoch 119 training complete\nCost on training data: 0.01639757707029055\nAccuracy on evaluation data: 8245 / 10000\nEpoch 120 training complete\nCost on training data: 0.01621579584970799\nAccuracy on evaluation data: 8253 / 10000\nEpoch 121 training complete\nCost on training data: 0.01605936365039096\nAccuracy on evaluation data: 8248 / 10000\nEpoch 122 training complete\nCost on training data: 0.01589021235514923\nAccuracy on evaluation data: 8256 / 10000\nEpoch 123 training complete\nCost on training data: 0.01572518834641798\nAccuracy on evaluation data: 8256 / 10000\nEpoch 124 training complete\nCost on training data: 0.015569978429947886\nAccuracy on evaluation data: 8247 / 10000\nEpoch 125 training complete\nCost on training data: 0.015407319294389883\nAccuracy on evaluation data: 8250 / 10000\nEpoch 126 training complete\nCost on training data: 0.015251287024967676\nAccuracy on evaluation data: 8252 / 10000\nEpoch 127 training complete\nCost on training data: 0.015108016005986392\nAccuracy on evaluation data: 8249 / 10000\nEpoch 128 training complete\nCost on training data: 0.014959029113495606\nAccuracy on evaluation data: 8251 / 10000\nEpoch 129 training complete\nCost on training data: 0.014821946295991628\nAccuracy on evaluation data: 8246 / 10000\nEpoch 130 training complete\nCost on training data: 0.014676781773088376\nAccuracy on evaluation data: 8250 / 10000\nEpoch 131 training complete\nCost on training data: 0.014547077394996423\nAccuracy on evaluation data: 8258 / 10000\nEpoch 132 training complete\nCost on training data: 0.014406349446049009\nAccuracy on evaluation data: 8260 / 10000\nEpoch 133 training complete\nCost on training data: 0.014274582991979683\nAccuracy on evaluation data: 8260 / 10000\nEpoch 134 training complete\nCost on training data: 0.01415044622375008\nAccuracy on evaluation data: 8266 / 10000\nEpoch 135 training complete\nCost on training data: 0.014012607992043\nAccuracy on evaluation data: 8260 / 10000\nEpoch 136 training complete\nCost on training data: 0.01389321569049412\nAccuracy on evaluation data: 8264 / 10000\nEpoch 137 training complete\nCost on training data: 0.013766664380162422\nAccuracy on evaluation data: 8251 / 10000\nEpoch 138 training complete\nCost on training data: 0.013653798693873268\nAccuracy on evaluation data: 8256 / 10000\nEpoch 139 training complete\nCost on training data: 0.01352502677394385\nAccuracy on evaluation data: 8261 / 10000\nEpoch 140 training complete\nCost on training data: 0.013409418654958455\nAccuracy on evaluation data: 8269 / 10000\nEpoch 141 training complete\nCost on training data: 0.013301199019208685\nAccuracy on evaluation data: 8268 / 10000\nEpoch 142 training complete\nCost on training data: 0.013180497870486077\nAccuracy on evaluation data: 8270 / 10000\nEpoch 143 training complete\nCost on training data: 0.013077935972272886\nAccuracy on evaluation data: 8268 / 10000\nEpoch 144 training complete\nCost on training data: 0.01296117787556624\nAccuracy on evaluation data: 8263 / 10000\nEpoch 145 training complete\nCost on training data: 0.012856299557458047\nAccuracy on evaluation data: 8260 / 10000\nEpoch 146 training complete\nCost on training data: 0.012751451883080927\nAccuracy on evaluation data: 8263 / 10000\nEpoch 147 training complete\nCost on training data: 0.012645519491190157\nAccuracy on evaluation data: 8268 / 10000\nEpoch 148 training complete\nCost on training data: 0.012545166466142342\nAccuracy on evaluation data: 8267 / 10000\nEpoch 149 training complete\nCost on training data: 0.012447508071661858\nAccuracy on evaluation data: 8262 / 10000\nEpoch 150 training complete\nCost on training data: 0.01234312271672617\nAccuracy on evaluation data: 8270 / 10000\nEpoch 151 training complete\nCost on training data: 0.012247949541335238\nAccuracy on evaluation data: 8265 / 10000\nEpoch 152 training complete\nCost on training data: 0.012152139704774108\nAccuracy on evaluation data: 8268 / 10000\nEpoch 153 training complete\nCost on training data: 0.012064314828212068\nAccuracy on evaluation data: 8265 / 10000\nEpoch 154 training complete\nCost on training data: 0.011966400675534445\nAccuracy on evaluation data: 8273 / 10000\nEpoch 155 training complete\nCost on training data: 0.011874914838803683\nAccuracy on evaluation data: 8271 / 10000\nEpoch 156 training complete\nCost on training data: 0.011782724235167658\nAccuracy on evaluation data: 8273 / 10000\nEpoch 157 training complete\nCost on training data: 0.0116966319554314\nAccuracy on evaluation data: 8270 / 10000\nEpoch 158 training complete\nCost on training data: 0.011610256479557782\nAccuracy on evaluation data: 8273 / 10000\nEpoch 159 training complete\nCost on training data: 0.011520105258763945\nAccuracy on evaluation data: 8271 / 10000\nEpoch 160 training complete\nCost on training data: 0.011439755468064356\nAccuracy on evaluation data: 8274 / 10000\nEpoch 161 training complete\nCost on training data: 0.011357655493793929\nAccuracy on evaluation data: 8268 / 10000\nEpoch 162 training complete\nCost on training data: 0.011283409102421852\nAccuracy on evaluation data: 8276 / 10000\nEpoch 163 training complete\nCost on training data: 0.01118870672659996\nAccuracy on evaluation data: 8269 / 10000\nEpoch 164 training complete\nCost on training data: 0.01110691983469005\nAccuracy on evaluation data: 8270 / 10000\nEpoch 165 training complete\nCost on training data: 0.011027037584351687\nAccuracy on evaluation data: 8268 / 10000\nEpoch 166 training complete\nCost on training data: 0.0109480839360458\nAccuracy on evaluation data: 8270 / 10000\nEpoch 167 training complete\nCost on training data: 0.010868154795374474\nAccuracy on evaluation data: 8272 / 10000\nEpoch 168 training complete\nCost on training data: 0.010794628856790733\nAccuracy on evaluation data: 8269 / 10000\nEpoch 169 training complete\nCost on training data: 0.010718020864434652\nAccuracy on evaluation data: 8275 / 10000\nEpoch 170 training complete\nCost on training data: 0.010640652611700866\nAccuracy on evaluation data: 8272 / 10000\nEpoch 171 training complete\nCost on training data: 0.010568505850681333\nAccuracy on evaluation data: 8276 / 10000\nEpoch 172 training complete\nCost on training data: 0.010495144281480608\nAccuracy on evaluation data: 8270 / 10000\nEpoch 173 training complete\nCost on training data: 0.010423459700472372\nAccuracy on evaluation data: 8276 / 10000\nEpoch 174 training complete\nCost on training data: 0.01035215785574867\nAccuracy on evaluation data: 8274 / 10000\nEpoch 175 training complete\nCost on training data: 0.010281598389256895\nAccuracy on evaluation data: 8273 / 10000\nEpoch 176 training complete\nCost on training data: 0.010213743367284523\nAccuracy on evaluation data: 8275 / 10000\nEpoch 177 training complete\nCost on training data: 0.010144601049374632\nAccuracy on evaluation data: 8273 / 10000\nEpoch 178 training complete\nCost on training data: 0.010075490292771319\nAccuracy on evaluation data: 8277 / 10000\nEpoch 179 training complete\nCost on training data: 0.010008294371522152\nAccuracy on evaluation data: 8277 / 10000\nEpoch 180 training complete\nCost on training data: 0.009942008567899162\nAccuracy on evaluation data: 8274 / 10000\nEpoch 181 training complete\nCost on training data: 0.009878198710323437\nAccuracy on evaluation data: 8274 / 10000\nEpoch 182 training complete\nCost on training data: 0.009813496530298511\nAccuracy on evaluation data: 8278 / 10000\nEpoch 183 training complete\nCost on training data: 0.009745402673000931\nAccuracy on evaluation data: 8280 / 10000\nEpoch 184 training complete\nCost on training data: 0.009683709284427975\nAccuracy on evaluation data: 8280 / 10000\nEpoch 185 training complete\nCost on training data: 0.009620426739445657\nAccuracy on evaluation data: 8279 / 10000\nEpoch 186 training complete\nCost on training data: 0.009557455763009055\nAccuracy on evaluation data: 8277 / 10000\nEpoch 187 training complete\nCost on training data: 0.00949805534433109\nAccuracy on evaluation data: 8277 / 10000\nEpoch 188 training complete\nCost on training data: 0.009438038102481235\nAccuracy on evaluation data: 8274 / 10000\nEpoch 189 training complete\nCost on training data: 0.009375645959080825\nAccuracy on evaluation data: 8278 / 10000\nEpoch 190 training complete\nCost on training data: 0.009315332442636128\nAccuracy on evaluation data: 8274 / 10000\nEpoch 191 training complete\nCost on training data: 0.0092563441875046\nAccuracy on evaluation data: 8277 / 10000\nEpoch 192 training complete\nCost on training data: 0.009197103377941369\nAccuracy on evaluation data: 8276 / 10000\nEpoch 193 training complete\nCost on training data: 0.00913937219535242\nAccuracy on evaluation data: 8276 / 10000\nEpoch 194 training complete\nCost on training data: 0.009083971707609098\nAccuracy on evaluation data: 8279 / 10000\nEpoch 195 training complete\nCost on training data: 0.00902560657927742\nAccuracy on evaluation data: 8280 / 10000\nEpoch 196 training complete\nCost on training data: 0.008970855935428792\nAccuracy on evaluation data: 8278 / 10000\nEpoch 197 training complete\nCost on training data: 0.008913885902286431\nAccuracy on evaluation data: 8274 / 10000\nEpoch 198 training complete\nCost on training data: 0.008859597006419211\nAccuracy on evaluation data: 8275 / 10000\nEpoch 199 training complete\nCost on training data: 0.008805134195939951\nAccuracy on evaluation data: 8277 / 10000\nEpoch 200 training complete\nCost on training data: 0.008753320472002191\nAccuracy on evaluation data: 8276 / 10000\nEpoch 201 training complete\nCost on training data: 0.008699193168168104\nAccuracy on evaluation data: 8276 / 10000\nEpoch 202 training complete\nCost on training data: 0.008646628714748658\nAccuracy on evaluation data: 8278 / 10000\nEpoch 203 training complete\nCost on training data: 0.008596697512196652\nAccuracy on evaluation data: 8282 / 10000\nEpoch 204 training complete\nCost on training data: 0.00854450520511543\nAccuracy on evaluation data: 8282 / 10000\nEpoch 205 training complete\nCost on training data: 0.008494266012365406\nAccuracy on evaluation data: 8281 / 10000\nEpoch 206 training complete\nCost on training data: 0.00844535278211304\nAccuracy on evaluation data: 8280 / 10000\nEpoch 207 training complete\nCost on training data: 0.008396311920896578\nAccuracy on evaluation data: 8280 / 10000\nEpoch 208 training complete\nCost on training data: 0.008348879753283944\nAccuracy on evaluation data: 8284 / 10000\nEpoch 209 training complete\nCost on training data: 0.00830025892889786\nAccuracy on evaluation data: 8278 / 10000\nEpoch 210 training complete\nCost on training data: 0.008253635655113563\nAccuracy on evaluation data: 8285 / 10000\nEpoch 211 training complete\nCost on training data: 0.008206207699730876\nAccuracy on evaluation data: 8282 / 10000\nEpoch 212 training complete\nCost on training data: 0.008159949913241944\nAccuracy on evaluation data: 8282 / 10000\nEpoch 213 training complete\nCost on training data: 0.008115473505970949\nAccuracy on evaluation data: 8279 / 10000\nEpoch 214 training complete\nCost on training data: 0.008070457971559742\nAccuracy on evaluation data: 8277 / 10000\nEpoch 215 training complete\nCost on training data: 0.008025276871048191\nAccuracy on evaluation data: 8281 / 10000\nEpoch 216 training complete\nCost on training data: 0.00798175692971702\nAccuracy on evaluation data: 8277 / 10000\nEpoch 217 training complete\nCost on training data: 0.007937385276313748\nAccuracy on evaluation data: 8286 / 10000\nEpoch 218 training complete\nCost on training data: 0.00789409095460702\nAccuracy on evaluation data: 8287 / 10000\nEpoch 219 training complete\nCost on training data: 0.007851964134100609\nAccuracy on evaluation data: 8284 / 10000\nEpoch 220 training complete\nCost on training data: 0.007810620015641798\nAccuracy on evaluation data: 8283 / 10000\nEpoch 221 training complete\nCost on training data: 0.007768607301514054\nAccuracy on evaluation data: 8280 / 10000\nEpoch 222 training complete\nCost on training data: 0.007727871181417616\nAccuracy on evaluation data: 8283 / 10000\nEpoch 223 training complete\nCost on training data: 0.00768651891795349\nAccuracy on evaluation data: 8284 / 10000\nEpoch 224 training complete\nCost on training data: 0.007645607395488211\nAccuracy on evaluation data: 8286 / 10000\nEpoch 225 training complete\nCost on training data: 0.007606159909610643\nAccuracy on evaluation data: 8283 / 10000\nEpoch 226 training complete\nCost on training data: 0.0075671767015055395\nAccuracy on evaluation data: 8285 / 10000\nEpoch 227 training complete\nCost on training data: 0.007528717093426312\nAccuracy on evaluation data: 8284 / 10000\nEpoch 228 training complete\nCost on training data: 0.007490312469768559\nAccuracy on evaluation data: 8286 / 10000\nEpoch 229 training complete\nCost on training data: 0.007451488872162558\nAccuracy on evaluation data: 8289 / 10000\nEpoch 230 training complete\nCost on training data: 0.0074139164053383805\nAccuracy on evaluation data: 8287 / 10000\nEpoch 231 training complete\nCost on training data: 0.00737624578357679\nAccuracy on evaluation data: 8288 / 10000\nEpoch 232 training complete\nCost on training data: 0.007339102825272781\nAccuracy on evaluation data: 8285 / 10000\nEpoch 233 training complete\nCost on training data: 0.007303170786097757\nAccuracy on evaluation data: 8284 / 10000\nEpoch 234 training complete\nCost on training data: 0.007266443130721465\nAccuracy on evaluation data: 8288 / 10000\nEpoch 235 training complete\nCost on training data: 0.0072306431437006245\nAccuracy on evaluation data: 8288 / 10000\nEpoch 236 training complete\nCost on training data: 0.007195464529128684\nAccuracy on evaluation data: 8288 / 10000\nEpoch 237 training complete\nCost on training data: 0.007160239334472621\nAccuracy on evaluation data: 8291 / 10000\nEpoch 238 training complete\nCost on training data: 0.007125011344912007\nAccuracy on evaluation data: 8287 / 10000\nEpoch 239 training complete\nCost on training data: 0.0070907315932205815\nAccuracy on evaluation data: 8286 / 10000\nEpoch 240 training complete\nCost on training data: 0.0070564370106393146\nAccuracy on evaluation data: 8286 / 10000\nEpoch 241 training complete\nCost on training data: 0.007022949284325297\nAccuracy on evaluation data: 8287 / 10000\nEpoch 242 training complete\nCost on training data: 0.006989332090196003\nAccuracy on evaluation data: 8285 / 10000\nEpoch 243 training complete\nCost on training data: 0.0069575643021553045\nAccuracy on evaluation data: 8286 / 10000\nEpoch 244 training complete\nCost on training data: 0.006922972969011052\nAccuracy on evaluation data: 8287 / 10000\nEpoch 245 training complete\nCost on training data: 0.006890322585898007\nAccuracy on evaluation data: 8289 / 10000\nEpoch 246 training complete\nCost on training data: 0.0068582171355924875\nAccuracy on evaluation data: 8287 / 10000\nEpoch 247 training complete\nCost on training data: 0.006826825664746198\nAccuracy on evaluation data: 8287 / 10000\nEpoch 248 training complete\nCost on training data: 0.006795318387755124\nAccuracy on evaluation data: 8286 / 10000\nEpoch 249 training complete\nCost on training data: 0.006764382520680391\nAccuracy on evaluation data: 8289 / 10000\nEpoch 250 training complete\nCost on training data: 0.006732631530410944\nAccuracy on evaluation data: 8289 / 10000\nEpoch 251 training complete\nCost on training data: 0.006701933020287373\nAccuracy on evaluation data: 8289 / 10000\nEpoch 252 training complete\nCost on training data: 0.006671259538895656\nAccuracy on evaluation data: 8289 / 10000\nEpoch 253 training complete\nCost on training data: 0.006641157579902452\nAccuracy on evaluation data: 8285 / 10000\nEpoch 254 training complete\nCost on training data: 0.0066109929829551144\nAccuracy on evaluation data: 8290 / 10000\nEpoch 255 training complete\nCost on training data: 0.006581506524839147\nAccuracy on evaluation data: 8287 / 10000\nEpoch 256 training complete\nCost on training data: 0.006551805690698094\nAccuracy on evaluation data: 8287 / 10000\nEpoch 257 training complete\nCost on training data: 0.006523026610429851\nAccuracy on evaluation data: 8287 / 10000\nEpoch 258 training complete\nCost on training data: 0.006494546730728464\nAccuracy on evaluation data: 8288 / 10000\nEpoch 259 training complete\nCost on training data: 0.0064660179659774344\nAccuracy on evaluation data: 8288 / 10000\nEpoch 260 training complete\nCost on training data: 0.0064367914723149336\nAccuracy on evaluation data: 8285 / 10000\nEpoch 261 training complete\nCost on training data: 0.006408727168911711\nAccuracy on evaluation data: 8288 / 10000\nEpoch 262 training complete\nCost on training data: 0.006380707310155465\nAccuracy on evaluation data: 8288 / 10000\nEpoch 263 training complete\nCost on training data: 0.006354243273849885\nAccuracy on evaluation data: 8287 / 10000\nEpoch 264 training complete\nCost on training data: 0.006326277361867922\nAccuracy on evaluation data: 8286 / 10000\nEpoch 265 training complete\nCost on training data: 0.0062991969040432005\nAccuracy on evaluation data: 8288 / 10000\nEpoch 266 training complete\nCost on training data: 0.006271358916423154\nAccuracy on evaluation data: 8287 / 10000\nEpoch 267 training complete\nCost on training data: 0.006244483116143672\nAccuracy on evaluation data: 8287 / 10000\nEpoch 268 training complete\nCost on training data: 0.006218052193244496\nAccuracy on evaluation data: 8288 / 10000\nEpoch 269 training complete\nCost on training data: 0.006191649878248733\nAccuracy on evaluation data: 8287 / 10000\nEpoch 270 training complete\nCost on training data: 0.006165531628267799\nAccuracy on evaluation data: 8286 / 10000\nEpoch 271 training complete\nCost on training data: 0.006140146955646749\nAccuracy on evaluation data: 8289 / 10000\nEpoch 272 training complete\nCost on training data: 0.006113950325261941\nAccuracy on evaluation data: 8286 / 10000\nEpoch 273 training complete\nCost on training data: 0.006088633003724945\nAccuracy on evaluation data: 8287 / 10000\nEpoch 274 training complete\nCost on training data: 0.0060632176283099675\nAccuracy on evaluation data: 8285 / 10000\nEpoch 275 training complete\nCost on training data: 0.006038103695593386\nAccuracy on evaluation data: 8286 / 10000\nEpoch 276 training complete\nCost on training data: 0.006013333670245106\nAccuracy on evaluation data: 8285 / 10000\nEpoch 277 training complete\nCost on training data: 0.005988708172878443\nAccuracy on evaluation data: 8284 / 10000\nEpoch 278 training complete\nCost on training data: 0.005964067741801058\nAccuracy on evaluation data: 8287 / 10000\nEpoch 279 training complete\nCost on training data: 0.005939554584149414\nAccuracy on evaluation data: 8289 / 10000\nEpoch 280 training complete\nCost on training data: 0.005915545598152728\nAccuracy on evaluation data: 8291 / 10000\nEpoch 281 training complete\nCost on training data: 0.005891857072004096\nAccuracy on evaluation data: 8290 / 10000\nEpoch 282 training complete\nCost on training data: 0.00586778250095322\nAccuracy on evaluation data: 8291 / 10000\nEpoch 283 training complete\nCost on training data: 0.005844640036610796\nAccuracy on evaluation data: 8289 / 10000\nEpoch 284 training complete\nCost on training data: 0.005820750862614823\nAccuracy on evaluation data: 8285 / 10000\nEpoch 285 training complete\nCost on training data: 0.005797367894445975\nAccuracy on evaluation data: 8288 / 10000\nEpoch 286 training complete\nCost on training data: 0.005774598955034867\nAccuracy on evaluation data: 8294 / 10000\nEpoch 287 training complete\nCost on training data: 0.005751373286306451\nAccuracy on evaluation data: 8289 / 10000\nEpoch 288 training complete\nCost on training data: 0.005728814078936483\nAccuracy on evaluation data: 8290 / 10000\nEpoch 289 training complete\nCost on training data: 0.005706300804766538\nAccuracy on evaluation data: 8291 / 10000\nEpoch 290 training complete\nCost on training data: 0.005683950365914777\nAccuracy on evaluation data: 8292 / 10000\nEpoch 291 training complete\nCost on training data: 0.005662042864370976\nAccuracy on evaluation data: 8292 / 10000\nEpoch 292 training complete\nCost on training data: 0.005639337813036541\nAccuracy on evaluation data: 8294 / 10000\nEpoch 293 training complete\nCost on training data: 0.005617549865176806\nAccuracy on evaluation data: 8294 / 10000\nEpoch 294 training complete\nCost on training data: 0.00559550286933176\nAccuracy on evaluation data: 8295 / 10000\nEpoch 295 training complete\nCost on training data: 0.005574252235835078\nAccuracy on evaluation data: 8294 / 10000\nEpoch 296 training complete\nCost on training data: 0.005552555131129523\nAccuracy on evaluation data: 8294 / 10000\nEpoch 297 training complete\nCost on training data: 0.005531417672113423\nAccuracy on evaluation data: 8296 / 10000\nEpoch 298 training complete\nCost on training data: 0.005510311184376747\nAccuracy on evaluation data: 8295 / 10000\nEpoch 299 training complete\nCost on training data: 0.005488967115247259\nAccuracy on evaluation data: 8296 / 10000\nEpoch 300 training complete\nCost on training data: 0.005468213013422956\nAccuracy on evaluation data: 8296 / 10000\nEpoch 301 training complete\nCost on training data: 0.00544741925827183\nAccuracy on evaluation data: 8296 / 10000\nEpoch 302 training complete\nCost on training data: 0.005426749424850801\nAccuracy on evaluation data: 8295 / 10000\nEpoch 303 training complete\nCost on training data: 0.005406443086656321\nAccuracy on evaluation data: 8293 / 10000\nEpoch 304 training complete\nCost on training data: 0.005386234965373989\nAccuracy on evaluation data: 8294 / 10000\nEpoch 305 training complete\nCost on training data: 0.005365939021066804\nAccuracy on evaluation data: 8296 / 10000\nEpoch 306 training complete\nCost on training data: 0.005345641025678909\nAccuracy on evaluation data: 8296 / 10000\nEpoch 307 training complete\nCost on training data: 0.005325901343876197\nAccuracy on evaluation data: 8296 / 10000\nEpoch 308 training complete\nCost on training data: 0.005306269327524977\nAccuracy on evaluation data: 8296 / 10000\nEpoch 309 training complete\nCost on training data: 0.005286343727428884\nAccuracy on evaluation data: 8295 / 10000\nEpoch 310 training complete\nCost on training data: 0.005266895148305276\nAccuracy on evaluation data: 8294 / 10000\nEpoch 311 training complete\nCost on training data: 0.005247517707361536\nAccuracy on evaluation data: 8297 / 10000\nEpoch 312 training complete\nCost on training data: 0.005228179453774824\nAccuracy on evaluation data: 8296 / 10000\nEpoch 313 training complete\nCost on training data: 0.005209324835678492\nAccuracy on evaluation data: 8297 / 10000\nEpoch 314 training complete\nCost on training data: 0.005190128908498655\nAccuracy on evaluation data: 8298 / 10000\nEpoch 315 training complete\nCost on training data: 0.005171385294364427\nAccuracy on evaluation data: 8300 / 10000\nEpoch 316 training complete\nCost on training data: 0.00515268691928425\nAccuracy on evaluation data: 8299 / 10000\nEpoch 317 training complete\nCost on training data: 0.005133891651082554\nAccuracy on evaluation data: 8303 / 10000\nEpoch 318 training complete\nCost on training data: 0.005115622974893162\nAccuracy on evaluation data: 8300 / 10000\nEpoch 319 training complete\nCost on training data: 0.005097437333308749\nAccuracy on evaluation data: 8303 / 10000\nEpoch 320 training complete\nCost on training data: 0.005079048777339515\nAccuracy on evaluation data: 8301 / 10000\nEpoch 321 training complete\nCost on training data: 0.005061337321066521\nAccuracy on evaluation data: 8300 / 10000\nEpoch 322 training complete\nCost on training data: 0.005042854658846221\nAccuracy on evaluation data: 8297 / 10000\nEpoch 323 training complete\nCost on training data: 0.005025381949058676\nAccuracy on evaluation data: 8300 / 10000\nEpoch 324 training complete\nCost on training data: 0.005007264397397749\nAccuracy on evaluation data: 8299 / 10000\nEpoch 325 training complete\nCost on training data: 0.004989667240309711\nAccuracy on evaluation data: 8300 / 10000\nEpoch 326 training complete\nCost on training data: 0.004972418969779527\nAccuracy on evaluation data: 8297 / 10000\nEpoch 327 training complete\nCost on training data: 0.004954857529917982\nAccuracy on evaluation data: 8298 / 10000\nEpoch 328 training complete\nCost on training data: 0.004937466818368719\nAccuracy on evaluation data: 8298 / 10000\nEpoch 329 training complete\nCost on training data: 0.004920286690389597\nAccuracy on evaluation data: 8298 / 10000\nEpoch 330 training complete\nCost on training data: 0.004903642835898288\nAccuracy on evaluation data: 8297 / 10000\nEpoch 331 training complete\nCost on training data: 0.004886593368479418\nAccuracy on evaluation data: 8297 / 10000\nEpoch 332 training complete\nCost on training data: 0.004869638991650194\nAccuracy on evaluation data: 8296 / 10000\nEpoch 333 training complete\nCost on training data: 0.00485271148272832\nAccuracy on evaluation data: 8297 / 10000\nEpoch 334 training complete\nCost on training data: 0.004836033610728998\nAccuracy on evaluation data: 8295 / 10000\nEpoch 335 training complete\nCost on training data: 0.004819477012448158\nAccuracy on evaluation data: 8296 / 10000\nEpoch 336 training complete\nCost on training data: 0.00480316030175109\nAccuracy on evaluation data: 8295 / 10000\nEpoch 337 training complete\nCost on training data: 0.00478705986224669\nAccuracy on evaluation data: 8297 / 10000\nEpoch 338 training complete\nCost on training data: 0.004770778022837434\nAccuracy on evaluation data: 8295 / 10000\nEpoch 339 training complete\nCost on training data: 0.004754551562360567\nAccuracy on evaluation data: 8296 / 10000\nEpoch 340 training complete\nCost on training data: 0.004738621303450828\nAccuracy on evaluation data: 8297 / 10000\nEpoch 341 training complete\nCost on training data: 0.004722850165936068\nAccuracy on evaluation data: 8294 / 10000\nEpoch 342 training complete\nCost on training data: 0.004707135056049125\nAccuracy on evaluation data: 8298 / 10000\nEpoch 343 training complete\nCost on training data: 0.004691459445012149\nAccuracy on evaluation data: 8298 / 10000\nEpoch 344 training complete\nCost on training data: 0.004675963184852911\nAccuracy on evaluation data: 8297 / 10000\nEpoch 345 training complete\nCost on training data: 0.00466052074112378\nAccuracy on evaluation data: 8297 / 10000\nEpoch 346 training complete\nCost on training data: 0.004645277381835308\nAccuracy on evaluation data: 8297 / 10000\nEpoch 347 training complete\nCost on training data: 0.00462996261494318\nAccuracy on evaluation data: 8297 / 10000\nEpoch 348 training complete\nCost on training data: 0.004614739791104975\nAccuracy on evaluation data: 8297 / 10000\nEpoch 349 training complete\nCost on training data: 0.004599992358616343\nAccuracy on evaluation data: 8295 / 10000\nEpoch 350 training complete\nCost on training data: 0.004584734088606013\nAccuracy on evaluation data: 8297 / 10000\nEpoch 351 training complete\nCost on training data: 0.004570103992380071\nAccuracy on evaluation data: 8295 / 10000\nEpoch 352 training complete\nCost on training data: 0.004555271513090721\nAccuracy on evaluation data: 8296 / 10000\nEpoch 353 training complete\nCost on training data: 0.004540581544866581\nAccuracy on evaluation data: 8297 / 10000\nEpoch 354 training complete\nCost on training data: 0.004526186229498569\nAccuracy on evaluation data: 8298 / 10000\nEpoch 355 training complete\nCost on training data: 0.004511761068871661\nAccuracy on evaluation data: 8298 / 10000\nEpoch 356 training complete\nCost on training data: 0.004497118379147558\nAccuracy on evaluation data: 8294 / 10000\nEpoch 357 training complete\nCost on training data: 0.004482847142019097\nAccuracy on evaluation data: 8294 / 10000\nEpoch 358 training complete\nCost on training data: 0.004468448747955489\nAccuracy on evaluation data: 8293 / 10000\nEpoch 359 training complete\nCost on training data: 0.004454301662470557\nAccuracy on evaluation data: 8294 / 10000\nEpoch 360 training complete\nCost on training data: 0.004440241078412804\nAccuracy on evaluation data: 8296 / 10000\nEpoch 361 training complete\nCost on training data: 0.004426393304710121\nAccuracy on evaluation data: 8294 / 10000\nEpoch 362 training complete\nCost on training data: 0.004412696739676035\nAccuracy on evaluation data: 8294 / 10000\nEpoch 363 training complete\nCost on training data: 0.004398766430765669\nAccuracy on evaluation data: 8292 / 10000\nEpoch 364 training complete\nCost on training data: 0.0043851297529393434\nAccuracy on evaluation data: 8293 / 10000\nEpoch 365 training complete\nCost on training data: 0.004371278417418942\nAccuracy on evaluation data: 8294 / 10000\nEpoch 366 training complete\nCost on training data: 0.004357756426617245\nAccuracy on evaluation data: 8295 / 10000\nEpoch 367 training complete\nCost on training data: 0.004344284032422745\nAccuracy on evaluation data: 8296 / 10000\nEpoch 368 training complete\nCost on training data: 0.004330980365147555\nAccuracy on evaluation data: 8293 / 10000\nEpoch 369 training complete\nCost on training data: 0.004317686829298213\nAccuracy on evaluation data: 8293 / 10000\nEpoch 370 training complete\nCost on training data: 0.0043043472696091154\nAccuracy on evaluation data: 8294 / 10000\nEpoch 371 training complete\nCost on training data: 0.0042911592000915155\nAccuracy on evaluation data: 8293 / 10000\nEpoch 372 training complete\nCost on training data: 0.004278081739233593\nAccuracy on evaluation data: 8294 / 10000\nEpoch 373 training complete\nCost on training data: 0.0042650820843650825\nAccuracy on evaluation data: 8294 / 10000\nEpoch 374 training complete\nCost on training data: 0.004252218806671443\nAccuracy on evaluation data: 8294 / 10000\nEpoch 375 training complete\nCost on training data: 0.004239365142552146\nAccuracy on evaluation data: 8294 / 10000\nEpoch 376 training complete\nCost on training data: 0.004226469181877731\nAccuracy on evaluation data: 8294 / 10000\nEpoch 377 training complete\nCost on training data: 0.004213789008565988\nAccuracy on evaluation data: 8293 / 10000\nEpoch 378 training complete\nCost on training data: 0.004201082977746353\nAccuracy on evaluation data: 8294 / 10000\nEpoch 379 training complete\nCost on training data: 0.004188461670676879\nAccuracy on evaluation data: 8296 / 10000\nEpoch 380 training complete\nCost on training data: 0.0041759358215385746\nAccuracy on evaluation data: 8296 / 10000\nEpoch 381 training complete\nCost on training data: 0.004163631798612785\nAccuracy on evaluation data: 8292 / 10000\nEpoch 382 training complete\nCost on training data: 0.0041512850871808\nAccuracy on evaluation data: 8295 / 10000\nEpoch 383 training complete\nCost on training data: 0.004138751495244101\nAccuracy on evaluation data: 8295 / 10000\nEpoch 384 training complete\nCost on training data: 0.0041265318885868865\nAccuracy on evaluation data: 8295 / 10000\nEpoch 385 training complete\nCost on training data: 0.004114300724664924\nAccuracy on evaluation data: 8296 / 10000\nEpoch 386 training complete\nCost on training data: 0.004102243418451178\nAccuracy on evaluation data: 8294 / 10000\nEpoch 387 training complete\nCost on training data: 0.004090173483661543\nAccuracy on evaluation data: 8295 / 10000\nEpoch 388 training complete\nCost on training data: 0.004078249277886824\nAccuracy on evaluation data: 8295 / 10000\nEpoch 389 training complete\nCost on training data: 0.0040663651513213744\nAccuracy on evaluation data: 8294 / 10000\nEpoch 390 training complete\nCost on training data: 0.004054492619413702\nAccuracy on evaluation data: 8292 / 10000\nEpoch 391 training complete\nCost on training data: 0.004042732274433442\nAccuracy on evaluation data: 8292 / 10000\nEpoch 392 training complete\nCost on training data: 0.004030998674520286\nAccuracy on evaluation data: 8297 / 10000\nEpoch 393 training complete\nCost on training data: 0.0040193176064631094\nAccuracy on evaluation data: 8292 / 10000\nEpoch 394 training complete\nCost on training data: 0.004007769817727777\nAccuracy on evaluation data: 8293 / 10000\nEpoch 395 training complete\nCost on training data: 0.003996267221335551\nAccuracy on evaluation data: 8293 / 10000\nEpoch 396 training complete\nCost on training data: 0.0039846249761465655\nAccuracy on evaluation data: 8292 / 10000\nEpoch 397 training complete\nCost on training data: 0.003973323580052852\nAccuracy on evaluation data: 8297 / 10000\nEpoch 398 training complete\nCost on training data: 0.003961815179733587\nAccuracy on evaluation data: 8293 / 10000\nEpoch 399 training complete\nCost on training data: 0.0039505331603134535\nAccuracy on evaluation data: 8293 / 10000\n\n\n([],\n [5336,\n  6481,\n  7009,\n  7278,\n  7518,\n  7621,\n  7749,\n  7741,\n  7913,\n  7890,\n  7977,\n  7938,\n  7913,\n  8051,\n  8115,\n  8133,\n  8096,\n  8108,\n  8138,\n  8127,\n  8145,\n  8126,\n  8127,\n  8159,\n  8160,\n  8176,\n  8163,\n  8185,\n  8192,\n  8190,\n  8187,\n  8204,\n  8208,\n  8180,\n  8193,\n  8206,\n  8207,\n  8198,\n  8223,\n  8217,\n  8211,\n  8221,\n  8218,\n  8236,\n  8215,\n  8232,\n  8220,\n  8228,\n  8224,\n  8224,\n  8236,\n  8220,\n  8222,\n  8232,\n  8231,\n  8236,\n  8231,\n  8234,\n  8225,\n  8232,\n  8228,\n  8234,\n  8234,\n  8241,\n  8233,\n  8234,\n  8224,\n  8234,\n  8233,\n  8235,\n  8238,\n  8241,\n  8235,\n  8237,\n  8241,\n  8235,\n  8245,\n  8236,\n  8246,\n  8244,\n  8250,\n  8242,\n  8240,\n  8243,\n  8248,\n  8247,\n  8253,\n  8251,\n  8245,\n  8255,\n  8248,\n  8236,\n  8246,\n  8248,\n  8243,\n  8240,\n  8254,\n  8247,\n  8247,\n  8243,\n  8246,\n  8241,\n  8249,\n  8235,\n  8239,\n  8240,\n  8253,\n  8246,\n  8245,\n  8243,\n  8248,\n  8249,\n  8244,\n  8255,\n  8250,\n  8253,\n  8253,\n  8254,\n  8245,\n  8245,\n  8253,\n  8248,\n  8256,\n  8256,\n  8247,\n  8250,\n  8252,\n  8249,\n  8251,\n  8246,\n  8250,\n  8258,\n  8260,\n  8260,\n  8266,\n  8260,\n  8264,\n  8251,\n  8256,\n  8261,\n  8269,\n  8268,\n  8270,\n  8268,\n  8263,\n  8260,\n  8263,\n  8268,\n  8267,\n  8262,\n  8270,\n  8265,\n  8268,\n  8265,\n  8273,\n  8271,\n  8273,\n  8270,\n  8273,\n  8271,\n  8274,\n  8268,\n  8276,\n  8269,\n  8270,\n  8268,\n  8270,\n  8272,\n  8269,\n  8275,\n  8272,\n  8276,\n  8270,\n  8276,\n  8274,\n  8273,\n  8275,\n  8273,\n  8277,\n  8277,\n  8274,\n  8274,\n  8278,\n  8280,\n  8280,\n  8279,\n  8277,\n  8277,\n  8274,\n  8278,\n  8274,\n  8277,\n  8276,\n  8276,\n  8279,\n  8280,\n  8278,\n  8274,\n  8275,\n  8277,\n  8276,\n  8276,\n  8278,\n  8282,\n  8282,\n  8281,\n  8280,\n  8280,\n  8284,\n  8278,\n  8285,\n  8282,\n  8282,\n  8279,\n  8277,\n  8281,\n  8277,\n  8286,\n  8287,\n  8284,\n  8283,\n  8280,\n  8283,\n  8284,\n  8286,\n  8283,\n  8285,\n  8284,\n  8286,\n  8289,\n  8287,\n  8288,\n  8285,\n  8284,\n  8288,\n  8288,\n  8288,\n  8291,\n  8287,\n  8286,\n  8286,\n  8287,\n  8285,\n  8286,\n  8287,\n  8289,\n  8287,\n  8287,\n  8286,\n  8289,\n  8289,\n  8289,\n  8289,\n  8285,\n  8290,\n  8287,\n  8287,\n  8287,\n  8288,\n  8288,\n  8285,\n  8288,\n  8288,\n  8287,\n  8286,\n  8288,\n  8287,\n  8287,\n  8288,\n  8287,\n  8286,\n  8289,\n  8286,\n  8287,\n  8285,\n  8286,\n  8285,\n  8284,\n  8287,\n  8289,\n  8291,\n  8290,\n  8291,\n  8289,\n  8285,\n  8288,\n  8294,\n  8289,\n  8290,\n  8291,\n  8292,\n  8292,\n  8294,\n  8294,\n  8295,\n  8294,\n  8294,\n  8296,\n  8295,\n  8296,\n  8296,\n  8296,\n  8295,\n  8293,\n  8294,\n  8296,\n  8296,\n  8296,\n  8296,\n  8295,\n  8294,\n  8297,\n  8296,\n  8297,\n  8298,\n  8300,\n  8299,\n  8303,\n  8300,\n  8303,\n  8301,\n  8300,\n  8297,\n  8300,\n  8299,\n  8300,\n  8297,\n  8298,\n  8298,\n  8298,\n  8297,\n  8297,\n  8296,\n  8297,\n  8295,\n  8296,\n  8295,\n  8297,\n  8295,\n  8296,\n  8297,\n  8294,\n  8298,\n  8298,\n  8297,\n  8297,\n  8297,\n  8297,\n  8297,\n  8295,\n  8297,\n  8295,\n  8296,\n  8297,\n  8298,\n  8298,\n  8294,\n  8294,\n  8293,\n  8294,\n  8296,\n  8294,\n  8294,\n  8292,\n  8293,\n  8294,\n  8295,\n  8296,\n  8293,\n  8293,\n  8294,\n  8293,\n  8294,\n  8294,\n  8294,\n  8294,\n  8294,\n  8293,\n  8294,\n  8296,\n  8296,\n  8292,\n  8295,\n  8295,\n  8295,\n  8296,\n  8294,\n  8295,\n  8295,\n  8294,\n  8292,\n  8292,\n  8297,\n  8292,\n  8293,\n  8293,\n  8292,\n  8297,\n  8293,\n  8293],\n [1.9157265803666599,\n  1.4233085154513578,\n  1.140610742579291,\n  0.9673757700312096,\n  0.8219014211106735,\n  0.7009795108242491,\n  0.6210731479476688,\n  0.5649499666413196,\n  0.4967537573921582,\n  0.4456219552812798,\n  0.39901535860495846,\n  0.36146464324057803,\n  0.3545073579313579,\n  0.3087184094679663,\n  0.2797147873623128,\n  0.2525545832423591,\n  0.2380955942826455,\n  0.22646415893206082,\n  0.20683416913226718,\n  0.19279183350114984,\n  0.1866037912492253,\n  0.17397172190964394,\n  0.16138418421655057,\n  0.15258576699007645,\n  0.14559589206163331,\n  0.13733030674883276,\n  0.13286504240839203,\n  0.12490575508106312,\n  0.11897385331053419,\n  0.11416971608153519,\n  0.11001761063509545,\n  0.10569402966303736,\n  0.1004963762997966,\n  0.09764112227836957,\n  0.09319934983369883,\n  0.08868491774835854,\n  0.08595086097925883,\n  0.08350067214392239,\n  0.07869753555787781,\n  0.07594792345922877,\n  0.07346669031585368,\n  0.0708839567187532,\n  0.06831180691768503,\n  0.06711002154610325,\n  0.06405338526915547,\n  0.06160733031909577,\n  0.0596664860482716,\n  0.05782624278400594,\n  0.05620444418301355,\n  0.05465434524748508,\n  0.053404822360543044,\n  0.051435838493430454,\n  0.04995871675143006,\n  0.049178511975360266,\n  0.04761860543521613,\n  0.045694060305748366,\n  0.04471523134867321,\n  0.04353039157951212,\n  0.042274507860818385,\n  0.04128529084302505,\n  0.040370416208562215,\n  0.03927593215490957,\n  0.03849301313041344,\n  0.03784698326101495,\n  0.036834324900328834,\n  0.036043977294875976,\n  0.03532049078790165,\n  0.03457195553625183,\n  0.03396945778541524,\n  0.033330702513722874,\n  0.03261064682887517,\n  0.03190709529638142,\n  0.03152396913807024,\n  0.03076630409249701,\n  0.03023149206147074,\n  0.029662199460815355,\n  0.029187429963762536,\n  0.028691340398401113,\n  0.028152612115837413,\n  0.02781349540189489,\n  0.027304669134411593,\n  0.026856672884772514,\n  0.02639586449219745,\n  0.02595796469117669,\n  0.025575379849845398,\n  0.025208345723867195,\n  0.024804570034370994,\n  0.024434796921913336,\n  0.024103135768802273,\n  0.02376708064146453,\n  0.023434898537771164,\n  0.023070941073319938,\n  0.022738641763154207,\n  0.022405034123781458,\n  0.022116731931907656,\n  0.021811183501928306,\n  0.021542550179585496,\n  0.0212327173512337,\n  0.020955587687164007,\n  0.02070081988279502,\n  0.020464101024792077,\n  0.020183177431458525,\n  0.01990947657685897,\n  0.019673095002806538,\n  0.01945071220597259,\n  0.019183272328162273,\n  0.01896155749194874,\n  0.018737027644866,\n  0.01852884659595162,\n  0.018326055157791767,\n  0.01809023758910947,\n  0.017889669482253606,\n  0.017689472412734882,\n  0.01748919382769571,\n  0.01729858085590458,\n  0.017126279843091046,\n  0.016934041161704855,\n  0.01674109717372653,\n  0.01655962677581138,\n  0.01639757707029055,\n  0.01621579584970799,\n  0.01605936365039096,\n  0.01589021235514923,\n  0.01572518834641798,\n  0.015569978429947886,\n  0.015407319294389883,\n  0.015251287024967676,\n  0.015108016005986392,\n  0.014959029113495606,\n  0.014821946295991628,\n  0.014676781773088376,\n  0.014547077394996423,\n  0.014406349446049009,\n  0.014274582991979683,\n  0.01415044622375008,\n  0.014012607992043,\n  0.01389321569049412,\n  0.013766664380162422,\n  0.013653798693873268,\n  0.01352502677394385,\n  0.013409418654958455,\n  0.013301199019208685,\n  0.013180497870486077,\n  0.013077935972272886,\n  0.01296117787556624,\n  0.012856299557458047,\n  0.012751451883080927,\n  0.012645519491190157,\n  0.012545166466142342,\n  0.012447508071661858,\n  0.01234312271672617,\n  0.012247949541335238,\n  0.012152139704774108,\n  0.012064314828212068,\n  0.011966400675534445,\n  0.011874914838803683,\n  0.011782724235167658,\n  0.0116966319554314,\n  0.011610256479557782,\n  0.011520105258763945,\n  0.011439755468064356,\n  0.011357655493793929,\n  0.011283409102421852,\n  0.01118870672659996,\n  0.01110691983469005,\n  0.011027037584351687,\n  0.0109480839360458,\n  0.010868154795374474,\n  0.010794628856790733,\n  0.010718020864434652,\n  0.010640652611700866,\n  0.010568505850681333,\n  0.010495144281480608,\n  0.010423459700472372,\n  0.01035215785574867,\n  0.010281598389256895,\n  0.010213743367284523,\n  0.010144601049374632,\n  0.010075490292771319,\n  0.010008294371522152,\n  0.009942008567899162,\n  0.009878198710323437,\n  0.009813496530298511,\n  0.009745402673000931,\n  0.009683709284427975,\n  0.009620426739445657,\n  0.009557455763009055,\n  0.00949805534433109,\n  0.009438038102481235,\n  0.009375645959080825,\n  0.009315332442636128,\n  0.0092563441875046,\n  0.009197103377941369,\n  0.00913937219535242,\n  0.009083971707609098,\n  0.00902560657927742,\n  0.008970855935428792,\n  0.008913885902286431,\n  0.008859597006419211,\n  0.008805134195939951,\n  0.008753320472002191,\n  0.008699193168168104,\n  0.008646628714748658,\n  0.008596697512196652,\n  0.00854450520511543,\n  0.008494266012365406,\n  0.00844535278211304,\n  0.008396311920896578,\n  0.008348879753283944,\n  0.00830025892889786,\n  0.008253635655113563,\n  0.008206207699730876,\n  0.008159949913241944,\n  0.008115473505970949,\n  0.008070457971559742,\n  0.008025276871048191,\n  0.00798175692971702,\n  0.007937385276313748,\n  0.00789409095460702,\n  0.007851964134100609,\n  0.007810620015641798,\n  0.007768607301514054,\n  0.007727871181417616,\n  0.00768651891795349,\n  0.007645607395488211,\n  0.007606159909610643,\n  0.0075671767015055395,\n  0.007528717093426312,\n  0.007490312469768559,\n  0.007451488872162558,\n  0.0074139164053383805,\n  0.00737624578357679,\n  0.007339102825272781,\n  0.007303170786097757,\n  0.007266443130721465,\n  0.0072306431437006245,\n  0.007195464529128684,\n  0.007160239334472621,\n  0.007125011344912007,\n  0.0070907315932205815,\n  0.0070564370106393146,\n  0.007022949284325297,\n  0.006989332090196003,\n  0.0069575643021553045,\n  0.006922972969011052,\n  0.006890322585898007,\n  0.0068582171355924875,\n  0.006826825664746198,\n  0.006795318387755124,\n  0.006764382520680391,\n  0.006732631530410944,\n  0.006701933020287373,\n  0.006671259538895656,\n  0.006641157579902452,\n  0.0066109929829551144,\n  0.006581506524839147,\n  0.006551805690698094,\n  0.006523026610429851,\n  0.006494546730728464,\n  0.0064660179659774344,\n  0.0064367914723149336,\n  0.006408727168911711,\n  0.006380707310155465,\n  0.006354243273849885,\n  0.006326277361867922,\n  0.0062991969040432005,\n  0.006271358916423154,\n  0.006244483116143672,\n  0.006218052193244496,\n  0.006191649878248733,\n  0.006165531628267799,\n  0.006140146955646749,\n  0.006113950325261941,\n  0.006088633003724945,\n  0.0060632176283099675,\n  0.006038103695593386,\n  0.006013333670245106,\n  0.005988708172878443,\n  0.005964067741801058,\n  0.005939554584149414,\n  0.005915545598152728,\n  0.005891857072004096,\n  0.00586778250095322,\n  0.005844640036610796,\n  0.005820750862614823,\n  0.005797367894445975,\n  0.005774598955034867,\n  0.005751373286306451,\n  0.005728814078936483,\n  0.005706300804766538,\n  0.005683950365914777,\n  0.005662042864370976,\n  0.005639337813036541,\n  0.005617549865176806,\n  0.00559550286933176,\n  0.005574252235835078,\n  0.005552555131129523,\n  0.005531417672113423,\n  0.005510311184376747,\n  0.005488967115247259,\n  0.005468213013422956,\n  0.00544741925827183,\n  0.005426749424850801,\n  0.005406443086656321,\n  0.005386234965373989,\n  0.005365939021066804,\n  0.005345641025678909,\n  0.005325901343876197,\n  0.005306269327524977,\n  0.005286343727428884,\n  0.005266895148305276,\n  0.005247517707361536,\n  0.005228179453774824,\n  0.005209324835678492,\n  0.005190128908498655,\n  0.005171385294364427,\n  0.00515268691928425,\n  0.005133891651082554,\n  0.005115622974893162,\n  0.005097437333308749,\n  0.005079048777339515,\n  0.005061337321066521,\n  0.005042854658846221,\n  0.005025381949058676,\n  0.005007264397397749,\n  0.004989667240309711,\n  0.004972418969779527,\n  0.004954857529917982,\n  0.004937466818368719,\n  0.004920286690389597,\n  0.004903642835898288,\n  0.004886593368479418,\n  0.004869638991650194,\n  0.00485271148272832,\n  0.004836033610728998,\n  0.004819477012448158,\n  0.00480316030175109,\n  0.00478705986224669,\n  0.004770778022837434,\n  0.004754551562360567,\n  0.004738621303450828,\n  0.004722850165936068,\n  0.004707135056049125,\n  0.004691459445012149,\n  0.004675963184852911,\n  0.00466052074112378,\n  0.004645277381835308,\n  0.00462996261494318,\n  0.004614739791104975,\n  0.004599992358616343,\n  0.004584734088606013,\n  0.004570103992380071,\n  0.004555271513090721,\n  0.004540581544866581,\n  0.004526186229498569,\n  0.004511761068871661,\n  0.004497118379147558,\n  0.004482847142019097,\n  0.004468448747955489,\n  0.004454301662470557,\n  0.004440241078412804,\n  0.004426393304710121,\n  0.004412696739676035,\n  0.004398766430765669,\n  0.0043851297529393434,\n  0.004371278417418942,\n  0.004357756426617245,\n  0.004344284032422745,\n  0.004330980365147555,\n  0.004317686829298213,\n  0.0043043472696091154,\n  0.0042911592000915155,\n  0.004278081739233593,\n  0.0042650820843650825,\n  0.004252218806671443,\n  0.004239365142552146,\n  0.004226469181877731,\n  0.004213789008565988,\n  0.004201082977746353,\n  0.004188461670676879,\n  0.0041759358215385746,\n  0.004163631798612785,\n  0.0041512850871808,\n  0.004138751495244101,\n  0.0041265318885868865,\n  0.004114300724664924,\n  0.004102243418451178,\n  0.004090173483661543,\n  0.004078249277886824,\n  0.0040663651513213744,\n  0.004054492619413702,\n  0.004042732274433442,\n  0.004030998674520286,\n  0.0040193176064631094,\n  0.004007769817727777,\n  0.003996267221335551,\n  0.0039846249761465655,\n  0.003973323580052852,\n  0.003961815179733587,\n  0.0039505331603134535],\n [])\n\n\n\nclass VanillaMNIST(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.linear = nn.Sequential(\n            nn.Linear(784, 30), nn.Sigmoid(), nn.Linear(30, 10), nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.linear(x)\n\n\n# Training loop\ntraining_data = training_data[:1000]\nevaluation_data = test_data\n\nn = len(training_data)\nepochs = 400\nmini_batch_size = 10\nalpha = 0.5\nmodel = VanillaMNIST()\n\ncosts = []\naccuracies = []\n\n# Experiment: comparing regularized and non-regularized models\nfor j in range(epochs):\n    random.shuffle(training_data)\n    mini_batches = [\n        training_data[k : k + mini_batch_size] for k in range(0, n, mini_batch_size)\n    ]\n    epoch_loss = 0\n    for mini_batch in mini_batches:\n        total_loss = torch.tensor([0.0])\n        for x, y in mini_batch:\n            x = torch.tensor(x).squeeze(1)\n            y = torch.tensor(y).squeeze(1)\n\n            # Calculate log loss and add to loss\n            y_hat = model(x)\n            loss = y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat)\n            total_loss += sum(loss)\n\n        log_loss = -(1 / len(mini_batch)) * total_loss\n        model.zero_grad()\n        log_loss.backward()\n        for p in model.parameters():\n            p.data = p.data - alpha * p.grad\n        epoch_loss += log_loss.item()\n\n    # Evaluate model on evaluation_data\n    correct = 0\n    for x, y in evaluation_data:\n        x = torch.tensor(x).squeeze(1)\n        y_hat = model(x)\n        if torch.argmax(y_hat) == y:\n            correct += 1\n\n    costs.append(epoch_loss / len(mini_batches))\n    accuracies.append(correct / len(evaluation_data))\n    print(\n        f\"Epoch {j}, Loss: {epoch_loss / len(mini_batches)}, Accuracy: {correct/len(evaluation_data)}\"\n    )\n\nEpoch 0, Loss: 2.4531275963783266, Accuracy: 0.7109\nEpoch 1, Loss: 1.2405188855528833, Accuracy: 0.8002\nEpoch 2, Loss: 0.8996775972843171, Accuracy: 0.8283\nEpoch 3, Loss: 0.6996856625378132, Accuracy: 0.8622\nEpoch 4, Loss: 0.5463169002532959, Accuracy: 0.8649\nEpoch 5, Loss: 0.46642671793699264, Accuracy: 0.874\nEpoch 6, Loss: 0.3940893569588661, Accuracy: 0.8774\nEpoch 7, Loss: 0.31663369677960873, Accuracy: 0.8706\nEpoch 8, Loss: 0.26746165841817854, Accuracy: 0.8696\nEpoch 9, Loss: 0.22563948720693588, Accuracy: 0.8668\nEpoch 10, Loss: 0.1941769213229418, Accuracy: 0.8728\nEpoch 11, Loss: 0.16567592516541482, Accuracy: 0.8714\nEpoch 12, Loss: 0.14149875968694686, Accuracy: 0.8695\nEpoch 13, Loss: 0.12648260202258826, Accuracy: 0.8735\nEpoch 14, Loss: 0.11159584682434798, Accuracy: 0.8771\nEpoch 15, Loss: 0.10187144234776496, Accuracy: 0.8752\nEpoch 16, Loss: 0.090857108309865, Accuracy: 0.8723\nEpoch 17, Loss: 0.0825225205719471, Accuracy: 0.8732\nEpoch 18, Loss: 0.07501107199117542, Accuracy: 0.8708\nEpoch 19, Loss: 0.06901620700955391, Accuracy: 0.8744\nEpoch 20, Loss: 0.06235401973128319, Accuracy: 0.8715\nEpoch 21, Loss: 0.05752668173983693, Accuracy: 0.8731\nEpoch 22, Loss: 0.053382117561995984, Accuracy: 0.871\nEpoch 23, Loss: 0.048515842566266655, Accuracy: 0.8706\nEpoch 24, Loss: 0.045617444226518276, Accuracy: 0.8739\nEpoch 25, Loss: 0.042514854790642854, Accuracy: 0.8726\nEpoch 26, Loss: 0.040018187407404184, Accuracy: 0.8749\nEpoch 27, Loss: 0.037688900977373124, Accuracy: 0.8745\nEpoch 28, Loss: 0.03576532278209925, Accuracy: 0.8738\nEpoch 29, Loss: 0.03388645425438881, Accuracy: 0.871\nEpoch 30, Loss: 0.031943229897879066, Accuracy: 0.872\nEpoch 31, Loss: 0.030804977854713797, Accuracy: 0.8725\nEpoch 32, Loss: 0.029267427837476136, Accuracy: 0.8724\nEpoch 33, Loss: 0.027988012125715615, Accuracy: 0.8727\nEpoch 34, Loss: 0.02692528400570154, Accuracy: 0.8734\nEpoch 35, Loss: 0.02578953033313155, Accuracy: 0.8739\nEpoch 36, Loss: 0.02473074817098677, Accuracy: 0.8727\nEpoch 37, Loss: 0.023948485157452525, Accuracy: 0.8734\nEpoch 38, Loss: 0.023083161497488618, Accuracy: 0.8722\nEpoch 39, Loss: 0.022216263893060386, Accuracy: 0.8724\nEpoch 40, Loss: 0.021428094282746314, Accuracy: 0.8729\nEpoch 41, Loss: 0.0207216667663306, Accuracy: 0.8725\nEpoch 42, Loss: 0.020075792497955262, Accuracy: 0.8734\nEpoch 43, Loss: 0.019438890577293932, Accuracy: 0.8726\nEpoch 44, Loss: 0.018888883148320018, Accuracy: 0.8729\nEpoch 45, Loss: 0.018310073078610004, Accuracy: 0.8743\nEpoch 46, Loss: 0.01786278264131397, Accuracy: 0.8723\nEpoch 47, Loss: 0.017336098682135342, Accuracy: 0.8726\nEpoch 48, Loss: 0.016839303341694176, Accuracy: 0.8724\nEpoch 49, Loss: 0.016438655094243585, Accuracy: 0.8735\nEpoch 50, Loss: 0.016002707018051298, Accuracy: 0.8733\nEpoch 51, Loss: 0.015571096451021732, Accuracy: 0.8734\nEpoch 52, Loss: 0.015197516572661697, Accuracy: 0.8726\nEpoch 53, Loss: 0.01483210429083556, Accuracy: 0.8721\nEpoch 54, Loss: 0.0144432037579827, Accuracy: 0.8726\nEpoch 55, Loss: 0.014115709997713565, Accuracy: 0.8724\nEpoch 56, Loss: 0.013824242195114493, Accuracy: 0.8727\nEpoch 57, Loss: 0.013513712235726416, Accuracy: 0.8724\nEpoch 58, Loss: 0.013189801815897226, Accuracy: 0.8726\nEpoch 59, Loss: 0.012927961926907301, Accuracy: 0.8727\nEpoch 60, Loss: 0.012648520213551818, Accuracy: 0.8728\nEpoch 61, Loss: 0.01237380321836099, Accuracy: 0.8719\nEpoch 62, Loss: 0.01211765622952953, Accuracy: 0.8729\nEpoch 63, Loss: 0.011900372395757586, Accuracy: 0.8733\nEpoch 64, Loss: 0.01165993848349899, Accuracy: 0.8731\nEpoch 65, Loss: 0.011433309789281339, Accuracy: 0.8723\nEpoch 66, Loss: 0.011227149807382375, Accuracy: 0.8728\nEpoch 67, Loss: 0.01101183719234541, Accuracy: 0.8725\nEpoch 68, Loss: 0.010828937229234725, Accuracy: 0.8726\nEpoch 69, Loss: 0.01060554861323908, Accuracy: 0.8718\nEpoch 70, Loss: 0.010421425142558291, Accuracy: 0.8725\nEpoch 71, Loss: 0.010229339357465506, Accuracy: 0.8725\nEpoch 72, Loss: 0.0100756605155766, Accuracy: 0.8721\nEpoch 73, Loss: 0.009899040868040174, Accuracy: 0.8715\nEpoch 74, Loss: 0.009749302093405276, Accuracy: 0.8715\nEpoch 75, Loss: 0.009569611989427358, Accuracy: 0.8733\nEpoch 76, Loss: 0.009425292597152293, Accuracy: 0.8727\nEpoch 77, Loss: 0.009264195358846337, Accuracy: 0.8721\nEpoch 78, Loss: 0.009127193087479099, Accuracy: 0.8724\nEpoch 79, Loss: 0.00898524905089289, Accuracy: 0.8723\nEpoch 80, Loss: 0.008832463757134973, Accuracy: 0.8723\nEpoch 81, Loss: 0.008708758614957332, Accuracy: 0.8713\nEpoch 82, Loss: 0.008584543475881218, Accuracy: 0.8721\nEpoch 83, Loss: 0.008455280263442546, Accuracy: 0.8722\nEpoch 84, Loss: 0.0083364762715064, Accuracy: 0.8723\nEpoch 85, Loss: 0.008208162419032305, Accuracy: 0.8729\nEpoch 86, Loss: 0.008105757315643131, Accuracy: 0.8724\nEpoch 87, Loss: 0.007989216134883464, Accuracy: 0.8722\nEpoch 88, Loss: 0.007875825720839202, Accuracy: 0.8722\nEpoch 89, Loss: 0.007766608237288892, Accuracy: 0.8721\nEpoch 90, Loss: 0.00767501711146906, Accuracy: 0.8722\nEpoch 91, Loss: 0.007565572921885177, Accuracy: 0.8719\nEpoch 92, Loss: 0.0074633844604250045, Accuracy: 0.8718\nEpoch 93, Loss: 0.007358712801942602, Accuracy: 0.8719\nEpoch 94, Loss: 0.007279909494100139, Accuracy: 0.8724\nEpoch 95, Loss: 0.007189764491049573, Accuracy: 0.8724\nEpoch 96, Loss: 0.007097371772397309, Accuracy: 0.8716\nEpoch 97, Loss: 0.0070083232445176695, Accuracy: 0.8723\nEpoch 98, Loss: 0.0069213293492794035, Accuracy: 0.8721\nEpoch 99, Loss: 0.006838312244508415, Accuracy: 0.8727\nEpoch 100, Loss: 0.006754095115466044, Accuracy: 0.8717\nEpoch 101, Loss: 0.006674795488361269, Accuracy: 0.8716\nEpoch 102, Loss: 0.006585179039975628, Accuracy: 0.8728\nEpoch 103, Loss: 0.006517461003968492, Accuracy: 0.8721\nEpoch 104, Loss: 0.006449149603722617, Accuracy: 0.8722\nEpoch 105, Loss: 0.006372452431824058, Accuracy: 0.8722\nEpoch 106, Loss: 0.0063019146304577585, Accuracy: 0.8722\nEpoch 107, Loss: 0.006214031298877671, Accuracy: 0.8722\nEpoch 108, Loss: 0.006165632943157107, Accuracy: 0.8726\nEpoch 109, Loss: 0.006090059682028368, Accuracy: 0.8726\nEpoch 110, Loss: 0.006027454623254016, Accuracy: 0.8723\nEpoch 111, Loss: 0.005960115757770837, Accuracy: 0.8725\nEpoch 112, Loss: 0.005901065523503348, Accuracy: 0.8723\nEpoch 113, Loss: 0.005839881401043385, Accuracy: 0.8726\nEpoch 114, Loss: 0.005779328050557524, Accuracy: 0.8723\nEpoch 115, Loss: 0.005719172375975177, Accuracy: 0.8725\nEpoch 116, Loss: 0.005660945548443124, Accuracy: 0.8724\nEpoch 117, Loss: 0.0056067858287133275, Accuracy: 0.872\nEpoch 118, Loss: 0.005553687383653596, Accuracy: 0.8721\nEpoch 119, Loss: 0.005491173712071032, Accuracy: 0.8723\nEpoch 120, Loss: 0.005438129890244454, Accuracy: 0.8724\nEpoch 121, Loss: 0.0053864287014584985, Accuracy: 0.8724\nEpoch 122, Loss: 0.005330148183275014, Accuracy: 0.8723\nEpoch 123, Loss: 0.005280320981983095, Accuracy: 0.8724\nEpoch 124, Loss: 0.005229571026284248, Accuracy: 0.8722\nEpoch 125, Loss: 0.0051804867188911885, Accuracy: 0.8726\nEpoch 126, Loss: 0.005136047217529267, Accuracy: 0.8724\nEpoch 127, Loss: 0.005084951400058344, Accuracy: 0.8727\nEpoch 128, Loss: 0.005040062270127237, Accuracy: 0.8724\nEpoch 129, Loss: 0.00499373213853687, Accuracy: 0.8726\nEpoch 130, Loss: 0.004950132307130844, Accuracy: 0.872\nEpoch 131, Loss: 0.004904215610586107, Accuracy: 0.8723\nEpoch 132, Loss: 0.004856875179102644, Accuracy: 0.8723\nEpoch 133, Loss: 0.004813711573369801, Accuracy: 0.8721\nEpoch 134, Loss: 0.00477546792360954, Accuracy: 0.8722\nEpoch 135, Loss: 0.004735157394316047, Accuracy: 0.8721\nEpoch 136, Loss: 0.004693779923254624, Accuracy: 0.8723\nEpoch 137, Loss: 0.004653818474616855, Accuracy: 0.8725\nEpoch 138, Loss: 0.004610908387112431, Accuracy: 0.8723\nEpoch 139, Loss: 0.004574819676345214, Accuracy: 0.8725\nEpoch 140, Loss: 0.004536065114079974, Accuracy: 0.8723\nEpoch 141, Loss: 0.004500375627540052, Accuracy: 0.8723\nEpoch 142, Loss: 0.004460379459196702, Accuracy: 0.8722\nEpoch 143, Loss: 0.004422831879928708, Accuracy: 0.8723\nEpoch 144, Loss: 0.004387139801401645, Accuracy: 0.872\nEpoch 145, Loss: 0.0043527864816132935, Accuracy: 0.8723\nEpoch 146, Loss: 0.004314843084430322, Accuracy: 0.8725\nEpoch 147, Loss: 0.004283120817271993, Accuracy: 0.8724\nEpoch 148, Loss: 0.004246901428559795, Accuracy: 0.8719\nEpoch 149, Loss: 0.004213269703905098, Accuracy: 0.8723\nEpoch 150, Loss: 0.004179239843506366, Accuracy: 0.872\nEpoch 151, Loss: 0.004146985519910231, Accuracy: 0.8722\nEpoch 152, Loss: 0.004116269942605868, Accuracy: 0.8722\nEpoch 153, Loss: 0.004082613876089453, Accuracy: 0.8722\nEpoch 154, Loss: 0.004052915449719876, Accuracy: 0.872\nEpoch 155, Loss: 0.004019025803427212, Accuracy: 0.8726\nEpoch 156, Loss: 0.003991376632475294, Accuracy: 0.8722\nEpoch 157, Loss: 0.00395896740956232, Accuracy: 0.8723\nEpoch 158, Loss: 0.003932468290440738, Accuracy: 0.8721\nEpoch 159, Loss: 0.003902717604069039, Accuracy: 0.8721\nEpoch 160, Loss: 0.0038727907254360616, Accuracy: 0.8722\nEpoch 161, Loss: 0.003844907159218565, Accuracy: 0.8718\nEpoch 162, Loss: 0.0038146300194785, Accuracy: 0.8719\nEpoch 163, Loss: 0.0037885506550082936, Accuracy: 0.8718\nEpoch 164, Loss: 0.003761927291052416, Accuracy: 0.8717\nEpoch 165, Loss: 0.0037343191372929143, Accuracy: 0.8715\nEpoch 166, Loss: 0.0037076471466571093, Accuracy: 0.8722\nEpoch 167, Loss: 0.003683471523690969, Accuracy: 0.8718\nEpoch 168, Loss: 0.0036561070953030138, Accuracy: 0.8716\nEpoch 169, Loss: 0.0036313842370873316, Accuracy: 0.8715\nEpoch 170, Loss: 0.003605616603163071, Accuracy: 0.8715\nEpoch 171, Loss: 0.003582297961693257, Accuracy: 0.8714\nEpoch 172, Loss: 0.0035547465935815126, Accuracy: 0.8717\nEpoch 173, Loss: 0.0035323927376884967, Accuracy: 0.8716\nEpoch 174, Loss: 0.003508404188323766, Accuracy: 0.8719\nEpoch 175, Loss: 0.003486423323629424, Accuracy: 0.8715\nEpoch 176, Loss: 0.003464667713851668, Accuracy: 0.8713\nEpoch 177, Loss: 0.003438445977517404, Accuracy: 0.8715\nEpoch 178, Loss: 0.0034181723801884802, Accuracy: 0.8714\nEpoch 179, Loss: 0.0033946451381780206, Accuracy: 0.8714\nEpoch 180, Loss: 0.003375308957183734, Accuracy: 0.8714\nEpoch 181, Loss: 0.003352334484225139, Accuracy: 0.8714\nEpoch 182, Loss: 0.0033308669796679167, Accuracy: 0.8714\nEpoch 183, Loss: 0.003310680742142722, Accuracy: 0.8712\nEpoch 184, Loss: 0.003289305975777097, Accuracy: 0.8714\nEpoch 185, Loss: 0.0032678735826630143, Accuracy: 0.8716\nEpoch 186, Loss: 0.003249023495009169, Accuracy: 0.8715\nEpoch 187, Loss: 0.003228127608308569, Accuracy: 0.8712\nEpoch 188, Loss: 0.0032079139054985717, Accuracy: 0.8714\nEpoch 189, Loss: 0.0031888568628346547, Accuracy: 0.871\nEpoch 190, Loss: 0.0031706344644771888, Accuracy: 0.8711\nEpoch 191, Loss: 0.003150515223969705, Accuracy: 0.8713\nEpoch 192, Loss: 0.003133080011466518, Accuracy: 0.8714\nEpoch 193, Loss: 0.0031122158240759743, Accuracy: 0.8712\nEpoch 194, Loss: 0.0030953169090207665, Accuracy: 0.8713\nEpoch 195, Loss: 0.003074698029085994, Accuracy: 0.8712\nEpoch 196, Loss: 0.003058700527762994, Accuracy: 0.871\nEpoch 197, Loss: 0.0030408232571789997, Accuracy: 0.8713\nEpoch 198, Loss: 0.0030220175167778507, Accuracy: 0.8713\nEpoch 199, Loss: 0.0030064428329933434, Accuracy: 0.8713\nEpoch 200, Loss: 0.0029877432656940073, Accuracy: 0.871\nEpoch 201, Loss: 0.0029709012963576244, Accuracy: 0.8711\nEpoch 202, Loss: 0.002955302106565796, Accuracy: 0.8712\nEpoch 203, Loss: 0.002937178130960092, Accuracy: 0.8713\nEpoch 204, Loss: 0.0029213359107961878, Accuracy: 0.871\nEpoch 205, Loss: 0.002905478093889542, Accuracy: 0.8712\nEpoch 206, Loss: 0.002889775988296606, Accuracy: 0.8712\nEpoch 207, Loss: 0.002872818151372485, Accuracy: 0.8711\nEpoch 208, Loss: 0.0028572181588970126, Accuracy: 0.8714\nEpoch 209, Loss: 0.0028422262269305067, Accuracy: 0.8713\nEpoch 210, Loss: 0.002825307647581212, Accuracy: 0.8715\nEpoch 211, Loss: 0.002811451707384549, Accuracy: 0.8714\nEpoch 212, Loss: 0.0027960397017886864, Accuracy: 0.8712\nEpoch 213, Loss: 0.002781112891389057, Accuracy: 0.8713\nEpoch 214, Loss: 0.00276749431330245, Accuracy: 0.8712\nEpoch 215, Loss: 0.002751429205527529, Accuracy: 0.8711\nEpoch 216, Loss: 0.0027380616537993775, Accuracy: 0.8712\nEpoch 217, Loss: 0.002723842751001939, Accuracy: 0.8713\nEpoch 218, Loss: 0.002709630559547804, Accuracy: 0.8713\nEpoch 219, Loss: 0.002695045521832071, Accuracy: 0.8714\nEpoch 220, Loss: 0.0026809486822457983, Accuracy: 0.871\nEpoch 221, Loss: 0.00266716729325708, Accuracy: 0.8709\nEpoch 222, Loss: 0.002654139047372155, Accuracy: 0.871\nEpoch 223, Loss: 0.002639865797245875, Accuracy: 0.8711\nEpoch 224, Loss: 0.0026271248620469124, Accuracy: 0.871\nEpoch 225, Loss: 0.002613979387097061, Accuracy: 0.8709\nEpoch 226, Loss: 0.00260116049204953, Accuracy: 0.871\nEpoch 227, Loss: 0.0025885800941614434, Accuracy: 0.871\nEpoch 228, Loss: 0.0025751701486296952, Accuracy: 0.871\nEpoch 229, Loss: 0.0025622550456319004, Accuracy: 0.8709\nEpoch 230, Loss: 0.002550723711028695, Accuracy: 0.871\nEpoch 231, Loss: 0.002537417007260956, Accuracy: 0.8712\nEpoch 232, Loss: 0.0025241345341783018, Accuracy: 0.871\nEpoch 233, Loss: 0.0025120416458230465, Accuracy: 0.8712\nEpoch 234, Loss: 0.002500651905138511, Accuracy: 0.871\nEpoch 235, Loss: 0.0024891715066041797, Accuracy: 0.8712\nEpoch 236, Loss: 0.0024773248785641046, Accuracy: 0.8713\nEpoch 237, Loss: 0.0024639483547070993, Accuracy: 0.8711\nEpoch 238, Loss: 0.002453864697017707, Accuracy: 0.8713\nEpoch 239, Loss: 0.002442161334911361, Accuracy: 0.8713\nEpoch 240, Loss: 0.00243002197064925, Accuracy: 0.8714\nEpoch 241, Loss: 0.002419068419840187, Accuracy: 0.8712\nEpoch 242, Loss: 0.002407979649724439, Accuracy: 0.8712\nEpoch 243, Loss: 0.0023968364484608174, Accuracy: 0.8711\nEpoch 244, Loss: 0.0023848529922543093, Accuracy: 0.8712\nEpoch 245, Loss: 0.002374072760867421, Accuracy: 0.871\nEpoch 246, Loss: 0.002364567670156248, Accuracy: 0.8709\nEpoch 247, Loss: 0.0023526177264284343, Accuracy: 0.8708\nEpoch 248, Loss: 0.002342516577336937, Accuracy: 0.8709\nEpoch 249, Loss: 0.002331469123600982, Accuracy: 0.871\nEpoch 250, Loss: 0.0023213181976461782, Accuracy: 0.8709\nEpoch 251, Loss: 0.0023112335213227196, Accuracy: 0.8709\nEpoch 252, Loss: 0.0023000263603171335, Accuracy: 0.8709\nEpoch 253, Loss: 0.002289841834572144, Accuracy: 0.871\nEpoch 254, Loss: 0.00228015550179407, Accuracy: 0.871\nEpoch 255, Loss: 0.0022699701314559204, Accuracy: 0.8711\nEpoch 256, Loss: 0.0022601132368436083, Accuracy: 0.871\nEpoch 257, Loss: 0.0022504444426158445, Accuracy: 0.8708\nEpoch 258, Loss: 0.0022413146717008203, Accuracy: 0.8709\nEpoch 259, Loss: 0.002230835007503629, Accuracy: 0.8709\nEpoch 260, Loss: 0.0022212238347856326, Accuracy: 0.8709\nEpoch 261, Loss: 0.00221187682938762, Accuracy: 0.8709\nEpoch 262, Loss: 0.002202078797854483, Accuracy: 0.8708\nEpoch 263, Loss: 0.002193367148283869, Accuracy: 0.8709\nEpoch 264, Loss: 0.002183799280319363, Accuracy: 0.871\nEpoch 265, Loss: 0.002174963945435593, Accuracy: 0.8709\nEpoch 266, Loss: 0.002165203649783507, Accuracy: 0.871\nEpoch 267, Loss: 0.0021560277676326224, Accuracy: 0.8711\nEpoch 268, Loss: 0.0021474329760530963, Accuracy: 0.8711\nEpoch 269, Loss: 0.002138752254541032, Accuracy: 0.8712\nEpoch 270, Loss: 0.002129816602100618, Accuracy: 0.871\nEpoch 271, Loss: 0.0021205379627645014, Accuracy: 0.8709\nEpoch 272, Loss: 0.0021121595607837664, Accuracy: 0.8712\nEpoch 273, Loss: 0.0021040490386076273, Accuracy: 0.871\nEpoch 274, Loss: 0.002094379081390798, Accuracy: 0.871\nEpoch 275, Loss: 0.0020858890347881243, Accuracy: 0.871\nEpoch 276, Loss: 0.0020783076633233578, Accuracy: 0.8712\nEpoch 277, Loss: 0.0020697774388827384, Accuracy: 0.8713\nEpoch 278, Loss: 0.00206155615975149, Accuracy: 0.8712\nEpoch 279, Loss: 0.0020534948847489433, Accuracy: 0.8712\nEpoch 280, Loss: 0.002044400015147403, Accuracy: 0.8712\nEpoch 281, Loss: 0.0020370573655236514, Accuracy: 0.871\nEpoch 282, Loss: 0.002028559994651005, Accuracy: 0.8712\nEpoch 283, Loss: 0.002021205201162957, Accuracy: 0.8711\nEpoch 284, Loss: 0.0020127742271870376, Accuracy: 0.871\nEpoch 285, Loss: 0.002004965897940565, Accuracy: 0.8712\nEpoch 286, Loss: 0.0019970442206249574, Accuracy: 0.871\nEpoch 287, Loss: 0.0019892251683631913, Accuracy: 0.8711\nEpoch 288, Loss: 0.0019814619119279085, Accuracy: 0.8711\nEpoch 289, Loss: 0.0019742965916520914, Accuracy: 0.8711\nEpoch 290, Loss: 0.0019663532212143764, Accuracy: 0.871\nEpoch 291, Loss: 0.0019586466933833434, Accuracy: 0.8711\nEpoch 292, Loss: 0.0019517541368259117, Accuracy: 0.8712\nEpoch 293, Loss: 0.001944709583185613, Accuracy: 0.8712\nEpoch 294, Loss: 0.0019366715245996602, Accuracy: 0.8711\nEpoch 295, Loss: 0.0019297550624469296, Accuracy: 0.8713\nEpoch 296, Loss: 0.0019225001736776902, Accuracy: 0.8712\nEpoch 297, Loss: 0.0019149558906792663, Accuracy: 0.8713\nEpoch 298, Loss: 0.001907810341217555, Accuracy: 0.8713\nEpoch 299, Loss: 0.0019006688188528643, Accuracy: 0.8713\nEpoch 300, Loss: 0.0018933655810542405, Accuracy: 0.8711\nEpoch 301, Loss: 0.0018862318247556686, Accuracy: 0.8713\nEpoch 302, Loss: 0.0018795065802987665, Accuracy: 0.8713\nEpoch 303, Loss: 0.0018727180315181613, Accuracy: 0.8712\nEpoch 304, Loss: 0.0018659405471407808, Accuracy: 0.8713\nEpoch 305, Loss: 0.0018592538722441532, Accuracy: 0.8713\nEpoch 306, Loss: 0.0018523339219973423, Accuracy: 0.871\nEpoch 307, Loss: 0.0018459917162545025, Accuracy: 0.8712\nEpoch 308, Loss: 0.0018391731262090616, Accuracy: 0.8714\nEpoch 309, Loss: 0.0018326091690687462, Accuracy: 0.871\nEpoch 310, Loss: 0.001825822796090506, Accuracy: 0.8713\nEpoch 311, Loss: 0.0018191748426761478, Accuracy: 0.8711\nEpoch 312, Loss: 0.0018127130664652214, Accuracy: 0.8711\nEpoch 313, Loss: 0.0018068717484129593, Accuracy: 0.8711\nEpoch 314, Loss: 0.0018000303907319904, Accuracy: 0.8711\nEpoch 315, Loss: 0.0017939521314110608, Accuracy: 0.871\nEpoch 316, Loss: 0.001787262193101924, Accuracy: 0.8709\nEpoch 317, Loss: 0.001781275784887839, Accuracy: 0.8711\nEpoch 318, Loss: 0.0017752518854103983, Accuracy: 0.8711\nEpoch 319, Loss: 0.0017690376954851673, Accuracy: 0.871\nEpoch 320, Loss: 0.0017626609787112103, Accuracy: 0.8712\nEpoch 321, Loss: 0.0017566242528846488, Accuracy: 0.8711\nEpoch 322, Loss: 0.0017507885175291448, Accuracy: 0.871\nEpoch 323, Loss: 0.0017445094406139106, Accuracy: 0.8711\nEpoch 324, Loss: 0.001738639596151188, Accuracy: 0.8712\nEpoch 325, Loss: 0.0017330055718775838, Accuracy: 0.8711\nEpoch 326, Loss: 0.0017273072953685187, Accuracy: 0.8712\nEpoch 327, Loss: 0.0017207844328368082, Accuracy: 0.8713\nEpoch 328, Loss: 0.0017152234722743742, Accuracy: 0.8712\nEpoch 329, Loss: 0.0017093861455214209, Accuracy: 0.8712\nEpoch 330, Loss: 0.001703717683267314, Accuracy: 0.8711\nEpoch 331, Loss: 0.0016980954818427562, Accuracy: 0.8711\nEpoch 332, Loss: 0.0016924311791080982, Accuracy: 0.871\nEpoch 333, Loss: 0.0016867760970490054, Accuracy: 0.871\nEpoch 334, Loss: 0.0016811643372057006, Accuracy: 0.8711\nEpoch 335, Loss: 0.001675440715334844, Accuracy: 0.871\nEpoch 336, Loss: 0.00167006987525383, Accuracy: 0.8709\nEpoch 337, Loss: 0.001664695209765341, Accuracy: 0.8711\nEpoch 338, Loss: 0.0016590836699469946, Accuracy: 0.8709\nEpoch 339, Loss: 0.0016538941190810874, Accuracy: 0.8711\nEpoch 340, Loss: 0.0016486171932774596, Accuracy: 0.8711\nEpoch 341, Loss: 0.0016430574402329513, Accuracy: 0.8711\nEpoch 342, Loss: 0.0016376413294347002, Accuracy: 0.8712\nEpoch 343, Loss: 0.0016324285630253143, Accuracy: 0.8711\nEpoch 344, Loss: 0.0016271665808744729, Accuracy: 0.871\nEpoch 345, Loss: 0.0016221266047796235, Accuracy: 0.871\nEpoch 346, Loss: 0.0016166865325067193, Accuracy: 0.8711\nEpoch 347, Loss: 0.001611373133782763, Accuracy: 0.8712\nEpoch 348, Loss: 0.0016063527099322529, Accuracy: 0.8711\nEpoch 349, Loss: 0.0016020132141420617, Accuracy: 0.8711\nEpoch 350, Loss: 0.0015968477178830653, Accuracy: 0.8711\nEpoch 351, Loss: 0.001591335761186201, Accuracy: 0.8711\nEpoch 352, Loss: 0.001586663743655663, Accuracy: 0.8711\nEpoch 353, Loss: 0.0015814493678044529, Accuracy: 0.8711\nEpoch 354, Loss: 0.001576767333317548, Accuracy: 0.8711\nEpoch 355, Loss: 0.001571656679152511, Accuracy: 0.8711\nEpoch 356, Loss: 0.0015664439179818145, Accuracy: 0.8712\nEpoch 357, Loss: 0.0015619764442089945, Accuracy: 0.8712\nEpoch 358, Loss: 0.0015573275077622383, Accuracy: 0.871\nEpoch 359, Loss: 0.0015525633713696153, Accuracy: 0.8711\nEpoch 360, Loss: 0.0015474984649335966, Accuracy: 0.8712\nEpoch 361, Loss: 0.001542620529071428, Accuracy: 0.8712\nEpoch 362, Loss: 0.0015381045304820873, Accuracy: 0.8712\nEpoch 363, Loss: 0.001533472722803708, Accuracy: 0.8713\nEpoch 364, Loss: 0.0015287217087461614, Accuracy: 0.8711\nEpoch 365, Loss: 0.0015244885924039408, Accuracy: 0.8712\nEpoch 366, Loss: 0.0015197245820309036, Accuracy: 0.8713\nEpoch 367, Loss: 0.0015152887077420018, Accuracy: 0.8712\nEpoch 368, Loss: 0.0015105630664038472, Accuracy: 0.8713\nEpoch 369, Loss: 0.0015060471434844658, Accuracy: 0.8713\nEpoch 370, Loss: 0.0015017509550671092, Accuracy: 0.8713\nEpoch 371, Loss: 0.0014970530266873539, Accuracy: 0.8713\nEpoch 372, Loss: 0.0014926615337026306, Accuracy: 0.8713\nEpoch 373, Loss: 0.0014883424041909166, Accuracy: 0.8713\nEpoch 374, Loss: 0.0014837359666125848, Accuracy: 0.8713\nEpoch 375, Loss: 0.0014795465845963917, Accuracy: 0.8712\nEpoch 376, Loss: 0.0014751584330224433, Accuracy: 0.8712\nEpoch 377, Loss: 0.0014710961113451049, Accuracy: 0.8712\nEpoch 378, Loss: 0.0014666162710636854, Accuracy: 0.8713\nEpoch 379, Loss: 0.0014625303848879413, Accuracy: 0.8713\nEpoch 380, Loss: 0.0014581719791749493, Accuracy: 0.871\nEpoch 381, Loss: 0.001453880798071623, Accuracy: 0.8712\nEpoch 382, Loss: 0.001449954669806175, Accuracy: 0.871\nEpoch 383, Loss: 0.0014454539271537214, Accuracy: 0.871\nEpoch 384, Loss: 0.00144140433287248, Accuracy: 0.871\nEpoch 385, Loss: 0.0014372784172883258, Accuracy: 0.8712\nEpoch 386, Loss: 0.0014331030694302172, Accuracy: 0.8713\nEpoch 387, Loss: 0.0014291053081979044, Accuracy: 0.8711\nEpoch 388, Loss: 0.0014250443107448518, Accuracy: 0.8711\nEpoch 389, Loss: 0.001421023474249523, Accuracy: 0.8713\nEpoch 390, Loss: 0.001416933377913665, Accuracy: 0.8713\nEpoch 391, Loss: 0.0014130184188252315, Accuracy: 0.871\nEpoch 392, Loss: 0.0014092936442466452, Accuracy: 0.8713\nEpoch 393, Loss: 0.001405337260221131, Accuracy: 0.8711\nEpoch 394, Loss: 0.001401318472926505, Accuracy: 0.8709\nEpoch 395, Loss: 0.0013972655023098924, Accuracy: 0.871\nEpoch 396, Loss: 0.0013935671478975565, Accuracy: 0.8711\nEpoch 397, Loss: 0.0013894930999958888, Accuracy: 0.8711\nEpoch 398, Loss: 0.0013857105001807213, Accuracy: 0.8711\nEpoch 399, Loss: 0.0013816894940100611, Accuracy: 0.8709\n\n\n\nplt.plot(costs, label=\"Cost\")\nplt.title(\"Cost\")\nplt.show()\nplt.plot(accuracies, label=\"Accuracy\")\nplt.title(\"Accuracy\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the accuracy plateaus after a certain point even though costs seems to keep going down.\n\n# Training loop\ntraining_data = training_data[:1000]\nevaluation_data = test_data\n\nn = len(training_data)\nepochs = 100\nmini_batch_size = 10\nalpha = 0.5\nlambd = 0.1\nmodel = VanillaMNIST()\n\ncosts = []\naccuracies = []\n\n# Experiment: comparing regularized and non-regularized models\nfor j in range(epochs):\n    random.shuffle(training_data)\n    mini_batches = [\n        training_data[k : k + mini_batch_size] for k in range(0, n, mini_batch_size)\n    ]\n    epoch_loss = 0\n    for mini_batch in mini_batches:\n        total_loss = torch.tensor([0.0])\n        for x, y in mini_batch:\n            x = torch.tensor(x).squeeze(1)\n            y = torch.tensor(y).squeeze(1)\n\n            # Calculate log loss and add to loss\n            y_hat = model(x)\n            loss = y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat)\n            total_loss += sum(loss)\n\n        m = len(mini_batch)\n        l2_reg = sum((p**2).sum() for p in model.parameters())\n        log_loss = -(1 / m) * total_loss + (lambd / (2 * m)) * l2_reg\n        model.zero_grad()\n        log_loss.backward()\n        for p in model.parameters():\n            p.data = p.data - alpha * p.grad\n        epoch_loss += log_loss.item()\n\n    # Evaluate model on evaluation_data\n    correct = 0\n    for x, y in evaluation_data:\n        x = torch.tensor(x).squeeze(1)\n        y_hat = model(x)\n        if torch.argmax(y_hat) == y:\n            correct += 1\n\n    costs.append(epoch_loss / len(mini_batches))\n    accuracies.append(correct / len(evaluation_data))\n    print(\n        f\"Epoch {j}, Loss: {epoch_loss / len(mini_batches)}, Accuracy: {correct/len(evaluation_data)}\"\n    )\n\nEpoch 0, Loss: 2.811732646226883, Accuracy: 0.688\nEpoch 1, Loss: 2.098552111387253, Accuracy: 0.7897\nEpoch 2, Loss: 1.9765801417827606, Accuracy: 0.806\nEpoch 3, Loss: 1.9365905106067658, Accuracy: 0.8352\nEpoch 4, Loss: 1.9170798242092133, Accuracy: 0.8167\nEpoch 5, Loss: 1.910514018535614, Accuracy: 0.82\nEpoch 6, Loss: 1.8914796423912048, Accuracy: 0.8404\nEpoch 7, Loss: 1.8967450559139252, Accuracy: 0.8208\nEpoch 8, Loss: 1.8929740846157075, Accuracy: 0.7972\nEpoch 9, Loss: 1.884774169921875, Accuracy: 0.8036\nEpoch 10, Loss: 1.8786686623096467, Accuracy: 0.8362\nEpoch 11, Loss: 1.8796998131275178, Accuracy: 0.8405\nEpoch 12, Loss: 1.8953367912769317, Accuracy: 0.7959\nEpoch 13, Loss: 1.8880756640434264, Accuracy: 0.8247\nEpoch 14, Loss: 1.8752827227115632, Accuracy: 0.7868\nEpoch 15, Loss: 1.8846649825572968, Accuracy: 0.8347\nEpoch 16, Loss: 1.8643178629875183, Accuracy: 0.8252\nEpoch 17, Loss: 1.8734481954574584, Accuracy: 0.8158\nEpoch 18, Loss: 1.882199420928955, Accuracy: 0.8237\nEpoch 19, Loss: 1.8695294904708861, Accuracy: 0.8312\nEpoch 20, Loss: 1.8701487267017365, Accuracy: 0.7798\nEpoch 21, Loss: 1.8746571886539458, Accuracy: 0.8275\nEpoch 22, Loss: 1.8737100040912629, Accuracy: 0.8015\nEpoch 23, Loss: 1.8749737966060638, Accuracy: 0.7958\nEpoch 24, Loss: 1.8804458105564117, Accuracy: 0.7597\nEpoch 25, Loss: 1.8606840109825133, Accuracy: 0.8365\nEpoch 26, Loss: 1.8901359486579894, Accuracy: 0.8173\nEpoch 27, Loss: 1.8789975214004517, Accuracy: 0.8171\nEpoch 28, Loss: 1.8712555146217347, Accuracy: 0.8295\nEpoch 29, Loss: 1.8660310840606689, Accuracy: 0.8339\nEpoch 30, Loss: 1.8687487435340882, Accuracy: 0.8134\nEpoch 31, Loss: 1.8768685281276702, Accuracy: 0.85\nEpoch 32, Loss: 1.868555223941803, Accuracy: 0.8298\nEpoch 33, Loss: 1.8685732638835908, Accuracy: 0.8234\nEpoch 34, Loss: 1.8782653164863587, Accuracy: 0.8134\nEpoch 35, Loss: 1.8778847789764403, Accuracy: 0.7236\nEpoch 36, Loss: 1.862051661014557, Accuracy: 0.7965\nEpoch 37, Loss: 1.8617102301120758, Accuracy: 0.8355\nEpoch 38, Loss: 1.859102840423584, Accuracy: 0.8115\nEpoch 39, Loss: 1.8760147869586945, Accuracy: 0.8351\nEpoch 40, Loss: 1.8777650582790375, Accuracy: 0.8285\nEpoch 41, Loss: 1.8769587755203248, Accuracy: 0.8204\nEpoch 42, Loss: 1.887664693593979, Accuracy: 0.8441\nEpoch 43, Loss: 1.894026916027069, Accuracy: 0.7511\nEpoch 44, Loss: 1.8941694283485413, Accuracy: 0.842\nEpoch 45, Loss: 1.8728662788867951, Accuracy: 0.8325\nEpoch 46, Loss: 1.88304829955101, Accuracy: 0.8531\nEpoch 47, Loss: 1.8651540327072142, Accuracy: 0.816\nEpoch 48, Loss: 1.868752690553665, Accuracy: 0.8364\nEpoch 49, Loss: 1.8535161626338958, Accuracy: 0.839\nEpoch 50, Loss: 1.8771829319000244, Accuracy: 0.8512\nEpoch 51, Loss: 1.8845791721343994, Accuracy: 0.7985\nEpoch 52, Loss: 1.8726356601715088, Accuracy: 0.8214\nEpoch 53, Loss: 1.8638122189044952, Accuracy: 0.8263\nEpoch 54, Loss: 1.8748895335197449, Accuracy: 0.8187\nEpoch 55, Loss: 1.8802372539043426, Accuracy: 0.8083\nEpoch 56, Loss: 1.8966482651233674, Accuracy: 0.83\nEpoch 57, Loss: 1.8658703136444093, Accuracy: 0.8292\nEpoch 58, Loss: 1.8631226658821105, Accuracy: 0.8379\nEpoch 59, Loss: 1.8755247700214386, Accuracy: 0.849\nEpoch 60, Loss: 1.8732005119323731, Accuracy: 0.7769\nEpoch 61, Loss: 1.8729154205322265, Accuracy: 0.8411\nEpoch 62, Loss: 1.8653748643398285, Accuracy: 0.8209\nEpoch 63, Loss: 1.8790792655944824, Accuracy: 0.8025\nEpoch 64, Loss: 1.8639093244075775, Accuracy: 0.829\nEpoch 65, Loss: 1.874306182861328, Accuracy: 0.7657\nEpoch 66, Loss: 1.873477020263672, Accuracy: 0.8424\nEpoch 67, Loss: 1.8652974033355714, Accuracy: 0.8212\nEpoch 68, Loss: 1.8774346148967742, Accuracy: 0.8436\nEpoch 69, Loss: 1.8679407298564912, Accuracy: 0.8155\nEpoch 70, Loss: 1.8858467376232146, Accuracy: 0.8535\nEpoch 71, Loss: 1.8650659394264222, Accuracy: 0.8127\nEpoch 72, Loss: 1.889530326128006, Accuracy: 0.8398\nEpoch 73, Loss: 1.8544638645648956, Accuracy: 0.8284\nEpoch 74, Loss: 1.8877349662780762, Accuracy: 0.8206\nEpoch 75, Loss: 1.8774409627914428, Accuracy: 0.835\nEpoch 76, Loss: 1.8813735735416413, Accuracy: 0.8131\nEpoch 77, Loss: 1.892322566509247, Accuracy: 0.8273\nEpoch 78, Loss: 1.8682674264907837, Accuracy: 0.8324\nEpoch 79, Loss: 1.8690711200237273, Accuracy: 0.845\nEpoch 80, Loss: 1.877561490535736, Accuracy: 0.8352\nEpoch 81, Loss: 1.8552240777015685, Accuracy: 0.8346\nEpoch 82, Loss: 1.8648957848548888, Accuracy: 0.8352\nEpoch 83, Loss: 1.864109970331192, Accuracy: 0.843\nEpoch 84, Loss: 1.8665581059455871, Accuracy: 0.8332\nEpoch 85, Loss: 1.859978528022766, Accuracy: 0.8399\nEpoch 86, Loss: 1.8604642963409423, Accuracy: 0.7741\nEpoch 87, Loss: 1.8703574883937835, Accuracy: 0.8261\nEpoch 88, Loss: 1.8701434445381164, Accuracy: 0.8374\nEpoch 89, Loss: 1.8783128380775451, Accuracy: 0.8478\nEpoch 90, Loss: 1.8842784976959228, Accuracy: 0.8366\nEpoch 91, Loss: 1.864534913301468, Accuracy: 0.7543\nEpoch 92, Loss: 1.889243975877762, Accuracy: 0.8016\nEpoch 93, Loss: 1.8540599608421326, Accuracy: 0.8119\nEpoch 94, Loss: 1.8750174713134766, Accuracy: 0.8096\nEpoch 95, Loss: 1.850728349685669, Accuracy: 0.8362\nEpoch 96, Loss: 1.8600666892528535, Accuracy: 0.8332\nEpoch 97, Loss: 1.870409198999405, Accuracy: 0.8343\nEpoch 98, Loss: 1.8518181240558624, Accuracy: 0.8012\nEpoch 99, Loss: 1.8821170127391815, Accuracy: 0.8406\n\n\n\nplt.plot(costs, label=\"Cost\")\nplt.title(\"Cost\")\nplt.show()\nplt.plot(accuracies, label=\"Accuracy\")\nplt.title(\"Accuracy\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization doesn’t seem to be working. Need to figure out what’s going wrong."
  },
  {
    "objectID": "learning/nielson-deep-learning/chapter-5.html",
    "href": "learning/nielson-deep-learning/chapter-5.html",
    "title": "Replication of some chapter 5 results",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport mnist_loader  # noqa\nimport torch\nimport random\nimport torch.nn as nn  # noqa\nimport network2\n\n\ntraining_data, validation_data, test_data = mnist_loader.load_data_wrapper()\ntraining_data = list(training_data)\nvalidation_data = list(validation_data)\ntest_data = list(test_data)\n\n\nnet = network2.Network([784, 30, 10])\n\n\nnet.SGD(\n    training_data=training_data,\n    epochs=30,\n    mini_batch_size=10,\n    eta=0.1,\n    lmbda=5.0,\n    evaluation_data=validation_data,\n    monitor_evaluation_accuracy=True,\n)\n\nEpoch 0 training complete\nAccuracy on evaluation data: 9325 / 10000\nEpoch 1 training complete\nAccuracy on evaluation data: 9471 / 10000\nEpoch 2 training complete\nAccuracy on evaluation data: 9499 / 10000\n\n\nKeyboardInterrupt:"
  }
]