[
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "Quick thoughts on ‘Mere Christianity’ by C.S Lewis\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\n\n\n\n\n\nArguments for and against God\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\n\n\n\n\n\nComparison\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\n\n\n\n\n\nWhat to do about the future\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\n\n\n\n\n\nHow did intelligence evolve and why isn’t it the single most important clue to how we might re-create intelligence?\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\n\n\n\n\n\nTrying to get good at math and science\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\n\n\n\n\n\nAmbitious but at peace\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kim Young Jin",
    "section": "",
    "text": "Hi! I’m currently studying Computer Science at the National University of Singapore. In a past life I was obsessed with creating software products (still am, to an extent). Now I am working to become a computational neuroscientist. I am at the core a deeply curious person - and one of the most curiosity-inducing mysteries of the world is that of intelligence. What is it, and how do we create it?\nMy thesis is that artificial intelligence needs to take inspiration from the only proof-of-existence we have, the brain, if we want a shot at AGI. Many say neuroscience is pre-paradigmic; on an initial survey of the field, it seems like there is a greater focus on finding interesting correlations rather than testing overarching theoretical models. Can the creation of artificial systems emulating discrete aspects of intelligence bridge that gap between data and conceptual clarity?\nI hope to find out.\n\nA TLDR of what I’ve been doing with life:\n\nIn my K-12 years I thought I wanted to be a writer so I got published in some obscure literary journals.\nStarted programming in high school so that I could make goofy websites.\nStudied CS at the National University of Singapore. Built huge Android apps to validate business ideas.\nServed in the Korean army for 2 years. In my free time I built random stuff using my iPad as my IDE (laptops weren’t allowed).\nCold emailed Zeet CEO for an internship because they were solving a problem I was facing (cloud deployment). Added some features, fixed some bugs. Flew to SF to meet the team.\nCold emailed Locofy CEO for an internship because they were solving a problem I was facing (frontend codegen from Figma). I believed the product would be re-invent the way frontends are built.\nTook a Leave of Absence to go all-in on the biggest product launch in Locofy’s history, where we automated away a huge portion of the Figma-to-frontend workflow for engineers.\nCame back to school, became fascinated by the brain after reading several books.\nCurrently trying to do some research at the intersection of AI and neuroscience by solving the Monkey and Banana problem with neuro-inspired methods."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m currently studying Computer Science at the National University of Singapore. In a past life I was obsessed with creating software products (still am, to an extent). Now I am working to become a computational neuroscientist. I am at the core a deeply curious person - and one of the most curiosity-inducing mysteries of the world is that of intelligence. What is it, and how do we create it?\nMy thesis is that artificial intelligence needs to take inspiration from the only proof-of-existence we have, the brain, if we want a shot at AGI. Many say neuroscience is pre-paradigmic; on an initial survey of the field, it seems like there is a greater focus on finding interesting correlations rather than testing overarching theoretical models. Can the creation of artificial systems emulating discrete aspects of intelligence bridge that gap between data and conceptual clarity?\nA TLDR of what I’ve been doing with life:\n\nIn my K-12 years I thought I wanted to be a writer so I got published in some obscure literary journals.\nStarted programming in high school so that I could make goofy websites.\nStudied CS at the National University of Singapore. Built huge Android apps to validate business ideas.\nServed in the Korean army for 2 years. In my free time I built random stuff using my iPad as my IDE (laptops weren’t allowed).\nCold emailed Zeet CEO for an internship because they were solving a problem I was facing (cloud deployment). Added some features, fixed some bugs. Flew to SF to meet the team.\nCold emailed Locofy CEO for an internship because they were solving a problem I was facing (frontend codegen from Figma). I believed the product would be re-invent the way frontends are built.\nTook a Leave of Absence to go all-in on the biggest product launch in Locofy’s history, where we automated away a huge portion of the Figma-to-frontend workflow for engineers.\nCame back to school, became fascinated by the brain after reading several books.\nCurrently trying to do some research at the intersection of AI and neuroscience by solving the Monkey and Banana problem with neuro-inspired methods."
  },
  {
    "objectID": "learning.html",
    "href": "learning.html",
    "title": "Learning",
    "section": "",
    "text": "Sutton’s Reinforcement Learning Chapter 2: Multi-armed Bandits\n\n\nImplementations of algorithms and code solutions for some exercises\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learning/test/index.html",
    "href": "learning/test/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "learning/other/index.html",
    "href": "learning/other/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "learning/reinforcement-learning/index.html",
    "href": "learning/reinforcement-learning/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "learning/other/hey.html",
    "href": "learning/other/hey.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "learning/reinforcement-learning/sutton-and-barto/chapter-2.html",
    "href": "learning/reinforcement-learning/sutton-and-barto/chapter-2.html",
    "title": "Sutton’s Reinforcement Learning Chapter 2: Multi-armed Bandits",
    "section": "",
    "text": "import numpy as np  # noqa\nimport matplotlib.pyplot as plt  # noqa\n\n\nComparison of 10-armed bandit problem between ε-greedy and greedy methods\nActions estimates are calculated using the sample-average method.\n\ndef sample_action(true_value):\n    return np.random.normal(true_value, 1)\n\n\ndef executor(runs=2000, time_steps=1000, epsilon=0.1):\n    rewards_agg = []\n    for i in range(runs):\n        rewards_history = []\n\n        # Define true action values\n        qstar = np.random.normal(0, 1, size=10)\n\n        # Action value estimates are all initially 0\n        qa = np.zeros(10)\n        action_counts = np.zeros(10)\n\n        for n in range(time_steps):\n            if np.random.rand() &gt; epsilon:\n                # Choose action greedily\n                action = np.argmax(qa)\n            else:\n                action = np.random.choice(np.arange(10))\n\n            # Sample action_value\n            reward = sample_action(qstar[action])\n            rewards_history.append(reward)\n\n            # Update action count\n            action_counts[action] += 1\n            # Update action value estimate\n            qa[action] = qa[action] + 1 / action_counts[action] * (reward - qa[action])\n\n        rewards_agg.append(rewards_history)\n\n    stacked = np.vstack(rewards_agg)\n    averaged_array = np.mean(stacked, axis=0)\n    return averaged_array\n\n\n# Plot different runs of the experiment in different colors on the same plot\nresults = executor(epsilon=0.1)\nplt.plot(results, color=\"blue\")\n\nresults = executor(epsilon=0.01)\nplt.plot(results, color=\"red\")\n\nresults = executor(epsilon=0)\nplt.plot(results, color=\"green\")\n\n# Increase width of plot\nplt.gcf().set_size_inches(10, 5)\n\n\n\n\n\n\n\n\n\n\nExercise 2.5\nComparison of non-stationary 10-armed bandit problem between sample-average and constant step-size parameter methods\n\ndef sample_action(true_value):\n    return np.random.normal(true_value, 1)\n\n\ndef update_true_action_values(true_values):\n    return [value + np.random.normal(0, 0.01) for value in true_values]\n\n\n# Define number of runs\ndef executor(runs=2000, time_steps=1000, epsilon=0.1, alpha=0.1, constant_step=False):\n    rewards_agg = []\n    for i in range(runs):\n        rewards_history = []\n\n        # Define true action values\n        qstar = np.random.normal(0, 1, size=10)\n\n        # Action value estimates are all initially 0\n        qa = np.zeros(10)\n        action_counts = np.zeros(10)\n\n        for n in range(1, time_steps + 1):\n            if np.random.rand() &gt; epsilon:\n                # Choose action greedily\n                action = np.argmax(qa)\n            else:\n                action = np.random.choice(np.arange(10))\n\n            # Sample action_value\n            reward = sample_action(qstar[action])\n            rewards_history.append(reward)\n\n            # Update action count\n            action_counts[action] += 1\n\n            # Update action value estimate\n            if constant_step:\n                qa[action] = qa[action] + (reward - qa[action]) * alpha\n            else:\n                qa[action] = qa[action] + (reward - qa[action]) * (\n                    1 / action_counts[action]\n                )\n\n            # Update true action values (to simulate non-stationarity)\n            qstar = update_true_action_values(qstar)\n\n        rewards_agg.append(rewards_history)\n\n    stacked = np.vstack(rewards_agg)\n    averaged_array = np.mean(stacked, axis=0)\n    return averaged_array\n\n\nresults = executor(time_steps=10000, epsilon=0.1, constant_step=False)\nplt.plot(results, color=\"blue\")\n\nresults = executor(time_steps=10000, epsilon=0.1, constant_step=True)\nplt.plot(results, color=\"red\")\n\nplt.gcf().set_size_inches(10, 5)\n\n\n\n\n\n\n\n\n\n\nExercise 2.11\n\n# Need to run for different epsilon values\n\nresults = []\nfor i in range(-5, 2):\n    epsilon = 2**i\n\n    results = executor(\n        runs=1000, time_steps=200000, epsilon=epsilon, constant_step=True\n    )\n\n    # Average the last 100,000 rewards\n    average_reward = np.mean(results[-100000:])\n    results.append(average_reward)\n\n# X-axis should be the epsilons\nplt.plot(range(-5, 2), results, color=\"blue\")\n\nplt.gcf().set_size_inches(10, 5)\n\nAttributeError: 'numpy.ndarray' object has no attribute 'append'"
  },
  {
    "objectID": "posts/ambitious-and-content.html",
    "href": "posts/ambitious-and-content.html",
    "title": "Temporary notes",
    "section": "",
    "text": "For most of my life I’ve felt a deep discontent myself.\nWe hear a lot of advice to “love yourself”, have a “kind inner voice”, and so on. I’ve always felt these were coping mechanisms, anesthesia to dull the pain of reality. Better to embrace the pain, I thought. Invigorate yourself with the fury of self-hatred and envy. Only the unambitious are content with the warm fuzziness of satisfaction. It’s like what they always say: the most successful people contain within them a contradiction - near delusional levels of self-belief fueled by an intense anxiety around never being good enough.\nI’m still in my early twenties, but my stance has changed somewhat. Ambition doesn’t mean you need to constantly hate where you are at. Ambition is about the magnitude of your end goals, not your mental state on the way there. If you could take a different road to the same destination, one that preserves your peace of mind, why wouldn’t you?\nAnd that other, more peaceful road, might get you to your goal faster. With peace of mind, you can keep going, day in and day out. By prioritising peace of mind, you are making the choice to run a marathon instead of a sprint. Who will be fresh-headed and grinding, ten years from now? The ambitious one who looks at every failure of his as the biggest catastrophe he has ever faced? Or the ambitious one who controls what is within his control, lets go of what is not, and is at peace with himself?\nReal strength comes from knowing that there is absolutely nothing that can happen to you that will stop you from continuing forward.\nHacker news discussion"
  },
  {
    "objectID": "posts/existence-of-god.html",
    "href": "posts/existence-of-god.html",
    "title": "Arguments for and against God",
    "section": "",
    "text": "A list of arguments and counter-arguments for the existence of God. For personal reference.\nMuch of the material for this first draft comes from Bertrand Russell’s “Why I Am Not a Christian”; what follows is a rough summary of his points.\n\nThe First Cause Argument\ngoes like this: everything has a cause. So there must have been a first cause that existed before any other cause. And that first cause, existing for eternity into the past and future, is God. This is one of those arguments that sounds plausible at first, but on closer inspection there is a logical error. If the premise is that everything has a cause, then God shuold have a cause too. If then one argues that no, there exists some things which do not have a cause, one of which is God, we can just as easily ask - why cannot the Universe be without cause? Either way, the existence of a God is not necessitated."
  },
  {
    "objectID": "posts/trying-to-get-good-at-math-and-science.html",
    "href": "posts/trying-to-get-good-at-math-and-science.html",
    "title": "Trying to get good at math and science",
    "section": "",
    "text": "In my primary school years, I did well at science and math simply by caring more about grades than my peers. It was pure conscientious at play. On hindsight there were probably a lot of people more gifted than me in my class, but\nI have a vision of learning Math and Science.\nGetting good at Math and Science occupies a hazy middle ground between memory and understanding, between mechanical and conceptual understanding. Memory and understanding are also not straightforward. There are layers. Understanding might beget an Aha! moment that slips cunningly away, tricking us into reassurance. Understanding once, does not mean you understand forever. So inevitably there is a component of memory in understanding. But there are also some types of understanding that are more “meta” - broad strokes that set the stage and context behind so many of a subject’s concepts. This kind of understanding can act like a pinboard for the rest of the subject, helping you see the connections and draw the lines.\nAnd further still, there is a type of understanding that can only come from practice, and this is the most difficult to acquire of them all. It can be fun, and even easy, to watch beautiful 3B1B videos or read textbooks and “understand” their theorems. You might even have memorized most of the concepts presented. But does that mean you can apply them to problems?\nTo solve the really challening problems, you need to twist and distort concepts, fit them together this way and that. If understanding gained from reading is like listening to your Dad explain how to balance on a 2-wheel bicycle, understanding gained from practice is like using every ounce of dexterity and physical instinct in your body to balance, balance, somehow try to balance!\nReading pushes information into your brain. Practice pulls from it. As strands of half-formed ideas are wrenched from your brain, they tangle with one another and with concepts wholly unconnected from what you may have been learning about. It is through that process that understanding starts to show hints of becoming yours.\nI have been thinking about how to make it easier to learn from textbooks.\nHow can we use LLMs to augment the learning process of a textbook? First, we need to accept that learning is never going to become “easy”. You need to put in the hours and the concentration, and no AI is going to do that for you. What it CAN do is act like a tool to streamline how you practice questions and try to remember content.\nA sort of “ecosystem” for autodidacts? Online forum for crowdsourced answers for textbooks, and an LLM-powered textbook reader for creating flashcards and knowledge graphs for textbooks. Maybe this can be community-driven as well? No, but maybe that defeats the purpose. Everyone just generates their own personalized flashcards, for their own learning."
  },
  {
    "objectID": "drafts/chapter-2.html",
    "href": "drafts/chapter-2.html",
    "title": "Sutton’s Reinforcement Learning Chapter 2: Multi-armed Bandits",
    "section": "",
    "text": "import numpy as np  # noqa\nimport matplotlib.pyplot as plt  # noqa\n\n\nComparison of 10-armed bandit problem between ε-greedy and greedy methods\nActions estimates are calculated using the sample-average method.\n\ndef sample_action(true_value):\n    return np.random.normal(true_value, 1)\n\n\ndef executor(runs=2000, time_steps=1000, epsilon=0.1):\n    rewards_agg = []\n    for i in range(runs):\n        rewards_history = []\n\n        # Define true action values\n        qstar = np.random.normal(0, 1, size=10)\n\n        # Action value estimates are all initially 0\n        qa = np.zeros(10)\n        action_counts = np.zeros(10)\n\n        for n in range(time_steps):\n            if np.random.rand() &gt; epsilon:\n                # Choose action greedily\n                action = np.argmax(qa)\n            else:\n                action = np.random.choice(np.arange(10))\n\n            # Sample action_value\n            reward = sample_action(qstar[action])\n            rewards_history.append(reward)\n\n            # Update action count\n            action_counts[action] += 1\n            # Update action value estimate\n            qa[action] = qa[action] + 1 / action_counts[action] * (reward - qa[action])\n\n        rewards_agg.append(rewards_history)\n\n    stacked = np.vstack(rewards_agg)\n    averaged_array = np.mean(stacked, axis=0)\n    return averaged_array\n\n\n# Plot different runs of the experiment in different colors on the same plot\nresults = executor(epsilon=0.1)\nplt.plot(results, color=\"blue\")\n\nresults = executor(epsilon=0.01)\nplt.plot(results, color=\"red\")\n\nresults = executor(epsilon=0)\nplt.plot(results, color=\"green\")\n\n# Increase width of plot\nplt.gcf().set_size_inches(10, 5)\n\n\n\n\n\n\n\n\n\n\nExercise 2.5\nComparison of non-stationary 10-armed bandit problem between sample-average and constant step-size parameter methods\n\ndef sample_action(true_value):\n    return np.random.normal(true_value, 1)\n\n\ndef update_true_action_values(true_values):\n    return [value + np.random.normal(0, 0.01) for value in true_values]\n\n\n# Define number of runs\ndef executor(runs=2000, time_steps=1000, epsilon=0.1, alpha=0.1, constant_step=False):\n    rewards_agg = []\n    for i in range(runs):\n        rewards_history = []\n\n        # Define true action values\n        qstar = np.random.normal(0, 1, size=10)\n\n        # Action value estimates are all initially 0\n        qa = np.zeros(10)\n        action_counts = np.zeros(10)\n\n        for n in range(1, time_steps + 1):\n            if np.random.rand() &gt; epsilon:\n                # Choose action greedily\n                action = np.argmax(qa)\n            else:\n                action = np.random.choice(np.arange(10))\n\n            # Sample action_value\n            reward = sample_action(qstar[action])\n            rewards_history.append(reward)\n\n            # Update action count\n            action_counts[action] += 1\n\n            # Update action value estimate\n            if constant_step:\n                qa[action] = qa[action] + (reward - qa[action]) * alpha\n            else:\n                qa[action] = qa[action] + (reward - qa[action]) * (\n                    1 / action_counts[action]\n                )\n\n            # Update true action values (to simulate non-stationarity)\n            qstar = update_true_action_values(qstar)\n\n        rewards_agg.append(rewards_history)\n\n    stacked = np.vstack(rewards_agg)\n    averaged_array = np.mean(stacked, axis=0)\n    return averaged_array\n\n\nresults = executor(time_steps=10000, epsilon=0.1, constant_step=False)\nplt.plot(results, color=\"blue\")\n\nresults = executor(time_steps=10000, epsilon=0.1, constant_step=True)\nplt.plot(results, color=\"red\")\n\nplt.gcf().set_size_inches(10, 5)\n\n\n\n\n\n\n\n\n\n\nExercise 2.11\n\n## TODO"
  },
  {
    "objectID": "posts/mere-christianity.html",
    "href": "posts/mere-christianity.html",
    "title": "Quick thoughts on ‘Mere Christianity’ by C.S Lewis",
    "section": "",
    "text": "1. Pride\nC.S Lewis talks at length about how Pride is one of those things that is easiest to criticize in others, but hardest to find in yourself.\nIt is effortless to despise the hubris of others, to whisper of their wicked arrogance behind their backs. But the great irony is that those who most despise pride in others are in fact the most prideful of the bunch. You cannot bear to see a friend boast and receive all the attention, because you secretly wish it were you in the spotlight instead. You cannot bear to see a neighbour be successful, because you wish it were you claiming success instead.\nI had a friend who always had that cocky air about him. It used to bother me a bit too much. I thought, “What good did he do, that he has the right to strut about like that?” Implicit in this statement is my belief that my friend is not as good as he makes himself out to be. It bothered me so much, because it pricked at my own outsized Pride.\nA low self-esteem, and insecurity, and self-hate - it may seem paradoxical, but now I see that these, too, are manifestations of Pride. When you cannot accept who you are - when you refuse to let go of ambition - every small blemish is reason for you to bemoan your wretched self. Why am I so dumb? Why am I so slow? A great self-loathing grows from this stream of negativity.\nBut if you so hate these shortcomings in yourself, how much more would you despise it in others? All the people who have even less than what you have - what about them? The secretly Prideful soul derives a twisted satisfaction from looking down at those beneath him. “At least I am better than them”. “Dull as I may be, at least I am less dull than them”. Resentment at those above you translates into hate towards youself, and a superiority complex at those beneath you.\nOnly when you embrace yourself, as you are, can you embrace others, as they are.\n\n\n2. Morality\nC.S Lewis points to the moral compass within every one of us as proof of God. As physical objects obey gravity, so do we obey the moral law. But unlike gravity, we are given a choice. Our free will means that every man, every day, makes decisions that break or keep the moral law.\nI would like to believe in this. It is a compelling argument, except for that one annoying possibility that our morality comes from our Biology, not from some objective moral truth. What if evolution selected for those populations that developed emotional responses to morally “right” or “wrong” actions? Certainly we can see how it might have helped some popluations survive and not others. If that is the case, we cannot argue for an objective moral truth by only “introspecting” on our consciences.\n\n\n3. Free will\nAnd that, of course, leads to the thorny issue of free will. Without free will, none of Christianity makes sense. How can we have free will? How would free will even work? In some sense, there are echoes of the “First Cause Argument” for the existence of God. It says that everything must have a cause, and so the Universe also needs to have a “first cause”. Only God can be that first cause, because God is the only thing that can exist without itself having a cause (this argument is actually flawed - the basis is that everything must have a cause, so we are contradicting ourselves by saying that God does not have a cause).\nCould we argue in a similar way about free will? That everything needs to have a cause, but free will, as something given by God, does not have a “cause” and is solely the product of our self-determining souls?\nWhat if the day comes when we fully understand how human consciousness works, and it turns out that we actually are just machines running on good old cause and effect, living under the illusion of autonomy?\n\n\n4. I hope it was all true\nOne part of me is sceptical, but another side of me wishes it were all true. I am starting to see the consequences that an “amoral” life and mindset is having on my psyche. It is not pretty. There is constantly a feeling of emptiness, a desperate wish that life was more than my petty ambitions and the little games that we all play.\nOne can only pray. But how can I pray, if I know no God?"
  },
  {
    "objectID": "drafts/test.html",
    "href": "drafts/test.html",
    "title": "Kim Young Jin",
    "section": "",
    "text": "import numpy as np  # noqa\nimport matplotlib.pyplot as plt  # noqa\n\n\n# GAMBLER PROBLEM\n#\n# EXERCISE 4.9\n#\n# Implement value iteration for the gambler's problem and solve it for ph = 0.25 and ph = 0.55. In programming, you may\n# find it convenient to introduce two dummy states corresponding to termination with capital of 0 and 100, giving them\n# values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. Are your results stable as theta→0?\n\n\n# A wrapper class for parameters of the algorithm\nclass Params:\n    def __init__(self):\n        # Money goal to reach\n        self.max_money = 100\n\n        # Possible values of probability of the coin coming up heads (winning the bet)\n        self.p_hs = [0.25, 0.4, 0.55]\n        # self.p_hs = [0.4]\n\n        # Small number determining the accuracy of policy evaluation's estimation\n        self.theta = 10**-10\n        # self.theta = 10**-1000\n        # self.theta = 0.00001\n\n        # Discount\n        self.gamma = 1\n\n\n# A wrapper class for Value Iteration algorithm\nclass ValueIteration:\n    def __init__(self, p_h, params):\n        \"\"\"\n        :param p_h: probability of the coin coming up heads (winning the bet)\n        :param params: the parameters instance\n        \"\"\"\n        # Set up the ph value\n        self.p_h = p_h\n\n        # Set up parameters\n        self.params = params\n\n        # All possible states\n        self.S = np.arange(1, self.params.max_money)\n\n        # Value function\n        self.V = np.zeros(self.params.max_money + 1)\n        self.V[0] = 0\n        self.V[self.params.max_money] = 1\n\n        # List of value functions\n        self.Vs = []\n\n        # Policy function\n        self.pi = None\n\n        # Number of sweeps needed to complete the problem\n        self.sweep_count = None\n\n    def solve_problem(self):\n        \"\"\"\n        Resolve Gambler Problem using Value Iteration\n        \"\"\"\n        self.sweep_count = 0\n        while True:\n            delta = 0\n            for s in self.S:\n                v = self.V[s]\n                self.V[s] = np.max([self.V_eval(s, a) for a in self.A(s)])\n                delta = np.maximum(delta, abs(v - self.V[s]))\n            if self.sweep_count &lt; 3:\n                self.Vs.append(self.V.copy())\n            self.sweep_count += 1\n            if delta &lt; self.params.theta:\n                break\n        print(\"Sweeps needed:\", self.sweep_count)\n        self.Vs.append(self.V.copy())\n        self.pi = [\n            self.A(s)[np.argmax([self.V_eval(s, a) for a in self.A(s)])] for s in self.S\n        ]\n\n    def A(self, s):\n        \"\"\"\n        Get all possible actions given a state\n        :param s: state\n        :return: possible actions\n        \"\"\"\n        # All possible actions\n        return np.arange(1, np.minimum(s, self.params.max_money - s) + 1)\n\n    def V_eval(self, s, a):\n        \"\"\"\n        Compute value given a state and an action for the state following the formula:\n        sum over all s',r of p(s',r|s, a)[r + gamma*V(s')]\n        :param s: state\n        :param a: action\n        :return: value\n        \"\"\"\n        return self.params.gamma * self.V[\n            s + a\n        ] * self.p_h + self.params.gamma * self.V[s - a] * (1 - self.p_h)\n\n    def print_Vs(self):\n        \"\"\"\n        Print values for the first three sweeps and for the last one\n        \"\"\"\n        plt.figure()\n        plt.plot(self.Vs[0], label=\"sweep 1\")\n        plt.plot(self.Vs[1], label=\"sweep 2\")\n        plt.plot(self.Vs[2], label=\"sweep 3\")\n        plt.plot(self.Vs[3], label=\"sweep {}\".format(self.sweep_count))\n        plt.legend(loc=\"upper left\")\n        plt.xlabel(\"Capital\")\n        plt.ylabel(\"Value estimates\")\n        plt.title(\"Values ph={}\".format(self.p_h))\n\n    def print_pi(self):\n        \"\"\"\n        Print policy\n        \"\"\"\n        plt.figure()\n        plt.step(self.S, self.pi)\n        plt.xlabel(\"Capital\")\n        plt.ylabel(\"Final policy (stake)\")\n        plt.title(\"pi ph={}\".format(self.p_h))\n\n\ndef exercise4_9():\n    print(\"Exercise 4.9\")\n\n    # Set up parameters\n    params = Params()\n\n    for p_h in params.p_hs:\n        print(\"Problem with ph:\", p_h)\n\n        # Set up the algorithm\n        policy_iteration = ValueIteration(p_h, params)\n\n        # Solve the problem\n        policy_iteration.solve_problem()\n\n        # Show results\n        policy_iteration.print_Vs()\n        policy_iteration.print_pi()\n\n\nexercise4_9()\nplt.show()\n\nExercise 4.9\nProblem with ph: 0.25\nSweeps needed: 16\nProblem with ph: 0.4\nSweeps needed: 19\nProblem with ph: 0.55\nSweeps needed: 1838"
  },
  {
    "objectID": "drafts/chapter-4.html",
    "href": "drafts/chapter-4.html",
    "title": "Sutton’s Reinforcement Learning Chapter 4: Dynamic Programming",
    "section": "",
    "text": "import numpy as np  # noqa\nfrom typing import Callable  # noqa\nimport math  # noqa\nimport matplotlib.pyplot as plt  # noqa\nimport itertools  # noqa\nimport functools  # noqa\nfrom scipy.stats import poisson  # noqa\nimport time  # noqa\n\n\nloc_1_cars = 20\nloc_2_cars = 2\n\nnum_cars = 20\n\n# Num rent is some infinite distribution\n# But in reality it ranges from number of cars available to 0 (we can adjust the PMF correspondingly)\n# Num return is also some infinite dist, but it ranges from 0 to num_cars - num_rent\n\n# So given loc_1_cars and loc_2_cars, we can find the range of num_rent and num_ret respectively\n# We iterate and come up with all possible combinations of these values\n# Use their probability distributions to calculate probabilities of each end state\n\n\npoisson.pmf([1, 2, 3], 3)\n\narray([0.14936121, 0.22404181, 0.22404181])\n\n\n\n# one = np.arange(5)\n# two = np.arange(5)\n# one - two\n\n\ndef final_state_prob_dist(max_cars, loc_1_cars, rent_poisson, return_poisson):\n    # Multiply number of cars that can be rented with number of cars that can be returned\n    num_rows = (loc_1_cars + 1) * (max_cars + 1)\n    loc_1_cars_list = np.repeat(loc_1_cars, num_rows)\n    loc_1_cars_rent_list = np.repeat(np.arange(loc_1_cars + 1), max_cars + 1)\n    loc_1_cars_return_list = np.tile(np.arange(max_cars + 1), loc_1_cars + 1)\n    # print(loc_1_cars_list)\n    # print(loc_1_cars_rent_list)\n    # print(loc_1_cars_return_list)\n\n    final = loc_1_cars - loc_1_cars_rent_list + loc_1_cars_return_list\n    # print(final)\n    # Get the indices of the states where the final number of cars is less than or equal to max_cars\n    indices = np.where(final &lt;= max_cars)\n\n    loc_1_cars_list = loc_1_cars_list[indices]\n    loc_1_cars_rent_list = loc_1_cars_rent_list[indices]\n    loc_1_cars_return_list = loc_1_cars_return_list[indices]\n    # print(loc_1_cars_list)\n    # print(loc_1_cars_rent_list)\n    # print(loc_1_cars_return_list)\n\n    # Apply poission.pmf(loc_1_cars_rent_list, 3) to get the probabilities of each rent value\n\n    loc_1_cars_rent_prob = np.zeros(len(loc_1_cars_rent_list))\n    max_rent_prob = 1 - poisson.cdf(loc_1_cars - 1, rent_poisson)\n    for i in range(len(loc_1_cars_rent_list)):\n        if loc_1_cars_rent_list[i] == loc_1_cars:\n            loc_1_cars_rent_prob[i] = max_rent_prob\n        else:\n            loc_1_cars_rent_prob[i] = poisson.pmf(loc_1_cars_rent_list[i], rent_poisson)\n    # print(\"Loc 1 cars rent prob\", loc_1_cars_rent_prob)\n\n    max_ret = loc_1_cars - loc_1_cars_rent_list\n    # print(\"Max_ret\", max_ret)\n    loc_1_cars_return_prob = np.zeros(len(loc_1_cars_return_list))\n\n    for i in range(len(loc_1_cars_return_list)):\n        if loc_1_cars_return_list[i] == max_ret[i]:\n            loc_1_cars_return_prob[i] = 1 - poisson.cdf(max_ret[i] - 1, return_poisson)\n        else:\n            loc_1_cars_return_prob[i] = poisson.pmf(\n                loc_1_cars_return_list[i], return_poisson\n            )\n\n    # print(\"Loc 1 cars return prob\", loc_1_cars_return_prob)\n    # print(\"Sum: \", sum(np.unique(loc_1_cars_return_prob)))\n    final_cars = loc_1_cars_list - loc_1_cars_rent_list + loc_1_cars_return_list\n    final_prob = loc_1_cars_rent_prob * loc_1_cars_return_prob\n    # Sum up all final_prob for each unique end state\n    sums = np.bincount(final_cars, weights=final_prob)\n    # sum(sums)\n    return sums\n\n\nfinal_states = [1, 1, 2, 2, 3, 3]\nfinal_prob = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\nunique_final_probs = np.bincount(final_states, weights=final_prob)\nunique_final_probs\n\narray([0. , 0.3, 0.7, 1.1])\n\n\n\npoisson.pmf(np.arange(4), 3)\n\narray([0.04978707, 0.14936121, 0.22404181, 0.22404181])\n\n\n\npoisson.cdf(3, 3)\n\n0.6472318887822313\n\n\n\nnp.arange(4)\n\narray([0, 1, 2, 3])\n\n\n\n# Parameters\nloc_1_cars = 20000\nloc_2_cars = 2\nnum_cars = 100000\n\n# Generate possible values for rent and returned\nrent = np.arange(loc_1_cars + 1)\nreturned = np.arange(num_cars - loc_1_cars + 1)\n\n# Create meshgrid for all combinations\nrent_grid, returned_grid = np.meshgrid(rent, returned, indexing=\"ij\")\n\n# Calculate remaining cars\nremaining_grid = loc_1_cars - rent_grid\n\n# Calculate sum of remaining and returned cars\nsum_grid = remaining_grid + returned_grid\n\n# Filter combinations where sum exceeds num_cars\nvalid_combinations = sum_grid &lt;= num_cars\n\n# Extract valid rent and returned combinations\nvalid_rent = rent_grid[valid_combinations]\nvalid_returned = returned_grid[valid_combinations]\n\nprint(\"Valid rent combinations:\", valid_rent)\nprint(\"Valid returned combinations:\", valid_returned)\n\nValid rent combinations: [    0     0     0 ... 20000 20000 20000]\nValid returned combinations: [    0     1     2 ... 79998 79999 80000]\n\n\n\n# %%timeit\n\nloc_1_cars = 20000\nloc_2_cars = 2\nnum_cars = 100000\n\nfor rent in range(loc_1_cars + 1):\n    remaining = loc_1_cars - rent\n    for returned in range(num_cars - remaining + 1):\n        1 + 1\n\n\nExercise 4.7\n\nclass JacksCarRental:\n    def __init__(\n        self,\n        cars=20,\n        cost_per_car_moved=2,\n        profit_per_car=10,\n        loc_1_rent=3,\n        loc_2_rent=4,\n        loc_1_ret=3,\n        loc_2_ret=2,\n        max_action=5,\n        theta=10,\n        gamma=0.9,\n    ):\n        self.cars = cars\n        self.cost_per_car_moved = cost_per_car_moved\n        self.profit_per_car = profit_per_car\n        self.loc_1_rent = loc_1_rent\n        self.loc_2_rent = loc_2_rent\n        self.loc_1_ret = loc_1_ret\n        self.loc_2_ret = loc_2_ret\n        self.max_action = max_action\n        self.theta = theta\n        self.gamma = gamma\n\n        (\n            self.loc_1_rent_dist,\n            self.loc_2_rent_dist,\n            self.loc_1_ret_dist,\n            self.loc_2_ret_dist,\n        ) = (\n            poisson(self.loc_1_rent),\n            poisson(self.loc_2_rent),\n            poisson(self.loc_1_ret),\n            poisson(self.loc_2_ret),\n        )\n\n        self.states = list(itertools.product(range(cars + 1), range(cars + 1)))\n\n    def policy_iteration(self):\n        policy_stable = False\n\n        state_values = np.zeros((self.cars + 1, self.cars + 1), dtype=int)\n        policy = np.zeros((self.cars + 1, self.cars + 1), dtype=int)\n        policies = []\n        iterations = 0\n        while not policy_stable:\n            new_state_values = self.policy_evaluation(state_values, policy)\n            policy, policy_stable = self.policy_improvement(new_state_values, policy)\n            policies.append(policy.copy())\n            iterations += 1\n            print(f\"Policy iteration {iterations}\")\n        return state_values, policy, policies\n\n    def policy_evaluation(self, state_values, policy):\n        biggest_change = np.inf\n        iterations = 0\n        while biggest_change &gt; self.theta:\n            biggest_change = 0\n\n            num_states = 0\n            average_times = []\n            for i, j in self.states:\n                num_states += 1\n                action = policy[i][j]\n                original_value = state_values[i][j]\n\n                start = time.time()\n                expected_reward = self.expected_reward((i, j), action)\n                expected_value = self.expected_value_2((i, j), action, state_values)\n                # state_values[i][j] = self.expected_value((i, j), action, state_values)\n                state_values[i][j] = expected_reward + expected_value\n                end = time.time()\n                average_times.append(end - start)\n                biggest_change = max(\n                    biggest_change, abs(original_value - state_values[i][j])\n                )\n\n            iterations += 1\n            print(f\"Policy evaluation {iterations}\")\n\n        return state_values\n\n    def policy_improvement(self, state_values, policy):\n        policy_stable = True\n        for i, j in self.states:\n            old_action = policy[i][j]\n            possible_actions = list(range(-j, i + 1))\n            best_action = None\n            best_value = -np.inf\n            if possible_actions == []:\n                continue\n            for action in possible_actions:\n                action_value = self.expected_value((i, j), action, state_values)\n                best_value = max(best_value, action_value)\n                if action_value == best_value:\n                    best_action = action\n            policy[i][j] = best_action\n            if old_action != best_action:\n                policy_stable = False\n        return policy, policy_stable\n\n    def expected_reward(self, state, action):\n        (loc_1_cars, loc_2_cars) = state\n        loc_1_cars -= action\n        loc_2_cars += action\n\n        cost = -(self.cost_per_car_moved * abs(action))\n\n        # Find expected value of returned cards for loc 1\n        loc_1_req = sum(self.loc_1_rent_dist.pmf(i) * i for i in range(loc_1_cars + 1))\n        loc_1_surplus = (1 - self.loc_1_rent_dist.cdf(loc_1_cars)) * loc_1_cars\n        expected_loc_1_returned = loc_1_req + loc_1_surplus\n\n        # Find expected value of returned cards for loc 2\n        loc_2_req = sum(self.loc_2_rent_dist.pmf(i) * i for i in range(loc_2_cars + 1))\n        loc_2_surplus = (1 - self.loc_2_rent_dist.cdf(loc_2_cars)) * loc_2_cars\n        expected_loc_2_returned = loc_2_req + loc_2_surplus\n\n        return cost + self.profit_per_car * (\n            expected_loc_1_returned + expected_loc_2_returned\n        )\n\n    def expected_value_2(self, state, action, state_values):\n        (loc_1_cars, loc_2_cars) = state\n        loc_1_cars -= action\n        loc_2_cars += action\n\n        loc_1_state_dist = final_state_prob_dist(\n            self.cars, loc_1_cars, self.loc_1_rent, self.loc_1_ret\n        )\n        loc_2_state_dist = final_state_prob_dist(\n            self.cars, loc_2_cars, self.loc_2_rent, self.loc_2_ret\n        )\n\n        expected_value = 0\n        for loc_1_cars, loc_1_prob in enumerate(loc_1_state_dist):\n            for loc_2_cars, loc_2_prob in enumerate(loc_2_state_dist):\n                # reward = self.expected_reward((loc_1_cars, loc_2_cars), action)\n                expected_value += (\n                    loc_1_prob\n                    * loc_2_prob\n                    * (\n                        # reward + self.gamma * state_values[loc_1_cars][loc_2_cars]\n                        self.gamma * state_values[loc_1_cars][loc_2_cars]\n                    )\n                )\n        return expected_value\n\n    def expected_value(self, state, action, state_values):\n        expected_value = 0\n\n        (loc_1_cars, loc_2_cars) = state\n\n        # Cars moved at night\n        loc_1_cars -= action\n        loc_2_cars += action\n        cost = 2 * abs(action)\n\n        sum_prob_loc_1_rented = 0\n        for loc_1_rented in range(loc_1_cars + 1):\n            if loc_1_rented == loc_1_cars:\n                p_loc_1_rented = 1 - sum_prob_loc_1_rented\n            else:\n                p_loc_1_rented = self.poisson(self.loc_1_rent, loc_1_rented)\n                sum_prob_loc_1_rented += p_loc_1_rented\n            sum_prob_loc_2_rented = 0\n            for loc_2_rented in range(loc_2_cars + 1):\n                if loc_2_rented == loc_2_cars:\n                    p_loc_2_rented = 1 - sum_prob_loc_2_rented\n                else:\n                    p_loc_2_rented = self.poisson(self.loc_2_rent, loc_2_rented)\n                    sum_prob_loc_2_rented += p_loc_2_rented\n\n                sum_prob_loc_1_returned = 0\n                max_returnable_1 = self.cars - (loc_1_cars - loc_1_rented)\n                for loc_1_returned in range(max_returnable_1 + 1):\n                    if loc_1_returned == max_returnable_1:\n                        p_loc_1_returned = 1 - sum_prob_loc_1_returned\n                    else:\n                        p_loc_1_returned = self.poisson(self.loc_1_ret, loc_1_returned)\n                        sum_prob_loc_1_returned += p_loc_1_returned\n\n                    max_returnable_2 = self.cars - (loc_2_cars - loc_2_rented)\n                    sum_prob_loc_2_returned = 0\n                    for loc_2_returned in range(max_returnable_2 + 1):\n                        if loc_2_returned == max_returnable_2:\n                            p_loc_2_returned = 1 - sum_prob_loc_2_returned\n                        else:\n                            p_loc_2_returned = self.poisson(\n                                self.loc_2_ret, loc_2_returned\n                            )\n                            sum_prob_loc_2_returned += p_loc_2_returned\n\n                        reward = 10 * (loc_1_rented + loc_2_rented)\n                        next_loc_1_cars = loc_1_cars + loc_1_returned - loc_1_rented\n                        next_loc_2_cars = loc_2_cars + loc_2_returned - loc_2_rented\n                        value = (\n                            reward\n                            - cost\n                            + self.gamma\n                            * state_values[next_loc_1_cars][next_loc_2_cars]\n                        )\n                        probability = (\n                            p_loc_1_rented\n                            * p_loc_2_rented\n                            * p_loc_1_returned\n                            * p_loc_2_returned\n                        )\n\n                        expected_value += probability * value\n\n        return expected_value\n\n\njacks_car_rental = JacksCarRental(cars=20)\nstate_values, policy, policies = jacks_car_rental.policy_iteration()\nprint(\"State values:\", state_values)\nprint(\"Policy:\", policy)\n\nPolicy evaluation 1\nPolicy evaluation 2\n\n\nKeyboardInterrupt: \n\n\n\nimport time\n\n\ndef transition(state, action):\n    (loc_1_cars, loc_2_cars) = state\n    loc_1_cars -= action\n    loc_2_cars += action\n    return (loc_1_cars, loc_2_cars)\n\n\ndef actions(state):\n    (loc_1_cars, loc_2_cars) = state\n    return range(loc_1_cars, -loc_2_cars - 1)\n\n\n@functools.cache\ndef poisson(lamb, n):\n    \"\"\"\n    :param lamb: lambda parameter of poisson distribution, rate\n    :param n: n variable of poisson distribution, number of occurrences\n    :return: probability of the event\n    \"\"\"\n    return (lamb**n) * math.exp(-lamb) / math.factorial(n)\n\n\n# Expected value of taking action from state\ndef expected_value(state, action, state_values, discount):\n    expected_value = 0\n    (loc_1_cars, loc_2_cars) = state\n\n    # Cars moved at night\n    loc_1_cars -= action\n    loc_2_cars += action\n    cost = 2 * abs(action)\n\n    sum_prob_loc_1_rented = 0\n    for loc_1_rented in range(loc_1_cars + 1):\n        if loc_1_rented == loc_1_cars:\n            p_loc_1_rented = 1 - sum_prob_loc_1_rented\n        else:\n            p_loc_1_rented = poisson(3, loc_1_rented)\n            sum_prob_loc_1_rented += p_loc_1_rented\n        sum_prob_loc_2_rented = 0\n        for loc_2_rented in range(loc_2_cars + 1):\n            if loc_2_rented == loc_2_cars:\n                p_loc_2_rented = 1 - sum_prob_loc_2_rented\n            else:\n                p_loc_2_rented = poisson(4, loc_2_rented)\n                sum_prob_loc_2_rented += p_loc_2_rented\n\n            sum_prob_loc_1_returned = 0\n            max_returnable_1 = 20 - (loc_1_cars - loc_1_rented)\n            for loc_1_returned in range(max_returnable_1 + 1):\n                if loc_1_returned == max_returnable_1:\n                    p_loc_1_returned = 1 - sum_prob_loc_1_returned\n                else:\n                    p_loc_1_returned = poisson(3, loc_1_returned)\n                    sum_prob_loc_1_returned += p_loc_1_returned\n\n                max_returnable_2 = 20 - (loc_2_cars - loc_2_rented)\n                sum_prob_loc_2_returned = 0\n                for loc_2_returned in range(max_returnable_2 + 1):\n                    if loc_2_returned == max_returnable_2:\n                        p_loc_2_returned = 1 - sum_prob_loc_2_returned\n                    else:\n                        p_loc_2_returned = poisson(2, loc_2_returned)\n                        sum_prob_loc_2_returned += p_loc_2_returned\n\n                    reward = 10 * (loc_1_rented + loc_2_rented)\n                    next_loc_1_cars = loc_1_cars + loc_1_returned - loc_1_rented\n                    next_loc_2_cars = loc_2_cars + loc_2_returned - loc_2_rented\n                    value = (\n                        reward\n                        - cost\n                        + discount * state_values[next_loc_1_cars][next_loc_2_cars]\n                    )\n                    probability = (\n                        p_loc_1_rented\n                        * p_loc_2_rented\n                        * p_loc_1_returned\n                        * p_loc_2_returned\n                    )\n\n                    expected_value += probability * value\n\n    return expected_value\n\n\ndef policy_evaluation(states, state_values, policy, discount, theta):\n    # Big number\n    biggest_change = np.inf\n    num_iterations = 0\n    while biggest_change &gt; theta:\n        num_iterations += 1\n        print(f\"Policy evaluation iteration {num_iterations}\")\n        biggest_change = 0\n\n        num_states = 0\n        average_times = []\n        for i, j in states:\n            num_states += 1\n            # print(f\"State {num_states}\")\n            action = policy[i][j]\n            # Reward from taking the action + discounted value of the next state\n            original_value = state_values[i][j]\n            # Sum of probability of particular state reward pair multiplied by reward and next state value\n\n            # Calculate time taken for one expected value:\n            start = time.time()\n            state_values[i][j] = expected_value((i, j), action, state_values, discount)\n            end = time.time()\n            average_times.append(end - start)\n            # print(f\"Time taken for one expected value: {end - start}\")\n            biggest_change = max(\n                biggest_change, abs(original_value - state_values[i][j])\n            )\n        print(\"Average time: \", np.mean(average_times))\n        print(\"Biggest change: \", biggest_change)\n\n    return state_values\n\n\ndef policy_improvement(states, state_values, policy, discount):\n    policy_stable = True\n    for i, j in states:\n        old_action = policy[i][j]\n        possible_actions = actions((i, j))\n        best_action = None\n        best_value = -np.inf\n        for action in possible_actions:\n            # action_value = reward(s, action) + discount * state_values[next_state]\n            action_value = expected_value((i, j), action, state_values, discount)\n            best_value = max(best_value, action_value)\n            if action_value == best_value:\n                best_action = action\n        policy[i][j] = best_action\n        if old_action != best_action:\n            policy_stable = False\n        return policy, policy_stable\n\n\ndef policy_iteration(states, state_values, policy, discount=0.9, theta=10):\n    policy_stable = False\n    while not policy_stable:\n        new_state_values = policy_evaluation(\n            states, state_values, policy, discount, theta\n        )\n        policy, policy_stable = policy_improvement(\n            states, new_state_values, policy, discount\n        )\n\n    return state_values, policy\n\n\n# num_states = 21\nnum_states = 11\nstates = list(itertools.product(range(num_states), range(num_states)))\nstate_values = np.zeros((num_states, num_states), dtype=int)\npolicy = np.zeros((num_states, num_states), dtype=int)\ndiscount = 0.9\ntheta = 10\n\n# policy_evaluation(state_values, policy, discount, theta)\n\nPolicy evaluation iteration 1\n\n\nIndexError: index 11 is out of bounds for axis 0 with size 11\n\n\n\n# state_values, policy = policy_iteration()\n\nNameError: name 'policy_iteration' is not defined\n\n\n\n\nExercise 4.9\n\n# Generalized value_iteration function\n\n# The \"dynamics\" of the problem:\n# states: A list of states. Each state is represented as a tuple\n# state_values: A dictionary mapping states to their values\n# policy: A dictionary mapping states to actions\n# actions(s): The possible actions in state s\n# reward(s, a): The reward for taking action a in state s\n# transition(s, a): Returns mapping of possible next states to their probabilities, given action a was taken in state s\n\n\ndef value_iteration(\n    states: np.ndarray,\n    state_values: np.ndarray,\n    policy: list[dict[int, int]],\n    actions: Callable[[int], np.ndarray],\n    reward: Callable[[int, int], int],\n    transition: Callable[[int, int], dict[int, float]],\n    gamma=1,\n    theta=0.1,\n):\n    def action_evaluation(s, action):\n        transitions = transition(s, action)\n        value = sum(\n            probability * (reward(s, action) + gamma * state_values[next_state])\n            for next_state, probability in transitions.items()\n        )\n        return value\n\n    sweeps = []\n    sweep_count = 0\n    biggest_change = np.inf\n    while biggest_change &gt; theta:\n        biggest_change = 0\n        for s in states:\n            original_value = state_values[s]\n            best_value = -np.inf\n            possible_actions = actions(s)\n            for action in possible_actions:\n                value = action_evaluation(s, action)\n                if value &gt; best_value:\n                    best_value = value\n            state_values[s] = best_value\n            biggest_change = max(biggest_change, abs(original_value - state_values[s]))\n        if sweep_count &lt; 3:\n            sweeps.append(state_values.copy())\n        sweep_count += 1\n    sweeps.append(state_values.copy())\n\n    # Output deterministic policy pi and value function v\n    for s in states:\n        possible_actions = actions(s)\n        best_value = -np.inf\n        best_action = None\n        for a in possible_actions:\n            value = action_evaluation(s, a)\n            if value &gt; best_value:\n                best_value = value\n                best_action = a\n        policy[s] = best_action\n\n    return state_values, policy, sweeps\n\n\n# State ranges from 1 to 99\nstates = np.arange(1, 100)\n\n# State values ranges from 0 to 100.\n# We include 0 and 100 in state_values as dummy terminal states.\nstate_values = np.zeros(101)\nstate_values[100] = 1\n\npolicy = {s: 0 for s in states}\nactions = lambda s: np.arange(1, np.minimum(s, 100 - s) + 1)\n\n# Reward is 0 for this problem\nreward = lambda s, a: 0\n\n# Generate three transition functions, with differing probabilities\nresults = []\n# for ph in [0.25, 0.4, 0.55]:\n#     transition = lambda s, a: {s + a: ph, s - a: 1 - ph}\n#     state_values_result, policy_result, sweeps = value_iteration(\n#         states=states,\n#         state_values=state_values,\n#         policy=policy,\n#         actions=actions,\n#         reward=reward,\n#         transition=transition,\n#         theta=1e-10,\n#     )\n#     print(policy_result)\n#     results.append((sweeps.copy(), policy_result.copy()))\n\n{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 8, 18: 7, 19: 19, 20: 20, 21: 4, 22: 22, 23: 2, 24: 1, 25: 25, 26: 1, 27: 2, 28: 3, 29: 4, 30: 5, 31: 6, 32: 7, 33: 33, 34: 9, 35: 10, 36: 11, 37: 12, 38: 38, 39: 39, 40: 10, 41: 41, 42: 8, 43: 43, 44: 44, 45: 5, 46: 4, 47: 47, 48: 2, 49: 1, 50: 50, 51: 1, 52: 2, 53: 3, 54: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 65: 10, 66: 16, 67: 8, 68: 18, 69: 19, 70: 20, 71: 4, 72: 22, 73: 2, 74: 1, 75: 25, 76: 1, 77: 2, 78: 22, 79: 4, 80: 5, 81: 6, 82: 7, 83: 8, 84: 9, 85: 10, 86: 11, 87: 12, 88: 12, 89: 11, 90: 10, 91: 9, 92: 8, 93: 7, 94: 6, 95: 5, 96: 4, 97: 3, 98: 2, 99: 1}\n{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 1, 27: 2, 28: 3, 29: 4, 30: 5, 31: 6, 32: 7, 33: 8, 34: 34, 35: 10, 36: 11, 37: 12, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 1, 52: 2, 53: 3, 54: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 65: 15, 66: 16, 67: 17, 68: 18, 69: 19, 70: 20, 71: 21, 72: 22, 73: 23, 74: 24, 75: 25, 76: 1, 77: 2, 78: 3, 79: 4, 80: 5, 81: 6, 82: 7, 83: 8, 84: 9, 85: 10, 86: 11, 87: 12, 88: 12, 89: 11, 90: 10, 91: 9, 92: 8, 93: 7, 94: 6, 95: 5, 96: 4, 97: 3, 98: 2, 99: 1}\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1}\n\n\n\n# for sweeps, policy in results:\n#     plt.figure()\n#     for sweep in sweeps:\n#         plt.plot(sweep)\n#     plt.title(\"Value Iteration\")\n\n#     plt.figure()\n#     print(policy)\n#     plt.step(policy.keys(), policy.values())\n#     plt.title(\"Policy\")\n\n{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 8, 18: 7, 19: 19, 20: 20, 21: 4, 22: 22, 23: 2, 24: 1, 25: 25, 26: 1, 27: 2, 28: 3, 29: 4, 30: 5, 31: 6, 32: 7, 33: 33, 34: 9, 35: 10, 36: 11, 37: 12, 38: 38, 39: 39, 40: 10, 41: 41, 42: 8, 43: 43, 44: 44, 45: 5, 46: 4, 47: 47, 48: 2, 49: 1, 50: 50, 51: 1, 52: 2, 53: 3, 54: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 65: 10, 66: 16, 67: 8, 68: 18, 69: 19, 70: 20, 71: 4, 72: 22, 73: 2, 74: 1, 75: 25, 76: 1, 77: 2, 78: 22, 79: 4, 80: 5, 81: 6, 82: 7, 83: 8, 84: 9, 85: 10, 86: 11, 87: 12, 88: 12, 89: 11, 90: 10, 91: 9, 92: 8, 93: 7, 94: 6, 95: 5, 96: 4, 97: 3, 98: 2, 99: 1}\n{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 1, 27: 2, 28: 3, 29: 4, 30: 5, 31: 6, 32: 7, 33: 8, 34: 34, 35: 10, 36: 11, 37: 12, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 1, 52: 2, 53: 3, 54: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 65: 15, 66: 16, 67: 17, 68: 18, 69: 19, 70: 20, 71: 21, 72: 22, 73: 23, 74: 24, 75: 25, 76: 1, 77: 2, 78: 3, 79: 4, 80: 5, 81: 6, 82: 7, 83: 8, 84: 9, 85: 10, 86: 11, 87: 12, 88: 12, 89: 11, 90: 10, 91: 9, 92: 8, 93: 7, 94: 6, 95: 5, 96: 4, 97: 3, 98: 2, 99: 1}\n{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolicy = results[0][1]\nplt.step(policy.keys(), policy.values())"
  },
  {
    "objectID": "drafts/test_2.html",
    "href": "drafts/test_2.html",
    "title": "Kim Young Jin",
    "section": "",
    "text": "import os  # noqa\nimport pickle  # noqa\nimport numpy as np  # noqa\n\n\nclass environment:\n    def steps(self, state, action):\n        self.state_win = state + action\n        self.state_lose = state - action\n        return self.state_win, self.state_lose\n\n\nclass agent:\n    def __init__(self):\n        self.states = 101\n        self.v = np.zeros((self.states), dtype=\"float\")\n        self.v[-1] = 1\n        self.stable = False\n        self.theta = 0.001\n\n    def possible_actions(self, state):\n        actions = np.arange(1, min(state, 100 - state) + 1, 1)\n        return actions\n\n    def value_iteration(self, ph, env):\n        delta = self.theta\n        sweeps = []\n        sweep = 0\n        while delta &gt;= self.theta:\n            old_values = self.v.copy()\n            for state in range(1, self.states - 1):\n                values = []\n                actions = self.possible_actions(state)\n                for a in actions:\n                    state_win, state_lose = env.steps(state, a)\n                    value = (ph * self.v[state_win]) + ((1 - ph) * self.v[state_lose])\n                    values.append(value)\n\n                values = np.array(values)\n                self.v[state] = np.amax(\n                    values\n                )  # update value function with value maximising action\n            sweeps.append(old_values)\n            sweep += 1\n            delta = np.max(np.abs(old_values - self.v))\n\n            print(f\"Probability of Heads: {ph}\")\n            print(f\"End of sweep: {sweep}, Delta = {delta}\")\n\n        return self.v, sweeps\n\n    def find_policy(self, v, env, ph):\n        stakes = []\n        for state in range(1, self.states - 1):\n            a_vals = []\n            actions = self.possible_actions(state)\n            for a in actions:\n                state_win, state_lose = env.steps(state, a)\n                a_val = (ph * v[state_win]) + ((1 - ph) * v[state_lose])\n                a_vals.append(a_val)\n\n            a_arr = np.array(a_vals)\n            a_max = np.argmax(a_arr) + 1\n            stakes.append(a_max)\n\n        return stakes\n\n\n    env = environment()\n    agent = agent()\n\n    phs = [0.25, 0.55]\n\n    final_vf = {}\n    sweeps = {}\n    policy = {}\n\n    for ph in phs:\n        v_func, sweep = agent.value_iteration(ph, env)\n        stakes = agent.find_policy(v_func, env, ph)\n        final_vf[str(ph)] = v_func\n        sweeps[str(ph)] = sweep\n        policy[str(ph)] = stakes\n\n    with open(\"data/final_v_functions.pickle\", \"wb\") as f:\n        pickle.dump(final_vf, f)\n\n    with open(\"data/sweeps.pickle\", \"wb\") as f:\n        pickle.dump(sweeps, f)\n\n    with open(\"data/policy.pickle\", \"wb\") as f:\n        pickle.dump(policy, f)\n\n\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwith open(\"data/final_v_functions.pickle\", \"rb\") as f:\n    v_functions = pickle.load(f)\n\nwith open(\"data/sweeps.pickle\", \"rb\") as f:\n    sweeps = pickle.load(f)\n\nwith open(\"data/policy.pickle\", \"rb\") as f:\n    policy = pickle.load(f)\n\nph = [\"0.25\", \"0.55\"]\n\nfor p in ph:\n    sweep_p = []\n    policy_p = []\n    for arr in sweeps[p]:\n        sweep_p.append(arr.flatten())\n    for arr in policy[p]:\n        policy_p.append(arr)\n\n    x = np.arange(0, 101, 1)\n    x_pol = np.arange(1, 100, 1)\n\n    fig = plt.figure(figsize=(15, 10))\n    fig.suptitle(f\"$P_h({p})$\", fontsize=16)\n\n    ax1 = fig.add_subplot(211)\n    ax1.title.set_text(\"Value Function Approximation per Sweep\")\n\n    i = 1\n    for arr in sweep_p:\n        ax1.plot(x, arr, label=\"sweep: {}\".format(i))\n        i += 1\n\n    ax1.legend()\n\n    ax2 = fig.add_subplot(212)\n    ax2.title.set_text(\"Optimal Policy\")\n    ax2.plot(x_pol, policy_p)\n\n    plt.savefig(\"images/p_{}.png\".format(p), dpi=300)"
  },
  {
    "objectID": "posts/what-to-do-about-the-future.html",
    "href": "posts/what-to-do-about-the-future.html",
    "title": "What to do about the future",
    "section": "",
    "text": "I think I am often sucseptible to drinking the Silicon Valley Kool-Aid - build, build, build. Build and grind. Make tons of money, and then post snarky Tweets. Treat science and technology as the primary force of good in the world. As long as people pay for and use your product, you are fulfiling an unmet need. Making the world a better place, one line of code at a time.\nIt’s easy to consume this narrative and live out your life according to its doctrine.\nBut there is a more cautious line of thinking that is emerging, and it implores us to look beyond the first-order effect of our technology. It comes under many labels, but the general idea is clear: technology can have many unintended consequences, the nature of which is not at all clear to us right now, before the technology has been created. We should tread carefully.\nI used to dismiss any attempts at this sort of thinking as unnecessary “doomerism”, because the world is a complex place so of course there will be some third-order effects. But that is true of anything we do. Well-intended actions sometimes do more harm than good, and vice versa. The public discouse has also mainly revolved around AGI.\nBut I think the conversation is much bigger than that, and it really requires some introspection on the very nature of our humanity. Because there will be an increasing amount of technology that can lead us astray, distract us from dealing with the “pain of being”. There will be technology that replicates aspects of our intelligence. There will be technology that lets us augment our worst insecurities. There will be room for our insecurities to manifest themselves in our children, by giving them genetic upgrades before birth.\nThis enthralls but also terrifies me. It is like we are starting to play with the essence of the human condition, without understanding its substance. For one - what is consciousness?\nIf we agree that our consciousness is what makes us human, then how will we know when we should start treating our AI counterparts as humans as well?\n\nNeurotechnology\nThe “Supersensorium”\nWhat are the things we should ultimately “value”"
  },
  {
    "objectID": "posts/comparison.html",
    "href": "posts/comparison.html",
    "title": "Comparison",
    "section": "",
    "text": "Instead of asking why someone is better than you - which forces you to look at the uncontrollable - ask how he might be better. And if the answers to the how are outside of your control, accept the difference in leagues. But it’s very rare that there is nothing at all you can do to emulate a characteristic you admire. People in general underestimate the number of things that are in their control.\nThe interplay of mimetic desire and social media is causing a mass convergence in the world. Different “groups” find their respective groups online. Tech people go to tech Twitter. Fitness and beauty go to Instagram. And so on. Everyone sees what everyone else is doing, and tries to emulate them, causing a convergence."
  }
]