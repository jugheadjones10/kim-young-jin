{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Neural network and SGD from scratch\n",
    "author: \"Kim Young Jin\"\n",
    "date: \"2024-06-27\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a neural network for the iris dataset from scratch, copying generously from Karpathy's [micrograd](https://github.com/karpathy/micrograd). We implement SGD for the training loop. Originally I wanted to replicate the MNIST network in Michael Nielson's [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits), but training it took too long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from graphviz import Digraph\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(\n",
    "            other, (int, float)\n",
    "        ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):  # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        return Value(1) / (1 + (-self).exp())\n",
    "\n",
    "    def log(self):\n",
    "        x = self.data\n",
    "        out = Value(math.log(x), (self,), \"log\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / x) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.sigmoid()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, nn, training_data, epochs, mini_batch_size, alpha):\n",
    "        self.nn = nn\n",
    "        self.training_data = training_data\n",
    "        self.epochs = epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            random.shuffle(self.training_data)\n",
    "            mini_batches = [\n",
    "                self.training_data[k : k + self.mini_batch_size]\n",
    "                for k in range(0, len(self.training_data), self.mini_batch_size)\n",
    "            ]\n",
    "            epoch_loss = 0\n",
    "            for mini_batch in mini_batches:\n",
    "                loss = self.update_mini_batch(mini_batch)\n",
    "                epoch_loss += loss\n",
    "            print(f\"Epoch {epoch} complete with loss {epoch_loss}\")\n",
    "\n",
    "    def update_mini_batch(self, mini_batch):\n",
    "        # Forward pass\n",
    "        cost_sum = Value(0)\n",
    "        for x, y in mini_batch:\n",
    "            pred = self.nn(x)\n",
    "            if not isinstance(pred, list):\n",
    "                pred = [pred]\n",
    "            if not isinstance(y, list):\n",
    "                y = [y]\n",
    "            squared_error = sum((pred - label) ** 2 for pred, label in zip(pred, y))\n",
    "            cost_sum += squared_error\n",
    "        final_cost = cost_sum * (1.0 / (2 * self.mini_batch_size))\n",
    "\n",
    "        # Backward pass\n",
    "        parameters = self.nn.parameters()\n",
    "        for p in parameters:\n",
    "            p.grad = 0.0\n",
    "        final_cost.backward()\n",
    "\n",
    "        # Update weights and biases (parameters) of the model\n",
    "        for p in parameters:\n",
    "            p.data = p.data - self.alpha * p.grad\n",
    "\n",
    "        return final_cost.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_iris(return_X_y=True)\n",
    "# One-hot encode the ys\n",
    "x = x.tolist()\n",
    "y = np.eye(3)[y].tolist()\n",
    "dataset = list(zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete with loss 5.228109863636578\n",
      "Epoch 1 complete with loss 4.811590527515836\n",
      "Epoch 2 complete with loss 4.571267491177194\n",
      "Epoch 3 complete with loss 4.3829861375233214\n",
      "Epoch 4 complete with loss 4.197913007139084\n",
      "Epoch 5 complete with loss 4.038164541182581\n",
      "Epoch 6 complete with loss 3.872290225148203\n",
      "Epoch 7 complete with loss 3.7068483386813447\n",
      "Epoch 8 complete with loss 3.5886929561863177\n",
      "Epoch 9 complete with loss 3.477471591816729\n",
      "Epoch 10 complete with loss 3.3726498536848086\n",
      "Epoch 11 complete with loss 3.3036160667866366\n",
      "Epoch 12 complete with loss 3.243014975791261\n",
      "Epoch 13 complete with loss 3.1598249158928664\n",
      "Epoch 14 complete with loss 3.108013472082669\n",
      "Epoch 15 complete with loss 3.0575501646923327\n",
      "Epoch 16 complete with loss 3.0128253161335805\n",
      "Epoch 17 complete with loss 2.9899654206296997\n",
      "Epoch 18 complete with loss 2.955836597378141\n",
      "Epoch 19 complete with loss 2.9267921080388213\n",
      "Epoch 20 complete with loss 2.9016587867839307\n",
      "Epoch 21 complete with loss 2.8681642530749394\n",
      "Epoch 22 complete with loss 2.8606096728389447\n",
      "Epoch 23 complete with loss 2.834266994928878\n",
      "Epoch 24 complete with loss 2.8046677795456088\n",
      "Epoch 25 complete with loss 2.791329804175733\n",
      "Epoch 26 complete with loss 2.7822197841959526\n",
      "Epoch 27 complete with loss 2.7542529806715854\n",
      "Epoch 28 complete with loss 2.748385622854117\n",
      "Epoch 29 complete with loss 2.7382969269286637\n",
      "Epoch 30 complete with loss 2.7259479717010637\n",
      "Epoch 31 complete with loss 2.7241156029486273\n",
      "Epoch 32 complete with loss 2.716380143046377\n",
      "Epoch 33 complete with loss 2.6974342318592397\n",
      "Epoch 34 complete with loss 2.705415435792862\n",
      "Epoch 35 complete with loss 2.684288367477209\n",
      "Epoch 36 complete with loss 2.670980469632754\n",
      "Epoch 37 complete with loss 2.6805757855525045\n",
      "Epoch 38 complete with loss 2.657954832034161\n",
      "Epoch 39 complete with loss 2.64030290091774\n",
      "Epoch 40 complete with loss 2.6506611606648565\n",
      "Epoch 41 complete with loss 2.6341131864461462\n",
      "Epoch 42 complete with loss 2.6389327236528217\n",
      "Epoch 43 complete with loss 2.658157255119224\n",
      "Epoch 44 complete with loss 2.630797927144032\n",
      "Epoch 45 complete with loss 2.6329285445810564\n",
      "Epoch 46 complete with loss 2.622100270705101\n",
      "Epoch 47 complete with loss 2.6189472505952627\n",
      "Epoch 48 complete with loss 2.6111506273809186\n",
      "Epoch 49 complete with loss 2.61483406926503\n",
      "Epoch 50 complete with loss 2.6098673840710576\n",
      "Epoch 51 complete with loss 2.580657262524024\n",
      "Epoch 52 complete with loss 2.5733024075782307\n",
      "Epoch 53 complete with loss 2.610888659454109\n",
      "Epoch 54 complete with loss 2.577303476238778\n",
      "Epoch 55 complete with loss 2.57697819606955\n",
      "Epoch 56 complete with loss 2.5611336453945763\n",
      "Epoch 57 complete with loss 2.5683212672640083\n",
      "Epoch 58 complete with loss 2.5546988456671924\n",
      "Epoch 59 complete with loss 2.6061177817069003\n",
      "Epoch 60 complete with loss 2.566269792542318\n",
      "Epoch 61 complete with loss 2.5581808898703944\n",
      "Epoch 62 complete with loss 2.541730748879436\n",
      "Epoch 63 complete with loss 2.5721121857107145\n",
      "Epoch 64 complete with loss 2.5388815951221813\n",
      "Epoch 65 complete with loss 2.5603894485519123\n",
      "Epoch 66 complete with loss 2.5240267588683505\n",
      "Epoch 67 complete with loss 2.5106422711898055\n",
      "Epoch 68 complete with loss 2.536292391324494\n",
      "Epoch 69 complete with loss 2.542833260411167\n",
      "Epoch 70 complete with loss 2.5866726144730645\n",
      "Epoch 71 complete with loss 2.500508807987302\n",
      "Epoch 72 complete with loss 2.4991181693767524\n",
      "Epoch 73 complete with loss 2.466521693412633\n",
      "Epoch 74 complete with loss 2.491675284642612\n",
      "Epoch 75 complete with loss 2.4582728459269796\n",
      "Epoch 76 complete with loss 2.543997645600261\n",
      "Epoch 77 complete with loss 2.5143626507390833\n",
      "Epoch 78 complete with loss 2.477686776613415\n",
      "Epoch 79 complete with loss 2.4692707546955805\n",
      "Epoch 80 complete with loss 2.4437285301473652\n",
      "Epoch 81 complete with loss 2.4452414022300277\n",
      "Epoch 82 complete with loss 2.550917058313376\n",
      "Epoch 83 complete with loss 2.4162815305296466\n",
      "Epoch 84 complete with loss 2.4249044135608044\n",
      "Epoch 85 complete with loss 2.4337177765246456\n",
      "Epoch 86 complete with loss 2.4117656410547705\n",
      "Epoch 87 complete with loss 2.366356444296752\n",
      "Epoch 88 complete with loss 2.36608388121565\n",
      "Epoch 89 complete with loss 2.360644997007254\n",
      "Epoch 90 complete with loss 2.3559306556490296\n",
      "Epoch 91 complete with loss 2.390512291618162\n",
      "Epoch 92 complete with loss 2.3841548299816377\n",
      "Epoch 93 complete with loss 2.4245216952651822\n",
      "Epoch 94 complete with loss 2.398266995973643\n",
      "Epoch 95 complete with loss 2.490096436730295\n",
      "Epoch 96 complete with loss 2.3256276985777484\n",
      "Epoch 97 complete with loss 2.3413305531696986\n",
      "Epoch 98 complete with loss 2.255239420693627\n",
      "Epoch 99 complete with loss 2.253498801877654\n",
      "Epoch 100 complete with loss 2.30457496528148\n",
      "Epoch 101 complete with loss 2.1386931518199903\n",
      "Epoch 102 complete with loss 2.1494432157565044\n",
      "Epoch 103 complete with loss 2.1562197789521362\n",
      "Epoch 104 complete with loss 1.9681237492874832\n",
      "Epoch 105 complete with loss 1.998383459657808\n",
      "Epoch 106 complete with loss 2.0916873114669725\n",
      "Epoch 107 complete with loss 1.9370039049463867\n",
      "Epoch 108 complete with loss 1.8082898404713235\n",
      "Epoch 109 complete with loss 1.812653036751982\n",
      "Epoch 110 complete with loss 1.6724278732455857\n",
      "Epoch 111 complete with loss 1.7614745256654614\n",
      "Epoch 112 complete with loss 1.6270490775055995\n",
      "Epoch 113 complete with loss 1.7125566344208305\n",
      "Epoch 114 complete with loss 1.6009986233077105\n",
      "Epoch 115 complete with loss 1.4982073887693281\n",
      "Epoch 116 complete with loss 1.475530739883789\n",
      "Epoch 117 complete with loss 1.3555408884653557\n",
      "Epoch 118 complete with loss 1.3344281774473197\n",
      "Epoch 119 complete with loss 1.6540645857991139\n",
      "Epoch 120 complete with loss 1.3785113902670765\n",
      "Epoch 121 complete with loss 1.3377315457682462\n",
      "Epoch 122 complete with loss 1.2055035299720998\n",
      "Epoch 123 complete with loss 1.5007868813897596\n",
      "Epoch 124 complete with loss 1.2124374709105101\n",
      "Epoch 125 complete with loss 1.1751378331535662\n",
      "Epoch 126 complete with loss 1.1468478852234119\n",
      "Epoch 127 complete with loss 1.1009705359398845\n",
      "Epoch 128 complete with loss 1.2528156212580208\n",
      "Epoch 129 complete with loss 1.052100028086615\n",
      "Epoch 130 complete with loss 1.0359952882633874\n",
      "Epoch 131 complete with loss 0.9593625855542283\n",
      "Epoch 132 complete with loss 1.0620169971479294\n",
      "Epoch 133 complete with loss 1.1674910174008557\n",
      "Epoch 134 complete with loss 1.1150473251730098\n",
      "Epoch 135 complete with loss 0.8601662220541026\n",
      "Epoch 136 complete with loss 0.9902350074041312\n",
      "Epoch 137 complete with loss 0.8462934959065628\n",
      "Epoch 138 complete with loss 0.8506323739609574\n",
      "Epoch 139 complete with loss 0.8572265828640774\n",
      "Epoch 140 complete with loss 0.8723254165562639\n",
      "Epoch 141 complete with loss 0.8375449909042699\n",
      "Epoch 142 complete with loss 0.7832565729525689\n",
      "Epoch 143 complete with loss 0.8490069738978305\n",
      "Epoch 144 complete with loss 0.9592682936105128\n",
      "Epoch 145 complete with loss 0.7700136543305345\n",
      "Epoch 146 complete with loss 0.715320302076721\n",
      "Epoch 147 complete with loss 0.7109569006887824\n",
      "Epoch 148 complete with loss 0.7440288649753813\n",
      "Epoch 149 complete with loss 0.865334355902204\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(4, [3, 3])\n",
    "trainer = SGD(mlp, dataset, 150, 10, 0.5)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model accuracy on dataset\n",
    "correct = 0\n",
    "for x, y in dataset:\n",
    "    pred = mlp(x)\n",
    "    pred = [p.data for p in pred]\n",
    "    if np.argmax(pred) == np.argmax(y):\n",
    "        correct += 1\n",
    "print(f\"Accuracy: {correct / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around with Michael Nielson's implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k : k + mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [\n",
    "            w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)\n",
    "        ]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # list to store all the activations, layer by layer\n",
    "        zs = []  # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return output_activations - y\n",
    "\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "# Convert zip iterators to lists\n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "test_data = list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9050 / 10000\n",
      "Epoch 1: 9187 / 10000\n",
      "Epoch 2: 9289 / 10000\n",
      "Epoch 3: 9295 / 10000\n",
      "Epoch 4: 9311 / 10000\n",
      "Epoch 5: 9328 / 10000\n",
      "Epoch 6: 9369 / 10000\n",
      "Epoch 7: 9329 / 10000\n",
      "Epoch 8: 9415 / 10000\n",
      "Epoch 9: 9393 / 10000\n",
      "Epoch 10: 9377 / 10000\n",
      "Epoch 11: 9438 / 10000\n",
      "Epoch 12: 9435 / 10000\n",
      "Epoch 13: 9388 / 10000\n",
      "Epoch 14: 9445 / 10000\n",
      "Epoch 15: 9437 / 10000\n",
      "Epoch 16: 9437 / 10000\n",
      "Epoch 17: 9430 / 10000\n",
      "Epoch 18: 9452 / 10000\n",
      "Epoch 19: 9447 / 10000\n",
      "Epoch 20: 9407 / 10000\n",
      "Epoch 21: 9468 / 10000\n",
      "Epoch 22: 9455 / 10000\n",
      "Epoch 23: 9450 / 10000\n",
      "Epoch 24: 9459 / 10000\n",
      "Epoch 25: 9430 / 10000\n",
      "Epoch 26: 9449 / 10000\n",
      "Epoch 27: 9476 / 10000\n",
      "Epoch 28: 9450 / 10000\n",
      "Epoch 29: 9467 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
