---
title: "Kim Young Jin"
page-layout: full
about:
  id: hero-heading
  template: trestles
  image: profile.jpg
  image-width: 20em
  links:
    - icon: twitter-x
      href: https://x.com/vonnynonnyj
    - icon: linkedin
      href: https://www.linkedin.com/in/kim-young-jin-951a55134/
    - icon: github
      href: https://github.com/jugheadjones10
    - icon: envelope
      href: mailto:kimyoungjin1001@gmail.com

listing:
  - id: intelligence
    contents: 
      - ml
    type: table
    sort-ui: false
    filter-ui: false
    fields: [date, title, reading-time]
    sort: "date desc"


  - id: life
    contents: 
      - posts
    type: table
    sort-ui: false
    filter-ui: false
    fields: [date, title, reading-time]
    sort: "date desc"
---

:::{#hero-heading}

Year 4 CS undergraduate at the National University of Singapore. 

Currently exploring Deep Reinforcement Learning, Open-Endedness, Unsupervised Environment Design, and Meta-learning. 

Some questions on my mind:

- What is the better way to understand intelligence - engineering it from the ground up or by trying to reverse-engineer the brain?

- What are the limits to meta-learning? What's stopping us from meta-learning every part of an ML algorithm?

- What should we vary in the environments we use to train generally capable agents? Transition dynamics? Observations? Reward functions? Intuitively, varying the reward function seems like it will produce agents that can succeed at arbitrary tasks.

- Is there any point in computational neuroscience if we aren't even able to properly understand how deep NNs do what they do?
:::

<br>
<br>
<br>
I like to write about ideas and things I find interesting. I also share dumb technical mistakes I make so others don't make them.
<br>
<br>
<br>

### Intelligence, ML, and Jax
:::{#intelligence}
:::

### Misc
:::{#life}
:::